\subsection{Description of Transform functions}

As mentioned earlier, different API patterns of the codes
require different transformation rules to correctly transform them.
To apply different transformation rules for different API pattern codes,
we defined sets of transform functions each for trainng API patterns.
This section explains the transform functions defined for each API patterns.

\subsubsection{Transform function for Session pattern}

\begin{figure}[ht!]
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
optimizer = tf.train.MomentumOptimizer(learning_rate = 0.01)

with tf.Session() as sess:
    for step in range(num_epochs): 
        sess.run(optimizer, feed_dict)
    \end{lstlisting}
    \caption{Original training code of Session pattern}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
optimizer = tf.train.MomentumOptimizer(learning_rate = 0.01 * hvd.size())
optimizer = hvd.DistributedOptimizer(optimizer)

with tf.Session() as sess:
    for step in range(num_epochs): 
        sess.run(optimizer, feed_dict)
    \end{lstlisting}
    \caption{Distributed training code of Session pattern}
  \end{subfigure}
  \caption{Code example of Session pattern training code}
  \label{fig:trans:sessiontrans}
\end{figure}

To correctly transform Session pattern training codes into distributed 
training codes, the {\tt Optimizer} instance must be modified.
Figure \ref{fig:trans:sessiontrans} shows a pair of code examples
illustrating the required transformation for Session pattern case.
In line 1 of figure \ref{fig:trans:sessiontrans}(b),
the learning rate argument is scaled by {\tt hvd.size()}.
The learning rate can be specified by keyword argument {\tt learning\_rate}
as the figure \ref{fig:trans:sessiontrans},
or by the first positional argument.
The transform function should be able to transform both of the cases.
The transformation also adds a new statementd that wraps the 
{\tt Optimizer} instance by {\tt DistributedOptimizer}.

\begin{figure}[ht!]
\noindent
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} }{\smodenv} = \\
  \inden \ktif ~ \nexprsubs{1} \ktsubtysubs{\smodenv} ~ {\tt tensorflow.keras.optimizers.Optimizer} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt learning\_rate} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i} {\tt * hvd.size()}
  ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \\
  \inden\inden\inden {\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],\\
  \inden\inden\inden \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\

  \inden\inden \ktelse \\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} {\tt * hvd.size()}... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i}
  ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \\
  \inden\inden\inden {\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}], \\
  \inden\inden\inden\smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\

\end{longtable}
  \caption{Session pattern transform function: Optimizer learning rate scaling and wrapping}
  \label{fig:trans:sessrule}
\end{figure}


Figure \ref{fig:trans:sessrule} formalizes the Sesison pattern transform 
function. The transform function 
matches assign statements that assign a result of function call expression.
If an assign statement is matched, the pattern guard checks if the
callee function is the {\tt Optimizer} instance constructor function.
The pattern guard uses the subclass predicate \ktsubtysubs{\smodenv}
to identify any expression that constructs the subclass of the TensorFlow
{\tt Optimizer} class including user-defined classes.

The third line of the transform function uses a branch to distinguish 
whether the {\tt learning\_rate} argument is given as a keyword argument or not. 
When the argument is given as a keyword argument,
the transform function modifies the corresponding keyword argument expression
as shown in the true branch in fourth line.
When the argument is not given as a keyword argument,
the transform function assumes that the first positional argument
is the {\tt learning\_rate} argument and modifies it
as shown in the eighth line.
In either case, the transform function additionally returns a new
assign statement that wraps the {\tt Optimizer} with
{\tt hvd.DistributedOptimizer}.

% todo: remove pagebreak
\pagebreak

\subsubsection{Transform function for MonitoredSession pattern}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
with tf.train.MonitoredTrainingSession(hooks=hooks) as mon_sess:
    while not mon_sess.should_stop():
        mon_sess.run()
    \end{lstlisting}
    \caption{Original training code of MonitoredSession pattern}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
with tf.train.MonitoredTrainingSession(hooks=hooks.append(hvd.BroadcastGlobalVariablesHook(0)) as mon_sess:
    while not mon_sess.should_stop():
        mon_sess.run()
    \end{lstlisting}
    \caption{Distributed training code of MonitoredSession pattern}
  \end{subfigure}
  \caption{Code example of MonitoredSession pattern training code}
  \label{fig:trans:monsesstrans}
\end{figure}


Figure \ref{fig:trans:monsesstrans} describes an example of transforming
a MonitoredSession pattern model.
To correctly transform the MonitoredSesion pattern codes,
a hook for the variable broadcasting should be added to the
{\tt MonitoredSession} object. 
As in figure \ref{fig:trans:monsesstrans}, for example,
the transformation appends {\tt BroadcastGlobalVariablesHook} to the
{\tt hooks} arguments of the {\tt MonitoredSession} constructor call.

\begin{figure}[ht!]
 \noindent
  \begin{tabular}{l}
    \typdesc{\fkstmt& : & \dstmt ~ $\rightarrow$ ~ \dmodenv ~ $\rightarrow$ ~ (\dstmt ~ list ~ $\times$ ~ \dmodenv)} \\
    \tstmt{\kwith ~ \mul{\nwithitem} ~ \kcolon ~ \mul{\nstmt}}{\smodenv} = \\
    \inden \ktlet ~ \mul{\nwithitem}$'$, \smodenvsubs{1} \kteq ~ \twwithitem{\mul{\nwithitem}}{\smodenv} \ktin \\
    \inden \ktlet ~ \mul{\nstmt}$'$, \smodenvsubs{2} \kteq ~ \tsstmt{\mul{\nstmt}}{\smodenvsubs{1}} \ktin \\

    \inden ([\kwith ~ \mul{\nwithitem}$'$ ~ \kcolon ~ \mul{\nstmt}$'$], \smodenvsubs{2})
  \end{tabular}\\

  \begin{tabular}{l}
    \typdesc{\fkwithitem & : & \dwithitem ~ $\rightarrow$ ~ \dmodenv ~ $\rightarrow$ ~ (\dwithitem ~ $\times$ \dmodenv)}  \\
    \twithitem{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ 
                \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} {\tt as} \nidsubs{as} }{\smodenv} = \\

    \inden \ktif ~ \nexprsubs{1} \ktsubtysubs{\smodenv} {\tt tensorflow.train.MonitoredSession} ~ \ktthen \\
    \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt hooks} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
    \inden\inden\inden(\nexprsubs{1} \sparen{\nexprsubs{11} ... 
              \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} \\
    \inden\inden\inden\inden ... \nidsubs{i} \oassign {\tt \nexprsubs{2i}.append(hvd.BroadcastGlobalVariablesHook(0))} \\
    \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k} }, \\
    \inden\inden\inden\inden \smodenv[ \msess $\mapsto$ \nidsubs{as}]) \\
  \end{tabular}\\\vpar
\caption{MonitoredSession pattern transform function: Modifying {\tt StopAtStepHook} instance}
  \label{fig:trans:monsessrule}
\end{figure}

Figure \ref{fig:trans:monsessrule} formalizes the transform functions for
MonitoredSession pattern.
The first transform function matches {\tt with} statements and transform
the {\tt with\_item}, a syntactic component representing 
a variable and an expression being assigned.
The transform function \fkwwithitem in the second line is a 
transform function that applies transformations to each {\tt with\_item}.
The first transform function returns a new {\tt with} statement 
that assigns the transformed {\tt with\_items}.

The second transform function in the figure transforms the {\tt with\_item}.
The function matches {\tt with\_items} that assignes a function call result.
Then the second line checks if the function is a constructor
of the subclass of the {\tt MonitoredSession} class. 
The transform function appends the {\tt BroadcastGlobalVariablesHook} object 
to the {\tt hooks} argument.
   

\pagebreak

\subsubsection{Transform function for GradientTape pattern}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf

with tf.GradientTape() as tape:
    probs = model(images)
    loss_value = loss(labels, probs)

grads = tape.gradient(loss_value, model.trainable_variables)
opt.apply_gradients(zip(grads, model.trainable_variables))
    \end{lstlisting}
    \caption{Original training code of GradientTape pattern}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
import horovod.tensorflow as hvd
hvd_broadcast_done = False

with tf.GradientTape() as tape:
    probs = model(images)
    loss_value = loss(labels, probs)
tape = hvd.DistributedGradientTape(tape)
grads = tape.gradient(loss_value, model.trainable_variables)
id_new = zip(grads, model.trainable_variables)
opt.apply_gradients(id_new)

global hvd_broadcast_done
if not hvd_broadcast_done:
    hvd.broadcast_variables([x[1] for x in id_new], root_rank=0, )
    hvd.broadcast_variables(opt.variables(), root_rank=0, )
    hvd_broadcast_done = True
    \end{lstlisting}
    \caption{Distributed training code of GradientTape pattern}
  \end{subfigure}
  \caption{Code example of GradientTape pattern training code}
  \label{fig:trans:gtapetrans}
\end{figure}


Figure \ref{fig:trans:gtapetrans} illustrates the transformation of
GradientTape pattern code to the corresponding distributed code.
To correctly transform GradientTape pattern training codes,
the {\tt GradientTape} instance should be modified for distributed training.
As shown in line 8, figure \ref{fig:trans:gtapetrans}(b),
the {\tt GradientTape} instance {\tt tape}
is wrapped by a Horovod library API {\tt DistributedGradientTape}.
In addition to modifying the {\tt GradientTape} instance,
lines 13 to 17 add the variable broadcasting code after the
{\tt apply\_gradients} method call.
Note that the variable {\tt hvd\_broadcast\_done} is initialized as 
{\tt False} at line 3. 
Then line 17 sets the variable to {\tt True}, after the variable broadcasting
is done.
We use the boolean variable {\tt hvd\_broadcast\_done} in GrdientTape pattern
model to ensure that variable broadcasting occurs exactly once.

\pagebreak

\begin{figure}[ht!]
\noindent
\begin{tabular}{l}
  \tstmt{\kwith ~ \nwithitem ~ \kcolon ~ \mul{\nstmt}}{\smodenv} = \\
  \inden \ktlet ~ \nwithitem$'$, \smodenvsubs{1} \kteq ~ \twwithitem{\nwithitem}{\smodenv} \ktin \\
  \inden \ktlet ~ \nstmt$'$, \smodenvsubs{2} \kteq ~ \tsstmt{\mul{\nstmt}}{\smodenvsubs{1}} \ktin \\
  \inden \ktif ~ \smodenvsubs{1} \envsub ~ \smodenv ~ \kteq ~ [\gtape $\mapsto$ \nid] ~ \ktthen\\
  \inden\inden ([\optypcomm ~ \kwith ~ \nwithitem$'$ ~ \kcolon ~ \mul{\nstmt}$'$, \\
  \inden\inden \nid ~ \oassign {\tt hvd.DistributedGradientTape(\nid)}], \smodenvsubs{2})\\
  \inden \ktelse ~ ([\kwith ~ \nwithitem$'$ ~ \kcolon ~ \mul{\nstmt}$'$], \smodenvsubs{2})
\end{tabular}
  \caption{GradientTape pattern transform function: wrapping {\tt GradientTape} instance}
  \label{fig:trans:gtaperule}
\end{figure}

Figure \ref{fig:trans:gtaperule} formalizes the partial transform function
that warps the {\tt GradientTape} instance with {\tt DistributedGradientTape}.
The transform function matches {\tt with} statements,
then compares the transform contexts to check
if the input with statement assigns a new {\tt GradientTape} instance.
The transform function adds a new assign statment, 
$\nid ~ \oassign {\tt hvd.DistributedGradientTape(\nid)}$ after the original
statement.


\begin{figure}[ht!]
\noindent
\begin{tabular}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} }{\smodenv} = \\
  \inden \ktif  ~ \smodenv(\optmizer) ~ \kteq ~ \nidsubs{t} ~ \ktand ~ \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{t}.apply\_gradients} ~ \ktthen\\
  \inden\inden \ktlet ~ \nidsubs{z} ~ \kteq ~ \newid ~ \ktin \\
  \inden\inden ([\nidsubs{z} ~ \oassign ~ \nexprsubs{11},\\
  \inden\inden \nidsubs{r} \oassign \nexprsubs{1} \sparen{\nidsubs{z} \nexprsubs{12} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} ,\\
  \inden\inden {\tt if not hvd\_broadcast\_done:} \\ 
  \inden\inden\inden [ {\tt hvd.broadcast\_variables([x[1] for x in \nidsubs{z}], root\_rank=0)}, \\
  \inden\inden\inden {\tt hvd.broadcast\_variables(\nidsubs{t}.variables(), root\_rank=0)}, \\
  \inden\inden\inden {\tt hvd\_broadcast\_done = True} ]\\
  \inden\inden ], \smodenv) \\
\end{tabular}
  \caption{GradientTape pattern transform function: adding variable broadcasting code}
  \label{fig:trans:gtaperule2}
\end{figure}

Figure \ref{fig:trans:gtaperule2} formalizes the partial transform function
that appends the variable broadcasting code after the 
{\tt apply\_gradients} method call.
The function matches assign statements that assigns a function call
result. Then the pattern guard checks if the called function is the
{\tt apply\_gradients} method of the {\tt Optimizer} instance.
Note that the function utilizes the tranform context \smodenv to
retreive the identifier of {\tt Optimizer} variable. 
The transform function adds the variable broadcasting code.

\pagebreak

\subsubsection{Transform function for Keras pattern}

\begin{figure}[ht!]
  \begin{subfigure}[t]{0.45\textwidth}
  \begin{lstlisting}[language=Python]
class ResNet(keras.Model):
    def __init__(self, block_list):
        ...

model = ResNet([2, 2, 2])

model.fit(x_train, y_train)
  \end{lstlisting}
  \caption{Original code}
  \end{subfigure}
  \hspace{3mm}
  \begin{subfigure}[t]{0.45\textwidth}
  \begin{lstlisting}[language=Python]
class ResNet(keras.Model):
    def __init__(self, block_list):
        ...

model = ResNet([2, 2, 2])

callbacks=[hvd.callbacks.BroadcastGlobalVariablesCallback(0)
model.fit(x_train, y_train,
          callbacks=callbacks)
  \end{lstlisting}
    \caption{Distributed code}
  \end{subfigure}
  \caption{Code transformation example: Keras Pattern}
  \label{fig:trans:keras}
\end{figure}


Figure \ref{fig:trans:keras} illustrates an example of Keras pattern model
codes.
To correctly transform the Keras pattern codes,
the transform function must first recognize the Keras library's {\tt Model} 
class instance, including the user-define classes.
Then the transform function should add the variable broadcasting callback to
the {\tt fit} method call argument.
The transform function must recognize that the call expression
contains the {\tt callbacks} keyword argument then append the
{\tt BroadcastGlobalVariablesCallback} on the original argument expression,
or add the keyword argument as shown in the figure \ref{fig:trans:keras}(b).

\begin{figure}[ht!]
\centering
\begin{longtable}{l}
  \tstmt{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
  \inden \ktif ~ \nidsubs{m} ~ \kteq ~ \smodenv({\tmodel}) ~ \ktand ~ 
          \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{m}.fit} ~ \ktthen \\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt callbacks} ~ \ktwhen ~ 2 $\leq$ i $\leq$ k ~ \ktthen \\
  \inden\inden\inden ([{\tt callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)]},\\
  \inden\inden\inden ~ {\tt if hvd.rank() == 0: callbacks.append(\nexprsubs{2i})}, \\
  \inden\inden\inden ~ {\tt \nexprsubs{1} (\nexprsubs{11} ... \nexprsubs{1n}}
                              \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                              \nidsubs{i} \oassign {\tt callbacks} ... 
                              \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}{\tt )}], \smodenv) \\
  \inden\inden \ktelse \\
  \inden\inden\inden ([{\tt callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)]},\\
  \inden\inden\inden ~ {\tt \nexprsubs{1} (\nexprsubs{11}... \nexprsubs{1n}}
                              \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                              \op{(\nidsubs{k} \oassign)} \nexprsubs{2k} ~ 
                              {\tt callbacks \oassign callbacks} {\tt )}], \smodenv) \\
\end{longtable}
  \caption{Transform function for the Keras pattern}
  \label{fig:trans:kerasrule}
\end{figure}

Figure \ref{fig:trans:kerasrule} formalizes the transform function 
for the Keras pattern.
The transform function matches function call statements. 
Then the pattern guard in the second line checks if the function expression
is the {\tt fit} method of the {\tt Model} subclass.
The branch in the third line checks if the method call already has
the {\tt callbacks} keyword argument or not.
If the keyword argument exists, the transform function returns the list of 
three statements in the true branch. The first and second statements
creates a temporary varaible {\tt callbacks} which will be later
used as the argument in the third statement.
Note the the second statement appends the original
callbacks to the {\tt callbacks} variable only when the {\tt hvd.rank()} 
is zero; this prevents multiple processes from repeating redundant callbacks
for same training step, and makes only one process repeats the original
callbacks.
If the keyword argument does not exist in the input call expression,
the transform function adds the {\tt callback} argument
as shown in the figure \ref{fig:trans:keras}(b).

\pagebreak



