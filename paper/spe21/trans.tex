\section{Code Transformation for Distributed Training}\label{sec:trans}
\subsection{Python Abstract Syntax}\label{sec:pysyn}

We first define the abstract syntax for Python programming language.
The syntax of Python is described in the Python Language Reference \cite{pythonref}.
The reference provides a full grammar specification based on the extended PEG,
and detailed explanation of syntatic components in each section.
We manually examined the grammar and the details
to define the Python abstract sytax.
The syntax is composed of three syntactic components: expressions, statements,
and top-level components.

\begin{tabular}{lrll}
  \nmodule & := & \mul{\nstmt} ~ \ntypignore & \desc{ModuleDef} \\
  \nstmt & ::= & \decolist ~ \kdef ~ \nid ~ \sparen{\nargs} ~ \op{(\krightarrow ~ \nexpr)} ~ \kcolon ~ \optypcomm ~ \mul{\nstmt} & \desc{FunDef} \\ 
  & $|$ & \decolist ~ \kclass ~ \nid ~ \sparen{\mul{\nexpr} \mul{\nkeyword}} ~ \kcolon ~ \mul{\nstmt} & \desc{ClassDef} \\
  & $|$ & \mul{\nexpr} ~ \oassign ~ \nexpr ~ \optypcomm & \desc{Assign} \\
  & $|$ & \optypcomm ~ \kfor ~ \nexpr ~ \kin ~ \nexpr ~ \kcolon ~ \mul{\nstmt} ~ \op{(\kelse ~ \kcolon ~ \mul{\nstmt})}& \desc{ForLoop} \\
  & $|$ & \kwhile ~ \sparen{\nexpr} ~ \kcolon ~ \mul{\nstmt} ~ \op{(\kelse ~ \kcolon ~ \mul{\nstmt})}& \desc{WhileLoop} \\
  & $|$ & \kif ~ \sparen{\nexpr} ~ \kcolon ~ \mul{\nstmt} ~ \op{(\kelse ~ \kcolon ~ \mul{\nstmt})}& \desc{If} \\
  & $|$ & \optypcomm ~ \kwith ~ \mul{\nwithitem} ~ \kcolon ~ \mul{\nstmt} & \desc{With} \\
  & $|$ & \kimport ~ \mul{\nalias} & \desc{Import} \\
  & $|$ & \kfrom ~ \nint ~ \op{\nid} \kimport ~ \mul{\nalias} & \desc{ImportFrom} \\
  & $|$ & \nexpr & \desc{ExprStmt} \\
\end{tabular}

\begin{tabular}{lrll}
  \nexpr & ::= & \nexpr ~ \nboolop ~ \nexpr & \desc{BoolOp} \\
  & $|$ & \nexpr ~ \nbinop ~ \nexpr & \desc{BinaryOp} \\ 
  & $|$ & \nunop ~ \nexpr & \desc{UnaryOp} \\ 
  & $|$ & \lparen{\mul{\nexpr}} & \desc{List} \\ 
  & $|$ & \sparen{\mul{\nexpr}} & \desc{Tuple} \\ 
  & $|$ & \nexpr ~ \mul{(\ncompop ~ \nexpr)} & \desc{CompOp} \\
  & $|$ & \nexpr ~ \sparen{\mul{\nexpr} \mul{\nkeyword}} & \desc{Call} \\
  & $|$ & \nconstant & \desc{Constant} \\
  & $|$ & \nexpr {\tt .}\nid& \desc{Attribute} \\
  & $|$ & \nexpr\lparen{\nexpr} & \desc{Subscript} \\
  & $|$ & \nid & \desc{Name} \\

  \nboolop & ::= & \oand ~ $|$ ~ \oor & \desc{BoolOperator} \\
  \nbinop & ::= & \oand ~ $|$ ~ \osub ~ $|$ ~ \omul & \desc{BinOperator} \\
  \nunop& ::= & \kinvert ~ $|$ ~ \knot ~ $|$ ~ \oadd ~ $|$ ~ \osub & \desc{UnOperator} \\
  \ncompop& ::= & \oeq ~ $|$ ~ \oneq ~ $|$ ~ \olt ~ $|$ ~ \olte ~ $|$ ~ \ogt ~ $|$ ~ \ogte ~ $|$ ~ \ois ~ $|$ ~ \onis ~ $|$ ~ \oin ~ $|$ ~ \onin & \desc{CompOperator}\\
  \nargs & ::= & \mul{(\narg ~ \op{(\oassign ~ \nexpr)})}, ~ \mul{(\narg ~ \op{(\oassign ~ \nexpr)})}, ~ \op{\narg}, ~ \mul{(\narg ~ \op{(\oassign ~ \nexpr)})}, ~ \op{\narg} & \desc{Arguments}\\
  \narg & ::= & \nid ~ \op{\nexpr}~\op{\nstr} & \desc{Argument} \\
  \nkeyword & ::= & \op{\nid} \oassign \nexpr & \desc{Keyword} \\ 
  \nalias & ::= & \nid ~\mul{(.\nid)} \op{(\kas ~ \nid)} & \desc{Alias} \\
  \nwithitem & ::= & \nexpr ~ \op{(\kas ~ \nexpr)} & \desc{WithItem}\\
\end{tabular}

\begin{tabular}{lrll}
  \nconstant & ::= & \knone & \desc{NoneLiteral} \\
  & $|$ & \nint & \desc{IntLiteral} \\
  & $|$ & \nfloat & \desc{FloatLiteral} \\
  & $|$ & \ncomplex & \desc{ComplexLiteral} \\
  & $|$ & \nstr & \desc{StringLiteral} \\
  & $|$ & \nbool & \desc{BooleanLiteral} \\
  & $|$ & \sparen{\mul{\nconstant}} & \desc{TupleLiteral} \\
  & $|$ & {\tt ...} & \desc{Ellipsis} \\
  \ntypignore & ::=  & \mul{\nint} & \desc{TypeIgnore} \\
  \nid & $\in$ & \did \\
  \nstr & $\in$ & \dstr \\
  \nbool & $\in$ & \{{\tt True}, {\tt False}\}\\
  \nint & $\in$ & $\mathbb{Z}$ \\
  \nfloat & $\in$ & $\mathbb{R}$ \\
  \ncomplex & $\in$ & $\mathbb{C}$ \\
\end{tabular}

Expressions are parts of the code that evaluates to a value.
Python has 5 kinds of primitive values, 
which are numbers, strings, booleans, the value {\itshape None} and {\itshape Ellipsis}.
The value None is used to denote a undefined value,
similar to the value {\itshape null} in Java.
The value Ellipsis corresponds to the notation ``...",
which can be used as an special placeholder meaning expansion of the sequence. 
Python has composite types of tuple, list, set, map, and custom classes.
The expression syntax defines ways to build up values
and complex expressions such as operators, comprehension, and function calls. 

Statements are parts of the code that changes the program state,
such as variable binding or control flow. 
Python statements are categorized into 
simple statements which denote a single state changing step, 
or compound statements which are composed of multiple statements.

Simple statements include assignment statements and import statements.
Assignment statements are used to declare a new variable and its value
or update a variable value.
Import statements are used to specify a module, load it
and get definitions of the module into current namespace.
In import statements, each target module and its aliased name is represented
as alias.   
Note that procedure call is a special case of expression statements,
where the expression is a function call.

Compound statements include conditional statements, loop statements,
and definitions for functions and classes.
Additionally, the with statement is an special kind of compound statement.
With statement is identical to the assignment statement 
in a way that it binds an expression to a name, 
but the statement additionally adds implicit calls
to the object's methods related to initialization and destruction.
Similar to the alias in import statement,
with statements use WithItem to represent then ame and expression.

Top-level components are representation of the program
in different execution environment of the Python interpreter.
For example, a module represents a Python code file, composed of
multiple definitions speicified of statements. 
The full Python abstract syntax is attched in the supplementary material.

\subsection{Training API Patterns for TensorFlow ML model}

\lstinputlisting[language=Python]
{pattern_ex.py}

TensorFlow provides various APIs to define the training process.
The above figure illustrates two examples of using 
different APIs for the model training.
Line 2 to 10 uses low-level APIs to repeat training steps over the training
data set. Inside the {\tt for} loop, 
the code uses the {\tt GradientTape} instance to record the loss computation,
then uses the {\tt gradient} method and the {\tt apply\_gradient} method to
back-propagate the gradient to the model parameters.
In contrast, line 13 and 14 uses high-level APIs to automatically
invoke the model training process by two methods calls, {\tt compile} and
{\tt fit}. The {\tt fit} method repeats a same process with the {\tt for}
loop in line 2; it takes each training data from the set and
applies the gradient to the model parameter.

While both code fragments train the model in a same way, 
the code structures significantly differ.
Using high-level APIs simplifies the program,
reducing additional codes that compute the prediction loss and apply gradients.
Low-level APIs are verbose, however, developer can fully control
the training process.
Developers can freely chose from different training code styles to
take advantage of each way.

To correctly transform the training codes,
the transform software must transform different training APIs
based on different transformation rules. 
We first inspected multiple TensorFlow training codes and categorized them
byr their training API usage. 
As as result, we defined total four \textit{training API categories},
two for TensorFlow version 1 and two for TensorFlow version 2.

\begin{center}
  \begin{tabular}{|c|c|l|}
  \hline
  TF version & Category name & Explanation \\ 
  \hline
  TFv1 & Session & Low-level training method using {\tt Session} API. \\
  \hline
  TFv1 & MonitoredSession & Low-level Training using {\tt MonitoredSession} instance. \\
  \hline
  TFv2 & GradientTape & Low-level training using {\tt GradientTape} instance. \\
  \hline
  TFv2 & Keras & High-level training using {\tt fit} method of {\tt keras.models.Model} instance. \\
  \hline
\end{tabular}
\end{center}

Then, we defined \textit{training API patterns} to automatically
categorize training codes into one of the four categories.
We adopted the pattern matching concept to define \textit{code patterns}.
The code patterns are \textit{pattern for code AST}; they are AST with
special holes that are later matched with concrete value.
When a code pattern is matched against a code AST,
it \textit{succeeds} with the holes matched with the AST subnodes,
or \textit{fails} to match the given AST.
The training API pattern is a set of code patterns
to match against a training code and decide whether the code belongs to
the corresponding category.
Given a training code and a training API pattern,
code patterns in the training API pattern is matched against the code. 
If all the code patterns succeed to match, the code is classified as 
the corresponding training API category.
If one of the code patterns fails to match, the code is not classified
as the category.

Each training API category has a corresponding training API pattern.
To categorize a training code into a training API category,
the code AST is matched against total four training API patterns.
If the code has exactly one match with the pattern,
the code is categorized into the corresponding training API category.
If the code has no match or has more than one match with the patterns, 
the code is thought to be an erroneous training code; 
the transform software aborts with an error indicating
no appropriate training category is matched.

\subsection{Transformation Rule for Distributed TensorFlow ML model}

% breifly introduce the transform function concept

In this section, we describe the transformation rule that
transforms single-GPU based TensorFlow model codes into
multi-GPU based TensorFlow model codes.
We informally understand code transformation rule as
conditions to select the target code parts
and methods to actually transform them into another.
The methods include addition, modification, and deletion of
specific code parts.
Currently, code transformation for distributed ML training
is only described by set of examples and informal explanations.
In order to automate the code transformation process,
we need to formally define the code transformation rule
and ways to convert the definition into software implementation.
We propose a formal definition of code transformation rule
for distributed ML training, and implement the automatic code transformation
software based on the formal definition.

Code transformation is formally defined as a pure function from AST to AST.
We call this function a transform function.
We may define multiple transform functions that act on different
language constructs and use them to define other transform functions.
For example, in order to transform a Python code into another,
we define a transform function that takes Module AST and returns Module AST.
Inside the Module transform function definition,
we may use Statement transform function to transform statements
that compose the Module AST.

Together with the AST parameter, transform functions take and return
the environment parameter.
Environment parameters are used to store specific identifiers
and pass it to the other calls of the transform function.
For example, statement transform function frequently use 
the identifier bound to the TensorFlow module.
The module name first appears from the import statement.
The transform function call on the import statement stores
the TensorFlow module name on the environment and returns it.
Then the later function calls to other statements can
retrieve the TensorFlow module name from the environment parameter.

% explain transform function with example

\begin{lstlisting}[language=Python, caption = Original code example]
import tensorflow as tf
optimizer = tf.keras.optimizers.Adam(lr)\end{lstlisting}

\begin{lstlisting}[language=Python, caption = Transformed code example]
import tensorflow as tf
# import and init horovod module
import horovod.tensorflow as hvd
hvd_broadcast_done = False
hvd.init()
import tensorflow.keras as keras

# scale the learning rate
optimizer = tf.keras.optimizers.Adam(lr * hvd.size())
# wrap the optimizer object
optimizer = hvd.DistributedOptimizer(optimizer)
\end{lstlisting}

We first describe how the transform function works by example.
The above figures illustrate a pair of TensorFlow model codes
before and after the transformation.
Informally, three kinds of transformation occur in between;
1) import and initialize the horovod module;
2) scale the optimizer's learning rate by {\tt hvd.size()};
3) Wrap the optimzier with {\tt hvd.DistributedOptimizer}.

The original code parses into a module AST with a list of 2 statements,
an import statement and an assign statement.

\begin{lstlisting}[language=Scala]
Module(List(
  ImportStmt(Id('tensorflow'), Id('tf')),
  AssignStmt(
    Id('optimizer'), 
    CallExpr(
      Expr('tf.keras.optimizers.Adam'),
      Args(Id('lr'))
    )
  )
))
\end{lstlisting}

\begin{tabular}{lcl}
  \tmodule{[\nstmtsubs{1}, \nstmtsubs{2}] ~ \ntypignore} 
  & \kteq & \tsstmt{[\nstmtsubs{1}, \nstmtsubs{2}]}{\smodenvempty} \\

  & \kteq & \ktlet ~ \mul{\nstmtsubs{1}}$'$, \smodenvsubs{1} ~ \kteq ~ 
  \tstmt{\nstmtsubs{1}}{\smodenvempty} ~ \ktin \\

  & & \ktlet ~ \mul{\nstmtsubs{2}}$'$, \smodenvsubs{2} ~ \kteq ~ 
  \tstmt{\nstmtsubs{2}}{\smodenvsubs{1}} ~ \ktin \\

  & & (\mul{\nstmtsubs{1}}$'$ \ktconl ~ \mul{\nstmtsubs{2}}$'$) \\ 
\end{tabular}

The figure describes how the module transform function 
is evaluated on the example module AST.
The module transform function, $\fkmodule$
applies statement list transform function $\fksstmt$ to its statement list.
Then $\fksstmt$ applies the statement transform funciton $\fkstmt$
to each statement in the list.
In addition, $\fksstmt$ passes the environment parameter $\sigma$
from a $\fkstmt$ call to the next $\fkstmt$ call.

\begin{tabular}{rcll}
  \tstmt{\nstmtsubs{1}}{\smodenvempty} & \kteq & 
  \tstmt{{\tt import tensorflow as tf}}{\smodenvempty} & \\

  & \kteq & \tstmt{\kimport ~ \mul{\nalias}}{\smodenvempty} & 
  (a) Pattern matching the input \\

  & & {\tt matching} ~ ( & \\
  && \indent \kimport ~ \mul{\nalias} \kteq ~ 
  \kimport ~ [{\tt tensorflow} \kas ~ {\tt tf}], & \\
  && \indent \mul{\nalias} \kteq ~ [\naliassubs{1}], & \\ 
  && \indent \naliassubs{1} \kteq ~ {\tt tensorflow} \kas ~ {\tt tf} ~ ) & \\

  & \kteq & 
  \ktlet ~ \smodenvsubs{1} ~ \kteq ~ \taalias{[\naliassubs{1}]}{\smodenv} 
  \ktin & 
  (b) Check if the TensorFlow module imported \\
  && \ktif ~ \smodenvsubs{1} ~ \envsub ~ \smodenv ~ 
  \kteq ~ [\tflow $\mapsto$ \nid] ~ \ktthen &\\ 
  && ([\kimport ~ \naliassubs{1}, & \\
  && {\tt import horovod.tensorflow as hvd}, & \\
  && {\tt hvd\_broadcast\_done = False}, & \\
  && {\tt hvd.init()}], \smodenvsubs{1}) & \\
  
  & \kteq &
  ([\kimport ~ {\tt tensorflow} \kas ~ {\tt tf}, & 
  (c) Construct output statements \\
  && {\tt import horovod.tensorflow as hvd}, & \\
  && {\tt hvd\_broadcast\_done = False}, & \\
  && {\tt hvd.init()}], \smodenvsubs{1}) & \\

  \kwith ~ \smodenvsubs{1} 
  & \kteq & \talias{\naliassubs{1}}{\smodenvempty} & \\
  & = & \talias{{\tt tensorflow} \kas ~ {\tt tf}}{\smodenvempty} & \\ 
  & \kteq & \smodenvempty[\tflow $\mapsto$ {\tt tf}] & \\
  & \kteq & [\tflow $\mapsto$ {\tt tf}] & \\
\end{tabular}

The first $\fkstmt$ call receives the statement
{\tt import tensorflow as tf} as an input.
In part (a), the function first matches the input statement
with the pattern $\kimport ~ \mul{\nalias}$.
Transform functions use pattern matching to discriminate target ASTs
by their syntactic structure.
Part (b) specifies the pattern guard, which describes detailed condition
on the content of the input.
The pattern guard checks if the import statement
is importing the TensorFlow module.
In process, the alias transform function $\fkalias$ is applied on the
{\tt tensorflow as tf}.
Note that $\fkalias$ also stores the module name {\tt tf}
in the environment parameter as a form of mapping;
later calls on transform function can retreive the
TensorFlow module name from the returned environment, $\smodenvsubs{1}$.
Finally, the part (c) constructs the output statements
and returns it with $\smodenvsubs{1}$.

% TODO typesetting this!!!!!
\begin{tabular}{rcll}
  \tstmt{\nstmtsubs{2}}{\smodenvempty} & \kteq &
  \tstmt{{\tt optimizer = tf.keras.optimizers.Adam(lr)}}{\smodenvsubs{1}} &\\
  &\kteq&
  \tstmt{\nidsubs{r} \oassign 
  \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ 
  \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
  \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenvsubs{1}} &\\
  && {\tt matching} ~ ( 
  \indent\indent\indent\indent\indent (a) Pattern matching the input &\\
  && \indent \nidsubs{r} \kteq ~ {\tt optimizer}, &\\
  && \indent \nexprsubs{1} = {\tt tf.keras.optimizers.Adam}, &\\
  && \indent \nexprsubs{11}= {\tt lr}~) &\\

  &\kteq& \ktif ~ \smodenvsubs{1}(\tflow) ~ \kteq ~ \nidsubs{t} ~ 
  \indent\indent\indent\indent\indent
  (b) Check if it calls {\tt tf.keras.optimizers.Adam} &\\ 
  && \ktand ~ 
  \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{t}.keras.optimizers.Adam} ~ 
  \ktthen& \\

  && ([\nidsubs{r} \oassign \nexprsubs{1} 
  \sparen{\nexprsubs{11} {\tt * hvd.size()} ~ ... ~ \nexprsubs{1n} 
  ~\op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
  \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}], &\\
  && \smodenvsubs{1}[\optmizer $\mapsto$ \nidsubs{r}])&\\
  
  &\kteq& 
  ([{\tt optimizer = tf.keras.optimizers.Adam(lr * hvd.size())}], &\\   
  && \smodenvsubs{1}[\optmizer $\mapsto$ \nidsubs{r}])
  \indent\indent\indent\indent\indent
  (c) Construct output statements &\\ 
\end{tabular}

The second $\fkstmt$ receives the statement
{\tt optimizer = tf.keras.optimizers.Adam(lr)} as an input.
The part (a) pattern matches the input to the assign statement pattern
with right-hand side of function call expression.
The part (b) is a pattern guard that checks if
the function call is {\tt tf.keras.optimizers.Adam}.
Note that the guard refers to the environment parameter
to check the TensorFlow module identifier.
Because after the first $\fkstmt$ call,
the environment parameter stores the TensorFlow module identifier {\tt tf},
so the second $\fkstmt$ call can use the information to
check if the function call is indeed a constructor for TensorFlow
optimizer object.
In the part (c), the output statement is returned.
As specified in the transform function definition, 
the first argument expression is multiplied by {\tt hvd.size()}.

\begin{tabular}{rcll}
  \tmodule{[\nstmtsubs{1}, \nstmtsubs{2}] ~ \ntypignore} 
  &\kteq& (\mul{\nstmtsubs{1}}$'$ \ktconl ~ \mul{\nstmtsubs{2}}$'$) &\\ 
  &\kteq& 
  [\kimport ~ {\tt tensorflow} \kas ~ {\tt tf}, &\\ 
  && {\tt import horovod.tensorflow as hvd}, & \\
  && {\tt hvd\_broadcast\_done = False}, & \\
  && {\tt hvd.init()}, &\\
  && {\tt optimizer = tf.keras.optimizers.Adam(lr * hvd.size())}] &\\   
\end{tabular}

Finally, the output statement lists are concatenated as specified in the
$\fksstmt$, and the transformed module AST is constructed.

As described in the example, the transform functions are defined for
each language constructs. The functions are described in terms of
pattern matching against the input AST; the matching patterns and 
pattern guards select the target AST to transform, and
the output AST is constructed with the matched variables.

Each training API category defines a corresponding transform function.
Given a training code of the category, the corresponding module AST transform
function is applied to the target code AST with an empty environment parameter.
In following subsections, we describe the transform function for each
traninig API category. The paper only explains important parts of the 
definition; the full definition of the transform functions can be found
in the supplementary material.

\subsubsection{Transform function for {\tt Session} category}

\subsubsection{Transform function for {\tt MonitoredSession} category}

\subsubsection{Transform function for {\tt GradientTape} category}

\subsubsection{Transform function for {\tt Keras} category}
