\pagebreak
\section{Code Transformation}\label{sec:trans}

\subsection{Formalization of Transformation Rules}

In this section, we describe formalization of the transformation rule
for distributed DL traininig with set of code examples and formal notions
related to them.
There are two main points in design of the formalization.
First, we informally understand the code transformation rule as methods to
\textit{select} the parts of codes to modify and \textit{construct} new code by 
reusing parts of the selected code.
To automate the transformation process, we need code transformation in a form 
that is implements above selection and construction.
Second, as mentioned earlier, to correctly transform tranining codes with different
API patterns, we must apply different transformation rules.
The formalization should also describe different transformation rules
for different API patterns.

In this end, we define the formal transformation rule as a set of
function that takes ASTs as input and returns ASTs as output.
We call these functions \textit{transform function}.
We define a set of transform functions for each training API pattern,
defining total of four sets transform functions.
For the selection and construction of code parts,
we use \textit{pattern matching} as an argument of the transform function;
given an AST as input, the transform function will first match the
input against the argument pattern, then only proceed to apply itself
when the pattern matching is successful.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf

dataset = ...
model = ...
optim = tf.optimizers.Adam(0.001) 

for (x, y) in dataset.take(10000):
  with tf.GradientTape() as tape:
    pred = model(x)
    loss_value = loss(y, pred) 

  gradients = tape.gradient(loss_value, model.trainable_variables)
  optim.apply_gradient(zip(gradients, model.trainable_variable)\end{lstlisting}
    \caption{Original DL training code}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
import horovod.tensorflow as hvd

dataset = ...
model = ...
optim = tf.optimizers.Adam(0.001 * hvd.size()) 

for (x, y) in dataset.take(10000):
  with tf.GradientTape() as tape:
    pred = model(x)
    loss_value = loss(y, pred) 
  tape = hvd.DistributedGradientTape(tape)

  gradients = tape.gradient(loss_value, model.trainable_variables)
  optim.apply_gradient(zip(gradients, model.trainable_variable)\end{lstlisting}
    \caption{Distributed DL training code}
  \end{subfigure}
  \caption{Code examples}
  \label{fig:trans:ex}
\end{figure}

We describe examples of the transform function by illustrating the transform
functions applied in the code example in figure \ref{fig:trans:ex}.
Figure \ref{fig:trans:ex}(a) shows an example of the input single-GPU code and
figure \ref{fig:trans:ex}(b) shows the coresponding distributed code. 
Some unnecessary details of the codes are removed from the code.
There are three main parts that are modified.
First, a new import statement that imports {\tt horovod.tensorflow} modules
is added in line 2. 
The statement is added right after the original {\tt tensorflow} module
import statement.
Second, in line 6, an argument of the constructor function 
{\tt tf.optimizers.Adam} is modified. The newly changed argument multiplies
{\tt hvd.size()} onto the original argument.
Third, in line 12, a new statement that assigns the variable {\tt tape}
is added. The assignment wraps the original {\tt GradientTape} object with
the Horovod library function, {\tt hvd.DistributedGradientTape}.
Except the above mentioned parts, the original code is not modified.     

\begin{figure}[ht!]
  \centering

  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
    \end{lstlisting}
    \caption{Original DL training code: only TensorFlow module is imported.}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
import horovod.tensorflow as hvd
    \end{lstlisting}
    \caption{Distributed DL training code: Horovod module import statement is added.}
  \end{subfigure}
  \caption{Code examples}
  \label{fig:trans:ex01}
\end{figure}

Figure \ref{fig:trans:ex01} is the first transformed part of the code.
The transform function takes an import statement as an input,
identifies that the input statement imports the TensorFlow module,
then outputs a list of two statements, which are the original input 
import statement and the new import statement for the Horovod module. 
The transform function should be able to recognize that the input import
statement actually imports the TensorFlow module.
To acheive this, the transform function utilzes the current transform context.
The transform context stores set of important identifiers related to the
training process; while transforming the code in figure \ref{fig:trans:ex01}(a),
for instance, the identifier {\tt tf} for the TensorFlow
module is stored after line 1, and the identifier {\tt optim}
for the {\tt Optimizer} instance is stored after line 5.
Transform function can compare the transfrom context before the input statement
and just after the input statement to check whether a specific module
is imported in the input statement.

\begin{figure}[ht!]
  \centering
  \noindent
  \begin{tabular}{l}
   \typdesc{\fkstmt& : & \dstmt ~ $\rightarrow$ ~ \dmodenv ~ $\rightarrow$ ~ (\dstmt ~ list ~ $\times$ ~ \dmodenv)}\\
   \tstmt{\kimport ~ \mul{\nalias}}{\smodenv} = \\
    \inden \ktlet ~ \smodenvsubs{1} ~ \kteq ~ \taalias{\mul{\nalias}}{\smodenv} \ktin \\
    \inden \ktif ~ \smodenvsubs{1} ~ \envsub ~ \smodenv ~ \kteq ~ [\tflow $\mapsto$ \nid]\\ 
    \inden\ktthen \\
    \inden\hspace{1em} ([\kimport ~ \mul{\nalias}, \\
    \inden\hspace{1em} \kimport ~ {\tt horovod.tensorflow} \kas ~ {\tt hvd}, \smodenvsubs{1})\\
    \inden \ktelse~([\kimport ~ \mul{\nalias}], \smodenvsubs{1})
  \end{tabular}\\\vpar
  \caption{Transform function: adding Horovod module import statement}
  \label{fig:trans:fn01}
\end{figure}

Figure \ref{fig:trans:fn01} describes the transform function that applies the
transform in figure \ref{fig:trans:ex01}. 
The function specifies input argument with pattern that matches any import
statement; thus the transformation only targets import statements.
As explained earlier, the second and third lines
compare $\smodenv$, the transform context
before the input statement, with $\smodenvsubs{1}$, the transform context
just after the input statement, to check if the input statement
imports the TensorFlow module.
Note that the transform function return the new transform context
$\smodenvsubs{1}$ together with the statement list; this allows
transform functions to propagate the transform context from a call
to the next call.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
optim = tf.optimizers.Adam(0.001)\end{lstlisting}
    \caption{Original DL training code}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
optim = tf.optimizers.Adam(0.001 * hvd.size())\end{lstlisting}
    \caption{Distributed DL training code}
  \end{subfigure}
  \caption{Code examples}
  \label{fig:trans:ex02}
\end{figure}

Figure \ref{fig:trans:ex02} illustrates the second part of the transformed code.
The assign statement in \ref{fig:trans:ex02}(a) is transformed to
another assign statement in \ref{fig:trans:ex02}(b), where the argument for the
constructor {\tt tf.optimizers.Adam} is multiplied by the expression
{\tt hvd.size()}. 

\subsection{Description of Transform functions}

As mentioned earlier, training codes of different API patterns
require different transformation rules to correctly transform them.
This section explains formal transform functions for each API patterns.

\subsubsection{Transform function for Session pattern}

To correctly transform Session pattern trainig codes into corresponding
distributed training codes, the {\tt Optimizer} instance must be modified.
Figure \ref{fig:trans:sessiontrans} shows an pair of code examples
illustrating the required transformation for Session pattern case.
In line 1 of figure \ref{fig:trans:sessiontrans}(b),
the {\tt learning\_rate} argument is scaled by {\tt hvd.size()}.
Then the line 2 is added, which wraps the {\tt Optimizer} instance
by designated Horovod API.

\begin{figure}[ht!]
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
optimizer = tf.train.MomentumOptimizer(learning_rate = 0.01)

with tf.Session() as sess:
    for step in range(num_epochs): 
        sess.run(optimizer, feed_dict)
    \end{lstlisting}
    \caption{Original training code of Session pattern}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
optimizer = tf.train.MomentumOptimizer(learning_rate = 0.01 * hvd.size())
optimizer = hvd.DistributedOptimizer(optimizer)

with tf.Session() as sess:
    for step in range(num_epochs): 
        sess.run(optimizer, feed_dict)
    \end{lstlisting}
    \caption{Distributed training code of Session pattern}
  \end{subfigure}
  \caption{Code example of Session pattern training code}
  \label{fig:trans:sessiontrans}
\end{figure}

The above transformation is formalized as shown in the figure 
\ref{fig:trans:sessrule}. The transform function stated in the figure
matches statements that assigns a evaluation result of function call expression
on the right-hand side.
If an assign statement is matched, the pattern guard checks if the
callee function is the {\tt Optimizer} instance constructor function.
Note that the pattern guard checks this by subclass relation between
the function expression and {\tt tensorflow.keras.optimizers.Optimizer}.
This way, the pattern guard can accept class constructors for {\tt Optimizer}
instances, including both TensorFlow library and user-defined classes.

The transform function proceeds to return a sequence of two statements;
the first one is modified version of the input assign statement,
and the second one is additional assign statement for wrapping the optimizer instance.
The first statement modifies the original {\tt learning\_rate} argument
by constructing a new expression, {\tt \nexprsubs{2i} * hvd.size()}.
The transform function can easily construct new ASTs by using
the pattern variables so that the part of the input statement can be conveniently
referred. This is also true for building the second output statement, which
reused the {\tt \nidsubs{r}} to construct a new assign statment.

\begin{figure}[h]
\noindent
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} }{\smodenv} = \\
\inden \ktif ~ \nexprsubs{1} \ktsubty ~ {\tt tensorflow.keras.optimizers.Optimizer} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt learning\_rate} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i} {\tt * hvd.size()}\\
  \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm \\
  \inden\inden\inden {\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
\end{longtable}
  \caption{Session pattern transform function: Optimizer learning rate scaling and wrapping}
  \label{fig:trans:sessrule}
\end{figure}

\subsubsection{Transform function for MonitoredSession pattern}

The MonitoredSession pattern codes use {\tt MonitoredSession} instances
to automatically repeat training steps.
By default, a training step includes feeding an training data to the network,
computing gradients, and optimize the network parameters.
In addition the default actions, users can add \textit{hooks},
a custom action that automatically executes for each training step.
For example, a {\tt CheckpointSaverHook} instance can be added to
save the model status into a file at each step.

In distributed training with Horovod, one of the multiple training processes
must broadcast the global variables states to every other processes so that
they can synchronize the initial model states.
To correctly transform MonitoredSession pattern training codes into
distributed training codes, the {\tt MonitoredSession} instances should have
variable broadcast hooks.
As in figure \ref{fig:trans:monsesstrans}, for example,
the correct transformation appends {\tt BroadcastGlobalVariablesHook} to the
{\tt hooks} arguments.
The {\tt hooks} keyword argument gets the hook list for the training steps;
the transformed code adds the Horovod library hook that broadcases the
global variable at the first training step.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
with tf.train.MonitoredTrainingSession(hooks=hooks) as mon_sess:
    while not mon_sess.should_stop():
        mon_sess.run()
    \end{lstlisting}
    \caption{Original training code of MonitoredSession pattern}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
with tf.train.MonitoredTrainingSession(hooks=hooks.append(hvd.BroadcastGlobalVariablesHook(0)) as mon_sess:
    while not mon_sess.should_stop():
        mon_sess.run()
    \end{lstlisting}
    \caption{Distributed training code of MonitoredSession pattern}
  \end{subfigure}
  \caption{Code example of MonitoredSession pattern training code}
  \label{fig:trans:monsesstrans}
\end{figure}

We formalize the transformation for MonitoredSession pattern as illustrated in
figure \ref{fig:trans:monsessrule}. 
The transform function matches \textit{with item}, an expression that is 
assigned by the {\tt with} statement, which is a function call expression of
the {\tt MonitoredSession} class constructor.
Note that the pattern guard refers to the environment variable to
get the identifier of TensorFlow 1.x compatability module.
This ensures the transform function to correctly identify the
{\tt MonitoredSession} class constructor from the TensorFlow module.   

\begin{figure}[ht!]
 \noindent
  \begin{tabular}{l}
    \twithitem{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ 
                \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
    % monitored session
    \inden \ktif ~ \smodenv(\tflowc) ~ \kteq ~ \nidsubs{t} ~ \ktand \\
    \inden\inden \nexprsubs{1} ~ \kteq ~ 
                {\tt \nidsubs{t}.train.MonitoredTrainingSession} ~ \ktthen \\
    \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt hooks} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
    \inden\inden\inden(\nexprsubs{1} \sparen{\nexprsubs{11} ... 
              \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} \\
    \inden\inden\inden\inden ... \nidsubs{i} \oassign {\tt \nexprsubs{2i}.append(hvd.BroadcastGlobalVariablesHook(0))} \\
    \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k} }, \\
    \inden\inden\inden\inden \smodenv[$\msess$ $\mapsto$ \kas]) \\
  \end{tabular}\\\vpar
\caption{MonitoredSession pattern transform function: Modifying {\tt StopAtStepHook} instance}
  \label{fig:trans:monsessrule}
\end{figure}

\subsubsection{Transform function for GradientTape pattern}

To correctly transform GradientTape pattern training codes,
the {\tt GradientTape} instance should be modified for distributed training.
Figure \ref{fig:trans:gtapetrans} illustrates the transformation of
GradientTape pattern code to the distributed code.
As shown in the figure \ref{fig:trans:gtapetrans}(b), line 8,
the {\tt GradientTape} instance {\tt tape}
is wrapped by {\tt DistributedGradientTape}.
In addition to modifying the {\tt GradientTape} instance,
line 13 to 17 adds variable broadcasting codes in the distributed training
code. Note that the variable {\tt hvd\_broadcast\_done} is initialized as 
{\tt False} at line 3, 
then set to {\tt True} at line 17 where the variable broadcasting is done. 
We use the boolean variable {\tt hvd\_broadcast\_done} in GrdientTape pattern
distributed training code to ensure that variable broadcasting occurs exactly
once.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf

with tf.GradientTape() as tape:
    probs = model(images)
    loss_value = loss(labels, probs)

grads = tape.gradient(loss_value, model.trainable_variables)
opt.apply_gradients(zip(grads, model.trainable_variables))
    \end{lstlisting}
    \caption{Original training code of GradientTape pattern}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
import horovod.tensorflow as hvd
hvd_broadcast_done = False

with tf.GradientTape() as tape:
    probs = model(images)
    loss_value = loss(labels, probs)
tape = hvd.DistributedGradientTape(tape)
grads = tape.gradient(loss_value, model.trainable_variables)
id_new = zip(grads, model.trainable_variables)
opt.apply_gradients(id_new)

global hvd_broadcast_done
if not hvd_broadcast_done:
    hvd.broadcast_variables([x[1] for x in id_new], root_rank=0, )
    hvd.broadcast_variables(opt.variables(), root_rank=0, )
    hvd_broadcast_done = True
    \end{lstlisting}
    \caption{Distributed training code of GradientTape pattern}
  \end{subfigure}
  \caption{Code example of GradientTape pattern training code}
  \label{fig:trans:gtapetrans}
\end{figure}

Figure \ref{fig:trans:gtaperule} illustrates a GradientTape pattern
transform function.
The function is responsible of wrapping the
{\tt GradientTape} instance with the {\tt DistributedGradientTape} API.
As illustrated in the figure, the function matches {\tt with} statements.
The pattern \textit{with\_item} matches the pair of identifier and expression 
that {\tt with} statement newly creates. 
For example, the pattern matches a pair of the identifier {\tt tf} and the 
expression {\tt tf.GradientTape()} when line 3 of figure 
\ref{fig:trans:gtapetrans}(a) is given.
In the second to fourth line, the function utilizes the environment variable  
to check if the input {\tt with} statement creates the a new {\tt GradientTape}
instance.
If the statement does create a new {\tt GradientTape} instance,
then the function appends a new statement that reassigns the identifier
wrapped with the {\tt DistributedGraidentTape} API.

\begin{figure}[ht!]
\noindent
\begin{tabular}{l}
  \tstmt{\kwith ~ \nwithitem ~ \kcolon ~ \mul{\nstmt}}{\smodenv} = \\
  \inden \ktlet ~ \nwithitem$'$, \smodenvsubs{1} \kteq ~ \twwithitem{\nwithitem}{\smodenv} \ktin \\
  \inden \ktlet ~ \nstmt$'$, \smodenvsubs{2} \kteq ~ \tsstmt{\mul{\nstmt}}{\smodenvsubs{1}} \ktin \\
  \inden \ktif ~ \smodenvsubs{1} \envsub ~ \smodenv ~ \kteq ~ [\gtape $\mapsto$ \nid] ~ \ktthen\\
  \inden\inden ([\optypcomm ~ \kwith ~ \nwithitem$'$ ~ \kcolon ~ \mul{\nstmt}$'$, \\
  \inden\inden \nid ~ \oassign {\tt hvd.DistributedGradientTape(\nid)}], \smodenvsubs{2})\\
  \inden \ktelse ~ ([\kwith ~ \nwithitem$'$ ~ \kcolon ~ \mul{\nstmt}$'$], \smodenvsubs{2})
\end{tabular}
  \caption{GradientTape pattern transform function: wrapping {\tt GradientTape} instance}
  \label{fig:trans:gtaperule}
\end{figure}

Figure \ref{fig:trans:gtaperule2} illustrates another GradientTape pattern
transform function which is responsible of appending the variable broadcasting
code. The function matches statements that assigns the value of function call
results. Then the pattern guard checks if the called function is the
{\tt apply\_gradients} method of the {\tt Optimizer} instance.
Here, the guard retreives the name of the {\tt Optimizer} instance from the
environment variable. 
The transform function finally returns a fixed sequence of statements that
performs the variable broadcast on the model and optimizer state variables.
Note that we utilize the list comprehension to extract the second element
of each pair in the list expression \nidsubs{z} as line 15 in figure
\ref{fig:trans:gtapetrans}(b).

\begin{figure}[ht!]
\noindent
\begin{tabular}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} }{\smodenv} = \\
  \inden \ktif  ~ \smodenv(\optmizer) ~ \kteq ~ \nidsubs{t} ~ \ktand ~ \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{t}.apply\_gradients} ~ \ktthen\\
  \inden\inden \ktlet ~ \nidsubs{z} ~ \kteq ~ \newid ~ \ktin \\
  \inden\inden ([\nidsubs{z} ~ \oassign ~ \nexprsubs{11},\\
  \inden\inden \nidsubs{r} \oassign \nexprsubs{1} \sparen{\nidsubs{z} \nexprsubs{12} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} ,\\
  \inden\inden {\tt if not hvd\_broadcast\_done:} \\ 
  \inden\inden\inden [ {\tt hvd.broadcast\_variables([x[1] for x in \nidsubs{z}], root\_rank=0)}, \\
  \inden\inden\inden {\tt hvd.broadcast\_variables(\nidsubs{t}.variables(), root\_rank=0)}, \\
  \inden\inden\inden {\tt hvd\_broadcast\_done = True} ]\\
  \inden\inden ], \smodenv) \\


\end{tabular}
  \caption{GradientTape pattern transform function: adding variable broadcasting code}
  \label{fig:trans:gtaperule2}
\end{figure}

\subsubsection{Transform function for Keras pattern}

In the training codes matching the Keras pattern,
the {\tt Model} instances are created and used to invoke the
training process. The transform function first stores the {\tt Model} instance
variable name in the environment as described in figure \ref{fig:trans:ker}.
The AST pattern guard tests that the given function expression is a subclass
of {\tt tensorflow.keras.Model}, which corresponds to the class constructors
of subclass instances of {\tt tensorflow.keras.Model} class.

\begin{figure}[ht!]
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm}{\smodenv} = \\
  \inden \ktelif ~ \nexprsubs{1} ~ \ktsubty ~ {\tt tensorflow.keras.Model} ~ \ktthen\\
  \inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{2} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}], \smodenv[\tmodel \nidsubs{r}])\\
\end{longtable}
  \caption{Keras pattern transform function: storing the {\tt Model} instance}
  \label{fig:trans:ker}
\end{figure}

The training process is invoked with {\tt fit} method of the {\tt Model} instance.
For the training codes matching the Keras pattern, the variable broadcast should
be done by adding the {\tt hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)}
callback to the {\tt fit} method argument.
The transform function utilizes the environment parameter to identify the {\tt fit}
method call and insert the callback object to the argument.
Figure \ref{fig:trans:ker2} describes the transform function that
adds the callback to the {\tt fit} function call expression.

\begin{figure}[ht!]
\begin{longtable}{l}
  \tstmt{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
  \inden \ktelif ~ \nidsubs{m} ~ \kteq ~ \smodenv(\tmodel) ~ \ktand ~ 
          \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{m}.fit} ~ \ktthen \\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt callbacks} ~ \ktwhen ~ 2 $\leq$ i $\leq$ k ~ \ktthen \\
  \inden\inden\inden ([{\tt callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)]},\\
  \inden\inden\inden ~ {\tt if hvd.rank() == 0: callbacks.append(\nexprsubs{2i})}, \\
  \inden\inden\inden ~ {\tt \nexprsubs{1} (optim ... \nexprsubs{1n}}
                              \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                              {\tt callbacks \oassign callbacks} ... 
                              \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}{\tt )}], \smodenv) \\
  \inden\inden \ktelse \\
  \inden\inden\inden ([{\tt callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)]},\\
  \inden\inden\inden ~ {\tt \nexprsubs{1} (optim ... \nexprsubs{1n}}
                              \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                              \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}...
                              {\tt callbacks \oassign callbacks} {\tt )}], \smodenv) \\ 
\end{longtable}
  \caption{Keras pattern transform function: adding the broadcast callback}
  \label{fig:trans:ker2}
\end{figure}

The Keras pattern transform function also modifies the {\tt Optimizer} instance.
First, the learning rate parameter of the instance should be scaled by
{\tt hvd.size()}. Second, the instance should be wrapped with
{\tt hvd.Distributed Optimizer} function, similar to the GradientTape pattern's case.
Figure \ref{fig:trans:ker3} describes the transform function modifying
the {\tt Optimizer} instance.

\begin{figure}[ht!]
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm}{\smodenv} = \\
  \inden \ktelif ~ \nexprsubs{1} \ktsubty ~ {\tt tensorflow.keras.optimizers.Optimizer} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt learning\_rate} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i} {\tt * hvd.size()}\\
  \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm \\
  \inden\inden\inden {\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
  \inden\inden \ktelse \\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11}
  {\tt * hvd.size()} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)}
\nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm, \\
  \inden\inden\inden{\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
\end{longtable}
  \caption{Keras pattern transform function: modifying the {\tt Optimizer} instance}
  \label{fig:trans:ker3}
\end{figure}
