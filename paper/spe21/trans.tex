\section{Code Transformation}\label{sec:trans}
\subsection{Transformation Rule for Distributed TensorFlow ML model}

In this section, we describe the transformation rule that
transforms single-GPU based TensorFlow model codes into
multi-GPU based TensorFlow model codes.
We informally understand code transformation rule as
conditions to select the target code parts
and methods to actually transform them into another.
The methods include addition, modification, and deletion of
specific code parts.
Currently, code transformation for distributed ML training
is only described by set of examples and informal explanations.
In order to automate the code transformation process,
we need to formally define the code transformation rule
and ways to convert the definition into software implementation.
We propose a formal definition of code transformation rule
for distributed ML training, and implement the automatic code transformation
software based on the formal definition.

Code transformation is formally defined as a pure function from AST to AST.
We call this function a transform function.
We may define multiple transform functions that act on different
language constructs and use them to define other transform functions.
For example, in order to transform a Python code into another,
we define a transform function that takes Module AST and returns Module AST.
Inside the Module transform function definition,
we may use Statement transform function to transform statements
that compose the Module AST.

Together with the AST parameter, transform functions take and return
the environment parameter.
Environment parameters are used to store specific identifiers
and pass it to the other calls of the transform function.
For example, statement transform function frequently use 
the identifier bound to the TensorFlow module.
The module name first appears from the import statement.
The transform function call on the import statement stores
the TensorFlow module name on the environment and returns it.
Then the later function calls to other statements can
retrieve the TensorFlow module name from the environment parameter.

% explain transform function with example

\begin{lstlisting}[language=Python, caption = Original code example]
import tensorflow as tf
optimizer = tf.keras.optimizers.Adam(lr)\end{lstlisting}

\begin{lstlisting}[language=Python, caption = Transformed code example]
import tensorflow as tf
# import and init horovod module
import horovod.tensorflow as hvd
hvd_broadcast_done = False
hvd.init()
import tensorflow.keras as keras

# scale the learning rate
optimizer = tf.keras.optimizers.Adam(lr * hvd.size())
# wrap the optimizer object
optimizer = hvd.DistributedOptimizer(optimizer)
\end{lstlisting}

We first describe how the transform function works by example.
The above figures illustrate a pair of TensorFlow model codes
before and after the transformation.
Informally, three kinds of transformation occur in between;
1) import and initialize the horovod module;
2) scale the optimizer's learning rate by {\tt hvd.size()};
3) Wrap the optimzier with {\tt hvd.DistributedOptimizer}.

The original code parses into a module AST with a list of 2 statements,
an import statement and an assign statement.

\begin{lstlisting}[language=Scala]
Module(List(
  ImportStmt(Id('tensorflow'), Id('tf')),
  AssignStmt(
    Id('optimizer'), 
    CallExpr(
      Expr('tf.keras.optimizers.Adam'),
      Args(Id('lr'))
    )
  )
))
\end{lstlisting}

\begin{tabular}{lcl}
  \tmodule{[\nstmtsubs{1}, \nstmtsubs{2}] ~ \ntypignore} 
  & \kteq & \tsstmt{[\nstmtsubs{1}, \nstmtsubs{2}]}{\smodenvempty} \\

  & \kteq & \ktlet ~ \mul{\nstmtsubs{1}}$'$, \smodenvsubs{1} ~ \kteq ~ 
  \tstmt{\nstmtsubs{1}}{\smodenvempty} ~ \ktin \\

  & & \ktlet ~ \mul{\nstmtsubs{2}}$'$, \smodenvsubs{2} ~ \kteq ~ 
  \tstmt{\nstmtsubs{2}}{\smodenvsubs{1}} ~ \ktin \\

  & & (\mul{\nstmtsubs{1}}$'$ \ktconl ~ \mul{\nstmtsubs{2}}$'$) \\ 
\end{tabular}

The figure describes how the module transform function 
is evaluated on the example module AST.
The module transform function, $\fkmodule$
applies statement list transform function $\fksstmt$ to its statement list.
Then $\fksstmt$ applies the statement transform funciton $\fkstmt$
to each statement in the list.
In addition, $\fksstmt$ passes the environment parameter $\sigma$
from a $\fkstmt$ call to the next $\fkstmt$ call.

\begin{tabular}{rcll}
  \tstmt{\nstmtsubs{1}}{\smodenvempty} & \kteq & 
  \tstmt{{\tt import tensorflow as tf}}{\smodenvempty} & \\

  & \kteq & \tstmt{\kimport ~ \mul{\nalias}}{\smodenvempty} & 
  (a) Pattern matching the input \\

  & & {\tt matching} ~ ( & \\
  && \indent \kimport ~ \mul{\nalias} \kteq ~ 
  \kimport ~ [{\tt tensorflow} \kas ~ {\tt tf}], & \\
  && \indent \mul{\nalias} \kteq ~ [\naliassubs{1}], & \\ 
  && \indent \naliassubs{1} \kteq ~ {\tt tensorflow} \kas ~ {\tt tf} ~ ) & \\

  & \kteq & 
  \ktlet ~ \smodenvsubs{1} ~ \kteq ~ \taalias{[\naliassubs{1}]}{\smodenv} 
  \ktin & 
  (b) Check if the TensorFlow module imported \\
  && \ktif ~ \smodenvsubs{1} ~ \envsub ~ \smodenv ~ 
  \kteq ~ [\tflow $\mapsto$ \nid] ~ \ktthen &\\ 
  && ([\kimport ~ \naliassubs{1}, & \\
  && {\tt import horovod.tensorflow as hvd}, & \\
  && {\tt hvd\_broadcast\_done = False}, & \\
  && {\tt hvd.init()}], \smodenvsubs{1}) & \\
  
  & \kteq &
  ([\kimport ~ {\tt tensorflow} \kas ~ {\tt tf}, & 
  (c) Construct output statements \\
  && {\tt import horovod.tensorflow as hvd}, & \\
  && {\tt hvd\_broadcast\_done = False}, & \\
  && {\tt hvd.init()}], \smodenvsubs{1}) & \\

  \kwith ~ \smodenvsubs{1} 
  & \kteq & \talias{\naliassubs{1}}{\smodenvempty} & \\
  & = & \talias{{\tt tensorflow} \kas ~ {\tt tf}}{\smodenvempty} & \\ 
  & \kteq & \smodenvempty[\tflow $\mapsto$ {\tt tf}] & \\
  & \kteq & [\tflow $\mapsto$ {\tt tf}] & \\
\end{tabular}

The first $\fkstmt$ call receives the statement
{\tt import tensorflow as tf} as an input.
In part (a), the function first matches the input statement
with the pattern $\kimport ~ \mul{\nalias}$.
Transform functions use pattern matching to discriminate target ASTs
by their syntactic structure.
Part (b) specifies the pattern guard, which describes detailed condition
on the content of the input.
The pattern guard checks if the import statement
is importing the TensorFlow module.
In process, the alias transform function $\fkalias$ is applied on the
{\tt tensorflow as tf}.
Note that $\fkalias$ also stores the module name {\tt tf}
in the environment parameter as a form of mapping;
later calls on transform function can retreive the
TensorFlow module name from the returned environment, $\smodenvsubs{1}$.
Finally, the part (c) constructs the output statements
and returns it with $\smodenvsubs{1}$.

% TODO typesetting this!!!!!
\begin{tabular}{rcll}
  \tstmt{\nstmtsubs{2}}{\smodenvempty} & \kteq &
  \tstmt{{\tt optimizer = tf.keras.optimizers.Adam(lr)}}{\smodenvsubs{1}} &\\
  &\kteq&
  \tstmt{\nidsubs{r} \oassign 
  \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ 
  \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
  \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenvsubs{1}} &\\
  && {\tt matching} ~ ( 
  \indent\indent\indent\indent\indent (a) Pattern matching the input &\\
  && \indent \nidsubs{r} \kteq ~ {\tt optimizer}, &\\
  && \indent \nexprsubs{1} = {\tt tf.keras.optimizers.Adam}, &\\
  && \indent \nexprsubs{11}= {\tt lr}~) &\\

  &\kteq& \ktif ~ \smodenvsubs{1}(\tflow) ~ \kteq ~ \nidsubs{t} ~ 
  \indent\indent\indent\indent\indent
  (b) Check if it calls {\tt tf.keras.optimizers.Adam} &\\ 
  && \ktand ~ 
  \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{t}.keras.optimizers.Adam} ~ 
  \ktthen& \\

  && ([\nidsubs{r} \oassign \nexprsubs{1} 
  \sparen{\nexprsubs{11} {\tt * hvd.size()} ~ ... ~ \nexprsubs{1n} 
  ~\op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
  \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}], &\\
  && \smodenvsubs{1}[\optmizer $\mapsto$ \nidsubs{r}])&\\
  
  &\kteq& 
  ([{\tt optimizer = tf.keras.optimizers.Adam(lr * hvd.size())}], &\\   
  && \smodenvsubs{1}[\optmizer $\mapsto$ \nidsubs{r}])
  \indent\indent\indent\indent\indent
  (c) Construct output statements &\\ 
\end{tabular}

The second $\fkstmt$ receives the statement
{\tt optimizer = tf.keras.optimizers.Adam(lr)} as an input.
The part (a) pattern matches the input to the assign statement pattern
with right-hand side of function call expression.
The part (b) is a pattern guard that checks if
the function call is {\tt tf.keras.optimizers.Adam}.
Note that the guard refers to the environment parameter
to check the TensorFlow module identifier.
Because after the first $\fkstmt$ call,
the environment parameter stores the TensorFlow module identifier {\tt tf},
so the second $\fkstmt$ call can use the information to
check if the function call is indeed a constructor for TensorFlow
optimizer object.
In the part (c), the output statement is returned.
As specified in the transform function definition, 
the first argument expression is multiplied by {\tt hvd.size()}.

\begin{tabular}{rcll}
  \tmodule{[\nstmtsubs{1}, \nstmtsubs{2}] ~ \ntypignore} 
  &\kteq& (\mul{\nstmtsubs{1}}$'$ \ktconl ~ \mul{\nstmtsubs{2}}$'$) &\\ 
  &\kteq& 
  [\kimport ~ {\tt tensorflow} \kas ~ {\tt tf}, &\\ 
  && {\tt import horovod.tensorflow as hvd}, & \\
  && {\tt hvd\_broadcast\_done = False}, & \\
  && {\tt hvd.init()}, &\\
  && {\tt optimizer = tf.keras.optimizers.Adam(lr * hvd.size())}] &\\   
\end{tabular}

Finally, the output statement lists are concatenated as specified in the
$\fksstmt$, and the transformed module AST is constructed.

As described in the example, the transform functions are defined for
each language constructs. The functions are described in terms of
pattern matching against the input AST; the matching patterns and 
pattern guards select the target AST to transform, and
the output AST is constructed with the matched variables.

Each training API category defines a corresponding transform function.
Given a training code of the category, the corresponding module AST transform
function is applied to the target code AST with an empty environment parameter.
In following subsections, we describe the transform function for each
traninig API category. The paper only explains important parts of the 
definition; the full definition of the transform functions can be found
in the supplementary material.

\subsection{Description of Transform functions}

\subsubsection{Transform function for Session pattern}

\subsubsection{Transform function for MonitoredSession pattern}

\subsubsection{Transform function for GradientTape pattern}

In training codes matched in the GradientTape pattern,
the {\tt GradientTape} instance is wrapped with
{\tt hvd.DistributedGradientTape} function.
The {\tt GradientTape} instances are created by {\tt with} statements
in the training codes, which automatically invokeds initialization
and destruction functions before and after the {\tt with} statement scope.
The transform function matches the {\tt with} statement that
creates the {\tt GradientTape} instance and insert the
wrapping statement after it.
Figure \ref{fig:trans:gtape} illustrates the transform function definition
regarding the {\tt GradientTape} wrapping.

\begin{figure}[h]
\noindent
\begin{tabular}{l}
  \tstmt{\optypcomm ~ \kwith ~ \mul{\nwithitem} ~ \kcolon ~ \mul{\nstmt}}{\smodenv} = \\
  \inden \ktlet ~ \mul{\nwithitem}$'$, \smodenvsubs{1} \kteq ~ \twwithitem{\mul{\nwithitem}}{\smodenv} \ktin \\
  \inden \ktlet ~ \mul{\nstmt}$'$, \smodenvsubs{2} \kteq ~ \tsstmt{\mul{\nstmt}}{\smodenvsubs{1}} \ktin \\
  \inden \ktif ~ \smodenvsubs{1} \envsub ~ \smodenv ~ \kteq ~ [\gtape $\mapsto$ \nid] ~ \ktthen\\
  \inden\inden ([\optypcomm ~ \kwith ~ \mul{\nwithitem}$'$ ~ \kcolon ~ \mul{\nstmt}$'$, \\
  \inden\inden \nid ~ \oassign {\tt hvd.DistributedGradientTape(\nid)}], \smodenvsubs{2})\\
  \inden \ktelse ~ ([\optypcomm ~ \kwith ~ \mul{\nwithitem}$'$ ~ \kcolon ~ \mul{\nstmt}$'$], \smodenvsubs{2})
\end{tabular}
  \caption{GradientTape pattern transform function: wrapping {\tt GradientTape} instance}
  \label{fig:trans:gtape}
\end{figure}

In addition, the variable broadcast should be manually done with
{\tt hvd.broadcast\_variables} function.
Because the variable broadcast should done exactly once at the start of the
training, we utilize a global boolean variable {\tt hvd\_broadcast\_done}
that tracks whether the broadcast is already performed or not.
The variable {\tt hvd\_broadcast\_done} is declared within the Horovod
initialization codes and initialized to {\tt False} as described in
the figure \ref{fig:trans:gtape2}.

\begin{figure}[h]
\begin{tabular}{l}
  \tstmt{\kimport ~ \mul{\nalias}}{\smodenv} = \\
  \inden \ktlet ~ \smodenvsubs{1} ~ \kteq ~ \taalias{\mul{\nalias}}{\smodenv} \ktin \\
  \inden \ktif ~ \smodenvsubs{1} ~ \envsub ~ \smodenv ~ \kteq ~ [\tflow $\mapsto$ \nid] ~ \ktthen \\
  \inden\hspace{1em} ([\kimport ~ \mul{\nalias}, \\
  \inden\hspace{1em} \kimport ~ {\tt horovod.tensorflow} \kas ~ {\tt hvd}, \\
  \inden\hspace{1em} {\tt hvd\_broadcast\_done} \oassign {\tt False}, \\
  \inden\hspace{1em} {\tt hvd.init()}, \\
  \inden\hspace{1em} {\tt gpus = \nid.config.experimental.list\_physical\_devices(`GPU')}, \\
  \inden\hspace{1em} {\tt for gpu in gpus: \nid.config.experimental.set\_memory\_growth(gpu, True)},\\
  \inden\hspace{1em} {\tt if gpus: \nid.config.experimental.set\_visible\_devices(gpus[hvd.local\_rank()], `GPU')}], \smodenvsubs{1})\\
  \inden \ktelse~([\kimport ~ \mul{\nalias}], \smodenvsubs{1})
\end{tabular}
  \caption{GradientTape pattern transform function: initializing {\tt hvd\_broadcast\_done}}
  \label{fig:trans:gtape2}
\end{figure}

The statements that perform the variable broadcast are inserted after
the {\tt Optimizer} instance applies the training gradient.
Corresponding transform function is described in figure \ref{fig:trans:gtape3}.
The inserted codes check and modify the boolean value of 
{\tt hvd\_broadcast\_done} to ensure that the variable broadcast
happens only once.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
  % variable broadcasting
  \inden \ktif  ~ \smodenv(\optmizer) ~ \kteq ~ \nidsubs{t} ~ \ktand ~ \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{t}.apply\_gradients} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt grads\_and\_vars} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden \ktlet ~ \nidsubs{z} ~ \kteq ~ \newid ~ \ktin \\
  \inden\inden\inden ([\nidsubs{z} ~ \oassign ~ \nexprsubs{2i},\\
  \inden\inden\inden \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nidsubs{z} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}},\\
  \inden\inden\inden {\tt global hvd\_broadcast\_done}, \\
  \inden\inden\inden {\tt if not hvd\_broadcast\_done:} [ {\tt hvd.broadcast\_variables([x[1] for x in \nidsubs{z}], root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd.broadcast\_variables(\nidsubs{t}.variables(), root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd\_broadcast\_done = True} ]], \smodenv) \\
  \inden\inden \ktelse \\
  \inden\inden\inden \ktlet ~ \nidsubs{z} ~ \kteq ~ \newid ~ \ktin \\
  \inden\inden\inden ([\nidsubs{z} ~ \oassign ~ \nexprsubs{11},\\
  \inden\inden\inden \nexprsubs{1} \sparen{\nidsubs{z} \nexprsubs{12} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}},\\
  \inden\inden\inden {\tt global hvd\_broadcast\_done}, \\
  \inden\inden\inden {\tt if not hvd\_broadcast\_done:} [ {\tt hvd.broadcast\_variables([x[1] for x in \nidsubs{z}], root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd.broadcast\_variables(\nidsubs{t}.variables(), root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd\_broadcast\_done = True} ]], \smodenv) \\
\end{longtable}
  \caption{GradientTape pattern transform fuinction: the variable broadcast}
  \label{fig:trans:gtape3}
\end{figure}
 
\subsubsection{Transform function for Keras pattern}

In the tranining codes matching the Keras pattern,
the {\tt Model} instances are created and used to invoke the
training process. The transform function first stores the {\tt Model} instance
variable name in the environment as described in the figure \ref{fig:trans:ker}.
The AST pattern guard tests that the given function expression is a subclass
of {\tt tensorflow.keras.Model}, which corresponds to the class constructors
of subclass instances of {\tt tensorflow.keras.Model} class.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm}{\smodenv} = \\
  \inden \ktelif ~ \nexprsubs{1} ~ \ktsubty ~ {\tt tensorflow.keras.Model} ~ \ktthen  \\
  \inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{2} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}], \smodenv[\model $\mapsto$ \nidsubs{r}])\\
\end{longtable}
  \caption{Keras pattern transform function: storing the {\tt Model} instance}
  \label{fig:trans:ker}
\end{figure}

The tranining process is invoked with {\tt fit} method of the {\tt Model} instance.
For the training codes matching the Keras pattern, the variable broadcast should
be done by adding the {\tt hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)}
callback to the {\tt fit} method argument.
The transform function utilizes the environment parameter to identify the {\tt fit}
method call and insert the callback object to the argument.
The figure \ref{fig:trans:ker2} describes the transform function that
adds the callback to the {\tt fit} function call expression.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
  \inden \ktelif ~ \nidsubs{m} ~ \kteq ~ \smodenv(\model) ~ \ktand ~ 
          \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{m}.fit} ~ \ktthen \\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt callbacks} ~ \ktwhen ~ 2 $\leq$ i $\leq$ k ~ \ktthen \\
  \inden\inden\inden ([{\tt callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)]},\\
  \inden\inden\inden ~ {\tt if hvd.rank() == 0: callbacks.append(\nexprsubs{2i})}, \\
  \inden\inden\inden ~ {\tt \nexprsubs{1} (optim ... \nexprsubs{1n}}
                              \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                              {\tt callbacks \oassign callbacks} ... 
                              \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}{\tt )}], \smodenv) \\
  \inden\inden \ktelse \\
  \inden\inden\inden ([{\tt callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)]},\\
  \inden\inden\inden ~ {\tt \nexprsubs{1} (optim ... \nexprsubs{1n}}
                              \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                              \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}...
                              {\tt callbacks \oassign callbacks} {\tt )}], \smodenv) \\ 
\end{longtable}
  \caption{Keras pattern transform function: adding the broadcast callback}
  \label{fig:trans:ker2}
\end{figure}

The Keras pattern transform function also modifies the {\tt Optimizer} instance.
First, the learning rate parameter of the instance should be scaled by
{\tt hvd.size()}. Second, the instance should be wrapped with
{\tt hvd.Distributed Optimizer} function, similar to the GradientTape pattern's case.
The firgure \ref{fig:trans:ker3} describes the transform function modifying
the {\tt Optimizer} instance.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm}{\smodenv} = \\
  \inden \ktelif ~ \nexprsubs{1} \ktsubty ~ {\tt tensorflow.keras.optimizers.Optimizer} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt learning\_rate} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i} {\tt * hvd.size()}\\
  \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm \\
  \inden\inden\inden {\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
  \inden\inden \ktelse \\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11}
  {\tt * hvd.size()} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)}
\nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm, \\
  \inden\inden\inden{\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
\end{longtable}
  \caption{Keras pattern transform function: modifying the {\tt Optimizer} instance}
  \label{fig:trans:ker3}
\end{figure}
