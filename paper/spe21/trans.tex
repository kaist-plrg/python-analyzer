pagebreak
\section{Code Transformation}\label{sec:trans}

\subsection{Formalization of the Transformation Rules}

% todo: graphical issue in this figure image
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.7\textwidth]{transfn_expl.pdf}
    \caption{Explanation of Transform Function Description}
    \label{fig:trans:fnexpl}
\end{figure}

% new
We formalize the transformation rule as \textit{transform functions}.
Transform functions are the pure functions that takes an input AST and
returns the output ASTs.
By applying the transform function to the single-GPU model AST,
we can get the transformed multi-GPU model AST.
% todo: better connection here?
However, the transform functions cannot utilize the contextual information
by only getting ASTs as inputs.
For instance, the transform function that inserts the {\tt Optimizer} class 
method call uses the variable identifier for the instance,
which should be retrieved from the previous statements in the context.
To solve this problem, the transform functions get and return
the \textit{transform context} objects as inputs and outputs.
The transform context is a finite mapping from strings to identifiers, which
stores arbitrary identifiers from the code and propagate them to the 
next transform function.
When transform functions are sequentially applied to sequence of ASTs, 
the output transform context from the previous call is given as the input
to the next tranform function call.
By this method, the transform functions can utilize contextual information
during transformation and propagate the context to next transform functions.

% transform function format expl
We describe the transform function as the collection of the partial transform
function. 
The figure~\ref{fig:trans:fnexpl} illustrates an example of the
partial transform function.
We use the AST pattern in the input parameter position.
The input AST pattern means that the partial function only applies
to the input ASTs that match the pattern.
The input ASTs taht does not match the pattern will not be modified.
For instance, the function in the figure~\ref{fig:trans:fnexpl} only matches
the {\tt import} statements and transform them. 
The function also takes the transform context $\smodenv$ as an input.
The function body defines the output expression. 
We use complex expressions such as {\tt LET-IN} or {\tt IF-THEN-ELSE} to 
check and specify conditions to return different outputs.
In the figure~\ref{fig:trans:fnexpl}, 
the function body uses {\tt IF-THEN-ELSE} expression to check if the 
{\tt import} statement imports the {\tt tensorflow} module, 
and returns different ASTs according to the condition.

\pagebreak
\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf

dataset = ...
model = ...
optim = tf.optimizers.Adam(0.001) 

for (x, y) in dataset.take(10000):
  with tf.GradientTape() as tape:
    pred = model(x)
    loss_value = loss(y, pred)\end{lstlisting} 
    \caption{Original DL training code}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
import horovod.tensorflow as hvd

dataset = ...
model = ...
optim = tf.optimizers.Adam(0.001 * hvd.size()) 

for (x, y) in dataset.take(10000):
  with tf.GradientTape() as tape:
    pred = model(x)
    loss_value = loss(y, pred) 
  tape = hvd.DistributedGradientTape(tape)\end{lstlisting}
    \caption{Distributed DL training code}
  \end{subfigure}
  \caption{Example of original DL model code transformed into the distributed model}
  \label{fig:trans:ex}
\end{figure}

We explain the details on the transform functions by code example in
the figure \ref{fig:trans:ex}. The figure is the pair of single-GPU model
and the corresponding multi-GPU model.
There are three transformed parts of the code. 
First, an import statement for the Horovod module is added in the line 2. 
The statement is added right after the {\tt tensorflow} module import statement.
Second, in the line 6, the argument of the {\tt Optimizer} instance constructor
is modified. The new argument expression multiplies {\tt hvd.size()} 
to the original argument expression.
Third, in the line 12, the statement that assigns the variable {\tt tape}
is added. The assignment wraps the original {\tt GradientTape} object with
the Horovod library function, {\tt hvd.DistributedGradientTape}.

\begin{figure}[ht!]
    \centering
    % org
    \begin{subfigure}[b]{0.48\textwidth}
        \begin{lstlisting}[language=Python]
import tensorflow as tf\end{lstlisting}
        \caption{Original DL training code: only TensorFlow module is imported.}
        \label{fig:trans:ex01:org}
    \end{subfigure}
    ~
    % hvd
    \begin{subfigure}[b]{0.48\textwidth}
        \begin{lstlisting}[language=Python]
import tensorflow as tf
import horovod.tensorflow as hvd\end{lstlisting}
        \caption{Distributed DL training code: Horovod module import statement is added.}
        \label{fig:trans:ex01:hvd}
    \end{subfigure}
    % fn
    \begin{subfigure}[t]{\textwidth}
        \centering
        \begin{tabular}{l}
            \typdesc{\fkstmt& : & \dstmt ~ $\rightarrow$ ~ \dmodenv ~ 
            $\rightarrow$ ~ (\dstmt ~ list ~ $\times$ ~ \dmodenv)}\\
            \tstmt{\kimport ~ \mul{\nalias}}{\smodenv} = \\
            \inden \ktlet ~ \smodenvsubs{1} ~ \kteq ~ \taalias{\mul{\nalias}}{\smodenv} \ktin \\
            \inden \ktif ~ \smodenvsubs{1} ~ \envsub ~ \smodenv ~ \kteq ~ [\tflow $\mapsto$ \nid]\\ 
            \inden\ktthen \\
            \inden\hspace{1em} ([\kimport ~ \mul{\nalias},
            \kimport ~ {\tt horovod.tensorflow} \kas ~ {\tt hvd}], \smodenvsubs{1})\\
            \inden \ktelse~([\kimport ~ \mul{\nalias}], \smodenvsubs{1})
\end{tabular}\\\vpar

        \caption{The Partial Transform function}
        \label{fig:trans:ex01:fn}
    \end{subfigure}

    \caption{Transformation of Adding the Horovod {\tt import} Statement}
    \label{fig:trans:ex01}
\end{figure}


Figure \ref{fig:trans:ex01} describes the first transformed part of the code
and the partial transform function responsible of the transformation.
The input AST pattern matches the {\tt import} statements. 
The transform function should be able to recognize that the input import
statement imports the TensorFlow module.
To acheive this, the transform function utilzes the transform context.
The line 2 in the figure~\ref{fig:trans:ex01:fn} first computes the
$\smodenvsubs{1}$, which is the transfom context that includes the aliases
importes by the input statement.
Then the line 3 compute the difference between $\smodenvsubs{1}$ 
and the input transform context $\smodenv$.
If the difference includes the new entry for string {\tt "tensor\_flow"},
it means that the {\tt import} statement imports the TensorFlow module.
In this case, the transform function adds the Horovod module {\tt import} 
statement to the output AST.

\pagebreak
\begin{figure}[ht!]
    \centering

    \begin{subfigure}[t]{0.48\textwidth}
        \begin{lstlisting}[language=Python]
        optim = tf.optimizers.Adam(0.001)\end{lstlisting}
        \caption{Original DL training code}
    \end{subfigure}

    \hspace{5mm}

    \begin{subfigure}[t]{0.48\textwidth}
        \begin{lstlisting}[language=Python]
        optim = tf.optimizers.Adam(0.001 * hvd.size())\end{lstlisting}
        \caption{Distributed DL training code}
    \end{subfigure}

    \begin{subfigure}[t]{\textwidth}
        \centering
        \begin{tabular}{l}

            \typdesc{\fkstmt& : & \dstmt ~ $\rightarrow$ ~ \dmodenv ~ $\rightarrow$ ~ (\dstmt ~ list ~ $\times$ ~ \dmodenv)}\\

            \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n}} }{\smodenv} = \\

            \inden \ktif ~ \nexprsubs{1} \ktsubtysubs{\smodenv} ~ {\tt tensorflow.keras.optimizers.Optimizer}\\
            \inden \ktthen \\

            \inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11}
            {\tt * hvd.size()} ... \nexprsubs{1n}}], \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\

        \end{tabular}
        \caption{Transform function: scaling the Optimizer instance argument}
        \label{fig:trans:fn02}
    \end{subfigure}
    \caption{Code examples}
    \label{fig:trans:ex02}
\end{figure}


The figure \ref{fig:trans:ex02} illustrates the second part of the transformed 
code, and the figure \ref{fig:trans:fn02} describes the transform function 
that applies the transformation in figure \ref{fig:trans:ex02}.
The learning rate argument for the {\tt Optimizer} class constructor is 
multiplied by the expression {\tt hvd.size()}. 
To apply the transformation, the transform function must first recognize a
assign statement that creates an {\tt Optimizer} instance.
One caveat here is that the {\tt Optimizer} instance can be 
created by not only TensorFlow library APIs, but also
user-defined class constructors. We utilize the class inheritance information to
recognize the constructors that creates any subclasses of the {\tt Optimizer}
class. 
After identifying the {\tt Optimizer} class assign statement, 
the transform function modifies the argument expression of the
constructor call.
The transform function would take the original argument expression,
then create a new expression that multiplies {\tt hvd.size()}
To modify the argument expression, the function body utilizes the AST pattern
variable $\nexprsubs{11}$ in the input AST pattern.
The pattern variable $\nexprsubs{11}$ will match {\tt 0.001} in the figure
\ref{fig:trans:ex02}(a).
In the output AST, the modified argument expression 
$\nexprsubs{11} {\tt * hvd.size()}$ will expand to {\tt 0.001 * hvd.size()}.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
for (x, y) in dataset.take(10000):
  with tf.GradientTape() as tape:
    pred = model(x)
    loss_value = loss(y, pred)\end{lstlisting} 
    \caption{Original DL training code}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
for (x, y) in dataset.take(10000):
  with tf.GradientTape() as tape:
    pred = model(x)
    loss_value = loss(y, pred) 
  tape = hvd.DistributedGradientTape(tape)\end{lstlisting}
    \caption{Distributed DL training code}
  \end{subfigure}
  \caption{Example of original DL model code transformed into the distributed model}
  \label{fig:trans:ex03}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{tabular}{l}
  \typdesc{\fkstmt& : & \dstmt ~ $\rightarrow$ ~ \dmodenv ~ $\rightarrow$ ~ (\dstmt ~ list ~ $\times$ ~ \dmodenv)}\\
  \tstmt{\kwith ~ \mul{\nwithitem} ~ \kcolon ~ \mul{\nstmt}}{\smodenv} = \\
  \inden \ktlet ~ \mul{\nwithitem}$'$, \smodenvsubs{1} \kteq ~ \twwithitem{\mul{\nwithitem}}{\smodenv} \ktin \\
  \inden \ktlet ~ \mul{\nstmt}$'$, \smodenvsubs{2} \kteq ~ \tsstmt{\mul{\nstmt}}{\smodenvsubs{1}} \ktin \\
  \inden \ktif ~ \smodenvsubs{1} \envsub ~ \smodenv ~ \kteq ~ [\gtape $\mapsto$ \nid] ~ \ktthen\\
  \inden\inden ([\kwith ~ \mul{\nwithitem}$'$ ~ \kcolon ~ \mul{\nstmt}$'$, \\
  \inden\inden \nid ~ \oassign {\tt hvd.DistributedGradientTape(\nid)}], \smodenvsubs{2})\\
  \inden \ktelse ~ ([\kwith ~ \mul{\nwithitem}$'$ ~ \kcolon ~ \mul{\nstmt}$'$], \smodenvsubs{2})
\end{tabular}\\\vpar
  \caption{Transform function: wrapping GraidentTape instance with Horovod API}
  \label{fig:trans:fn03}
\end{figure}

Figure \ref{fig:trans:ex03} is the third transformed part of the code, and
the figure \ref{fig:trans:fn03} describes the transform function responsible 
of the transformation of figure \ref{fig:trans:ex03}.
The new assign statement is added after the {\tt with} statement body end.
To apply this transformation, the transform function must first identify
a with statement that creates a new {\tt GradientTape} instance. 
The transform function utilizes the transform context to recognize a
with statement that creates a new {\tt GradientTape} instance. 
The transform function compares the input transform context with the 
transform context just after processing the with statement to check
whether the input with statement creates a {\tt GradientTape} instance.
Then, the transform function adds a new assign statement after the 
with statement, which wraps the variable with a
{\tt DistributedGradientTape} Horovod API.

% next subsection
\input{trans_patterns}
