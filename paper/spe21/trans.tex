\section{Code Transformation}\label{sec:trans}
\subsection{Transformation Rule for Distributed TensorFlow ML model}

In this section, we describe formalization of the transformation rule
for distributed DL traininig with set of code examples and formal notions
related to them.
There are two main points in design of the formalization.
First, we informally understand the code transformation rule as methods to
1) select the parts of codes to modify and 2) construct new code by reusing
parts of the input code.
To automate the transformation process, we need code transformation in a form 
that is implements above actions as a program.
Second, as mentioned earlier, to correctly transform tranining codes with different
API patterns, we must apply different transformation rules.
The formalization should also describe different transformation rules
for different API patterns.
In this end, we define a transformation rule for each API pattern as 
a set of pure function that takes ASTs as input and returns ASTs as output.
We call the function as \textit{transform function}.
Each transform function is defined to get an AST of given type and return an AST
of the same type. For instance, a transform function for statement,
$\fkstmt$, takes a statement AST and return a statement AST.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf

dataset = ...
model = ...
optim = tf.optimizers.Adam(0.001) 

for (x, y) in dataset.take(10000):
  with tf.GradientTape() as tape:
    pred = model(x)
    loss_value = loss(y, pred) 

  gradients = tape.gradient(loss_value, model.trainable_variables)
  optim.apply_gradient(zip(gradients, model.trainable_variable)\end{lstlisting}
    \caption{Original DL training code}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{lstlisting}[language=Python]
import tensorflow as tf
import horovod.tensorflow as hvd

dataset = ...
model = ...
optim = tf.optimizers.Adam(0.001 * hvd.size()) 

for (x, y) in dataset.take(10000):
  with tf.GradientTape() as tape:
    pred = model(x)
    loss_value = loss(y, pred) 
  tape = hvd.DistributedGradientTape(tape)

  gradients = tape.gradient(loss_value, model.trainable_variables)
  optim.apply_gradient(zip(gradients, model.trainable_variable)\end{lstlisting}
    \caption{Distributed DL training code}
  \end{subfigure}
  \caption{Code examples}
  \label{fig:trans:ex}
\end{figure}

Figure \ref{fig:trans:ex} shows an example of the input single-GPU code and
the coresponding transformed output distributed code. 
Some unnecessary details of the codes are removed from the codes.
There are three transformed parts in the distributed training code.
First, a new import statement that imports {\tt horovod.tensorflow} modules
is added in line 2. 
The statement is added right after the original {\tt tensorflow} module
import statement.
Second, in line 6, an argument of the constructor function 
{\tt tf.optimizers.Adam} is modified. The newly changed argument multiplies
{\tt hvd.size()} onto the original argument.
Third, in line 12, a new statement that assigns the variable {\tt tape}
is added. The assignment wraps the original {\tt GradientTape} object with
the Horovod library function, {\tt hvd.DistributedGradientTape}.
Except the above mentioned parts, the original code is not modified.     

We formalize the transformation rule as set of pure functions
that take ASTs as input and return ASTs as output.
For each training API pattern, we define  

\subsection{Description of Transform functions}

This section explains transform function each for training API pattern.  

\subsubsection{Transform function for Session pattern}

To correctly transform the Session pattern codes into 

Session pattern and MonitoredSesison pattern uses the following
transform function to match the legacy import statement
and add the import statement for Horovod library.

\begin{figure}[h]
\noindent
\begin{longtable}{l}
  \tstmt{\kimport ~ \mul{\nalias}}{\smodenv} = \\
  \inden \ktif ~ \smodenvsubs{1} ~ \envsub ~ \smodenv ~ \kteq ~ [\tflowc $\mapsto$ \nid] ~ \ktthen \\
  \inden\hspace{1em} ([\kimport ~ \mul{\nalias}, \\
  \inden\hspace{1em} \kimport ~ {\tt horovod.tensorflow} \kas ~ {\tt hvd}, \\
  \inden\hspace{1em} {\tt hvd.init()}], \smodenvsubs{1})\\
  \inden \ktelse~([\kimport ~ \mul{\nalias}], \smodenvsubs{1})
\end{longtable}
\caption{Session pattern transform function: Importing Horovod}
\end{figure}

The Session pattern codes use the {\tt Session} object to define the computation environment.
The {\tt Session} objects are usually defined by {\tt With} statements,
and later used by calling the method {\tt Session.run}.
(TODO: add explanation)

The {\tt Optimizer} instances must be modified for distributed training.
First, the learning rate is scaled by {\tt hvd.size()},
and the optimizer instance is wrapped with {\tt hvd.DistributedOptimizer}.
The following rule describes the part of the transform function that
performs learning rate scaling and the wrapping.
The function matches an assignment statement that assigns an
instance of {\tt Optimizer} class, and modifies the learning rate argument
given the the instance constructor. Then the rule appends the statement
that wraps the {\tt Optimizer} instance with {\tt hvd.DistributedOptimizer}.

\begin{figure}[h]
\noindent
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm}{\smodenv} = \\
\inden \ktif ~ \nexprsubs{1} \ktsubty ~ {\tt tensorflow.keras.optimizers.Optimizer} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt learning\_rate} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i} {\tt * hvd.size()}\\
  \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm \\
  \inden\inden\inden {\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
  \inden\inden \ktelse \\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11}
  {\tt * hvd.size()} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)}
\nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm, \\
  \inden\inden\inden{\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
\end{longtable}
  \caption{Session pattern transform function: Optimizer learning rate scaling and wrapping}
  \label{fig:sess:opt}
\end{figure}


\subsubsection{Transform function for MonitoredSession pattern}

The training codes matched in the MoniotoredSesion pattern uses the
{\tt MonitoredSession} instance to define the training step and
automatically repeat the step.  
The transform function for MonitoredSession pattern uses same rules
as the figure \ref{fig:sess:opt} to identify the optimizer instance
and modify it.

The codes for MonitoreSession pattern additionally need to modify the
{\tt StopAtStepHook} instance in order to scale down the number of
training steps. The {\tt Hook} instances are given as an list argument of
the {\tt MonitoredSesion} instance constructor.
To modify the number of traning steps, the transform function
identifies by expression that constructs the {\tt StopAtStepHook} instance
and transform its {\tt last\_step} keyword argument.

\begin{figure}[h]
 \noindent
\begin{tabular}{l}
  \texpr{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
  \inden \ktif ~ \smodenv(\tflowc) ~ \kteq ~ \nidsubs{t} ~ \ktand ~ \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{t}.train.StopAtStepHook} ~ \ktthen \\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt last\_step} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i} {\tt // hvd.size()}\\
  \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}\\
  \inden\inden \ktelse \\
  \inden\inden\inden \nexprsubs{1} \sparen{\nexprsubs{11} {\tt // hvd.size()} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \\
  \inden \ktelse ~ \texpr{\nexprsubs{1}}{\smodenv} \sparen{\texpr{\nexprsubs{11}}{\smodenv}  ... \texpr{\nexprsubs{1n}}{\smodenv} \\
  \inden\inden \op{(\nidsubs{1} \oassign)} \texpr{\nexprsubs{21}}{\smodenv} ... \op{(\nidsubs{k} \oassign)} \texpr{\nexprsubs{2k}}{\smodenv}}\\
\end{tabular}\\\vpar
\caption{MonitoredSession pattern transform function: Modifying {\tt StopAtStepHook} instance}
\end{figure}

\subsubsection{Transform function for GradientTape pattern}

In training codes matched in the GradientTape pattern,
the {\tt GradientTape} instance is wrapped with
{\tt hvd.DistributedGradientTape} function.
The {\tt GradientTape} instances are created by {\tt with} statements
in the training codes, which automatically invokes initialization
and destruction functions before and after the {\tt with} statement scope.
The transform function matches the {\tt with} statement that
creates the {\tt GradientTape} instance and inserts the
wrapping statement after it.
Figure \ref{fig:trans:gtape} illustrates the transform function definition
regarding the {\tt GradientTape} wrapping.

\begin{figure}[h]
\noindent
\begin{tabular}{l}
  \tstmt{\optypcomm ~ \kwith ~ \mul{\nwithitem} ~ \kcolon ~ \mul{\nstmt}}{\smodenv} = \\
  \inden \ktlet ~ \mul{\nwithitem}$'$, \smodenvsubs{1} \kteq ~ \twwithitem{\mul{\nwithitem}}{\smodenv} \ktin \\
  \inden \ktlet ~ \mul{\nstmt}$'$, \smodenvsubs{2} \kteq ~ \tsstmt{\mul{\nstmt}}{\smodenvsubs{1}} \ktin \\
  \inden \ktif ~ \smodenvsubs{1} \envsub ~ \smodenv ~ \kteq ~ [\gtape $\mapsto$ \nid] ~ \ktthen\\
  \inden\inden ([\optypcomm ~ \kwith ~ \mul{\nwithitem}$'$ ~ \kcolon ~ \mul{\nstmt}$'$, \\
  \inden\inden \nid ~ \oassign {\tt hvd.DistributedGradientTape(\nid)}], \smodenvsubs{2})\\
  \inden \ktelse ~ ([\optypcomm ~ \kwith ~ \mul{\nwithitem}$'$ ~ \kcolon ~ \mul{\nstmt}$'$], \smodenvsubs{2})
\end{tabular}
  \caption{GradientTape pattern transform function: wrapping {\tt GradientTape} instance}
  \label{fig:trans:gtape}
\end{figure}

In addition, the variable broadcast should be manually done with
{\tt hvd.broadcast\_variables} function.
Because the variable broadcast should be done exactly once at the start of the
training, we utilize a global boolean variable {\tt hvd\_broadcast\_done}
that tracks whether the broadcast is already performed or not.
The variable {\tt hvd\_broadcast\_done} is declared within the Horovod
initialization codes and initialized to {\tt False} as described in
figure \ref{fig:trans:gtape2}.

\begin{figure}[h]
\begin{tabular}{l}
  \tstmt{\kimport ~ \mul{\nalias}}{\smodenv} = \\
  \inden \ktlet ~ \smodenvsubs{1} ~ \kteq ~ \taalias{\mul{\nalias}}{\smodenv} \ktin \\
  \inden \ktif ~ \smodenvsubs{1} ~ \envsub ~ \smodenv ~ \kteq ~ [\tflow $\mapsto$ \nid] ~ \ktthen \\
  \inden\hspace{1em} ([\kimport ~ \mul{\nalias}, \\
  \inden\hspace{1em} \kimport ~ {\tt horovod.tensorflow} \kas ~ {\tt hvd}, \\
  \inden\hspace{1em} {\tt hvd\_broadcast\_done} \oassign {\tt False}, \\
  \inden\hspace{1em} {\tt hvd.init()}, \\
  \inden\hspace{1em} {\tt gpus = \nid.config.experimental.list\_physical\_devices(`GPU')}, \\
  \inden\hspace{1em} {\tt for gpu in gpus: \nid.config.experimental.set\_memory\_growth(gpu, True)},\\
  \inden\hspace{1em} {\tt if gpus: \nid.config.experimental.set\_visible\_devices(gpus[hvd.local\_rank()], `GPU')}], \smodenvsubs{1})\\
  \inden \ktelse~([\kimport ~ \mul{\nalias}], \smodenvsubs{1})
\end{tabular}
  \caption{GradientTape pattern transform function: initializing {\tt hvd\_broadcast\_done}}
  \label{fig:trans:gtape2}
\end{figure}

The statements that perform the variable broadcast are inserted after
the {\tt Optimizer} instance applies the training gradient.
The corresponding transform function is described in figure \ref{fig:trans:gtape3}.
The inserted codes check and modify the boolean value of 
{\tt hvd\_broadcast\_done} to ensure that the variable broadcast
happens only once.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
  % variable broadcasting
  \inden \ktif  ~ \smodenv(\optmizer) ~ \kteq ~ \nidsubs{t} ~ \ktand ~ \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{t}.apply\_gradients} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt grads\_and\_vars} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden \ktlet ~ \nidsubs{z} ~ \kteq ~ \newid ~ \ktin \\
  \inden\inden\inden ([\nidsubs{z} ~ \oassign ~ \nexprsubs{2i},\\
  \inden\inden\inden \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nidsubs{z} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}},\\
  \inden\inden\inden {\tt global hvd\_broadcast\_done}, \\
  \inden\inden\inden {\tt if not hvd\_broadcast\_done:} [ {\tt hvd.broadcast\_variables([x[1] for x in \nidsubs{z}], root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd.broadcast\_variables(\nidsubs{t}.variables(), root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd\_broadcast\_done = True} ]], \smodenv) \\
  \inden\inden \ktelse \\
  \inden\inden\inden \ktlet ~ \nidsubs{z} ~ \kteq ~ \newid ~ \ktin \\
  \inden\inden\inden ([\nidsubs{z} ~ \oassign ~ \nexprsubs{11},\\
  \inden\inden\inden \nexprsubs{1} \sparen{\nidsubs{z} \nexprsubs{12} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}},\\
  \inden\inden\inden {\tt global hvd\_broadcast\_done}, \\
  \inden\inden\inden {\tt if not hvd\_broadcast\_done:} [ {\tt hvd.broadcast\_variables([x[1] for x in \nidsubs{z}], root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd.broadcast\_variables(\nidsubs{t}.variables(), root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd\_broadcast\_done = True} ]], \smodenv) \\
\end{longtable}
  \caption{GradientTape pattern transform fuinction: the variable broadcast}
  \label{fig:trans:gtape3}
\end{figure}
 
\subsubsection{Transform function for Keras pattern}

In the training codes matching the Keras pattern,
the {\tt Model} instances are created and used to invoke the
training process. The transform function first stores the {\tt Model} instance
variable name in the environment as described in figure \ref{fig:trans:ker}.
The AST pattern guard tests that the given function expression is a subclass
of {\tt tensorflow.keras.Model}, which corresponds to the class constructors
of subclass instances of {\tt tensorflow.keras.Model} class.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm}{\smodenv} = \\
  \inden \ktelif ~ \nexprsubs{1} ~ \ktsubty ~ {\tt tensorflow.keras.Model} ~ \ktthen\\
  \inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{2} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}], \smodenv[\tmodel \nidsubs{r}])\\
\end{longtable}
  \caption{Keras pattern transform function: storing the {\tt Model} instance}
  \label{fig:trans:ker}
\end{figure}

The training process is invoked with {\tt fit} method of the {\tt Model} instance.
For the training codes matching the Keras pattern, the variable broadcast should
be done by adding the {\tt hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)}
callback to the {\tt fit} method argument.
The transform function utilizes the environment parameter to identify the {\tt fit}
method call and insert the callback object to the argument.
Figure \ref{fig:trans:ker2} describes the transform function that
adds the callback to the {\tt fit} function call expression.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
  \inden \ktelif ~ \nidsubs{m} ~ \kteq ~ \smodenv(\tmodel) ~ \ktand ~ 
          \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{m}.fit} ~ \ktthen \\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt callbacks} ~ \ktwhen ~ 2 $\leq$ i $\leq$ k ~ \ktthen \\
  \inden\inden\inden ([{\tt callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)]},\\
  \inden\inden\inden ~ {\tt if hvd.rank() == 0: callbacks.append(\nexprsubs{2i})}, \\
  \inden\inden\inden ~ {\tt \nexprsubs{1} (optim ... \nexprsubs{1n}}
                              \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                              {\tt callbacks \oassign callbacks} ... 
                              \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}{\tt )}], \smodenv) \\
  \inden\inden \ktelse \\
  \inden\inden\inden ([{\tt callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)]},\\
  \inden\inden\inden ~ {\tt \nexprsubs{1} (optim ... \nexprsubs{1n}}
                              \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                              \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}...
                              {\tt callbacks \oassign callbacks} {\tt )}], \smodenv) \\ 
\end{longtable}
  \caption{Keras pattern transform function: adding the broadcast callback}
  \label{fig:trans:ker2}
\end{figure}

The Keras pattern transform function also modifies the {\tt Optimizer} instance.
First, the learning rate parameter of the instance should be scaled by
{\tt hvd.size()}. Second, the instance should be wrapped with
{\tt hvd.Distributed Optimizer} function, similar to the GradientTape pattern's case.
Figure \ref{fig:trans:ker3} describes the transform function modifying
the {\tt Optimizer} instance.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm}{\smodenv} = \\
  \inden \ktelif ~ \nexprsubs{1} \ktsubty ~ {\tt tensorflow.keras.optimizers.Optimizer} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt learning\_rate} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i} {\tt * hvd.size()}\\
  \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm \\
  \inden\inden\inden {\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
  \inden\inden \ktelse \\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11}
  {\tt * hvd.size()} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)}
\nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm, \\
  \inden\inden\inden{\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
\end{longtable}
  \caption{Keras pattern transform function: modifying the {\tt Optimizer} instance}
  \label{fig:trans:ker3}
\end{figure}
