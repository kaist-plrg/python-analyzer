\section{Code Transformation}\label{sec:trans}
\subsection{Transformation Rule for Distributed TensorFlow ML model}

In this section, we describe the transformation rule that
transforms single-GPU-based TensorFlow model codes into
multi-GPU-based TensorFlow model codes.
We informally understand code transformation rule as
conditions to select the target code parts
and methods to transform them into another.
The methods include the addition, modification, and deletion of
specific code parts.
Currently, code transformation for distributed ML training
is only described by a set of examples and informal explanations.
To automate the code transformation process,
we need to formally define the code transformation rule
and ways to convert the definition into software implementation.
We propose a formal definition of the code transformation rule
for distributed ML training and implement the automatic code transformation
software based on the formal definition.

Code transformation is formally defined as a pure function from AST to AST.
We call this function a transform function.
We may define multiple transform functions that act on different
language constructs and use them to define other transform functions.
For example, to transform a Python code into another,
we define a transform function that takes Module AST and returns Module AST.
Inside the Module transform function definition,
we may use the statement transform function to transform statements
that compose the Module AST.

Together with the AST parameter, transform functions take and return
the environment parameter.
Environment parameters are used to store specific identifiers
and pass them to the other calls of the transform function.
For example, the statement transform function frequently uses 
the identifier bound to the TensorFlow module.
The module name first appears from the import statement.
The transform function call on the import statement stores
the TensorFlow module name on the environment and returns it.
Then the later function calls to other statements
retrieve the TensorFlow module name from the environment parameter.

% explain transform function with example

\begin{figure}[h]
\begin{lstlisting}[language=Python]
import tensorflow as tf
optimizer = tf.keras.optimizers.Adam(lr)
\end{lstlisting}
  \caption{Original code example}
  \label{fig:trans:org}
\end{figure}

\begin{figure}[h]
\begin{lstlisting}[language=Python]
import tensorflow as tf
# import and init horovod module
import horovod.tensorflow as hvd
hvd_broadcast_done = False
hvd.init()
import tensorflow.keras as keras

# scale the learning rate
optimizer = tf.keras.optimizers.Adam(lr * hvd.size())
# wrap the optimizer object
optimizer = hvd.DistributedOptimizer(optimizer)
\end{lstlisting}
  \caption{Transformed code example}
  \label{fig:trans:hvd}
\end{figure}

We first describe how the transform function works by example.
The above figures illustrate a pair of TensorFlow model codes
before and after the transformation;
the figure \ref{fig:trans:org} represents the original
training code before the transformation and
the figure \ref{fig:trans:org} represents the distributed
training code after the transformatioin.
Informally, three kinds of transformation occur in between;
1) import and initialize the horovod module;
2) scale the optimizer's learning rate by {\tt hvd.size()};
3) Wrap the optimizer with {\tt hvd.DistributedOptimizer}.

The original code parses into a module AST with a list of 2 statements,
an import statement and an assign statement, 
as illustrated in the figure \ref{fig:trans:ast}. 

\begin{figure}[h]
\begin{lstlisting}[language=Scala]
Module(List(
  ImportStmt(Id('tensorflow'), Id('tf')),
  AssignStmt(
    Id('optimizer'), 
    CallExpr(
      Expr('tf.keras.optimizers.Adam'),
      Args(Id('lr'))
    )
  )
))
\end{lstlisting}
  \caption{The AST of the code in figure \ref{fig:trans:org}, represented in Scala object}
  \label{fig:trans:ast}
\end{figure}

\begin{figure}[!ht]
\begin{tabular}{lcl}
  \tmodule{[\nstmtsubs{1}, \nstmtsubs{2}] ~ \ntypignore} 
  & \kteq & \tsstmt{[\nstmtsubs{1}, \nstmtsubs{2}]}{\smodenvempty} \\

  & \kteq & \ktlet ~ \mul{\nstmtsubs{1}}$'$, \smodenvsubs{1} ~ \kteq ~ 
  \tstmt{\nstmtsubs{1}}{\smodenvempty} ~ \ktin \\

  & & \ktlet ~ \mul{\nstmtsubs{2}}$'$, \smodenvsubs{2} ~ \kteq ~ 
  \tstmt{\nstmtsubs{2}}{\smodenvsubs{1}} ~ \ktin \\

  & & (\mul{\nstmtsubs{1}}$'$ \ktconl ~ \mul{\nstmtsubs{2}}$'$) \\ 
\end{tabular}
  \caption{Transform function evaluated on the module AST}
  \label{fig:trans:ex_module}
\end{figure}

The figure \ref{fig:trans:ex_module} 
describes how the module transform function 
is evaluated on the example module AST.
The module transform function, $\fkmodule$
applies statement list transform function $\fksstmt$ to its statement list.
Then $\fksstmt$ applies the statement transform function $\fkstmt$
to each statement in the list.
In addition, $\fksstmt$ passes the environment parameter $\sigma$
from a $\fkstmt$ call to the next $\fkstmt$ call.

The figure \ref{fig:trans:ex_stmt1} illustrates the evaluation of
the transform function on \nstmtsubs{1}.
The first $\fkstmt$ call receives the statement
{\tt import tensorflow as tf} as an input.
In part (a), the function first matches the input statement
with the pattern $\kimport ~ \mul{\nalias}$.
Transform functions use pattern matching to discriminate target ASTs
by their syntactic structure.
Part (b) specifies the pattern guard, which describes the detailed conditions
on the content of the input.
The pattern guard checks if the import statement
is importing the TensorFlow module.
In the process, the alias transform function $\fkalias$ is applied on the
{\tt tensorflow as tf}.
Note that $\fkalias$ also stores the module name {\tt tf}
in the environment parameter as a form of mapping;
later calls on the transform function can retrieve the
TensorFlow module name from the returned environment, $\smodenvsubs{1}$.
Finally, part (c) constructs the output statements
and returns it with $\smodenvsubs{1}$.


\begin{figure}[!ht]
\begin{tabular}{rcll}
  \tstmt{\nstmtsubs{1}}{\smodenvempty} & \kteq & 
  \tstmt{{\tt import tensorflow as tf}}{\smodenvempty} & \\

  & \kteq & \tstmt{\kimport ~ \mul{\nalias}}{\smodenvempty} & 
  (a) Pattern matching the input \\

  & & {\tt matching} ~ ( & \\
  && \indent \kimport ~ \mul{\nalias} \kteq ~ 
  \kimport ~ [{\tt tensorflow} \kas ~ {\tt tf}], & \\
  && \indent \mul{\nalias} \kteq ~ [\naliassubs{1}], & \\ 
  && \indent \naliassubs{1} \kteq ~ {\tt tensorflow} \kas ~ {\tt tf} ~ ) & \\

  & \kteq & 
  \ktlet ~ \smodenvsubs{1} ~ \kteq ~ \taalias{[\naliassubs{1}]}{\smodenv} 
  \ktin & 
  (b) Check if the TensorFlow module imported \\
  && \ktif ~ \smodenvsubs{1} ~ \envsub ~ \smodenv ~ 
  \kteq ~ [\tflow $\mapsto$ \nid] ~ \ktthen &\\ 
  && ([\kimport ~ \naliassubs{1}, & \\
  && {\tt import horovod.tensorflow as hvd}, & \\
  && {\tt hvd\_broadcast\_done = False}, & \\
  && {\tt hvd.init()}], \smodenvsubs{1}) & \\
  
  & \kteq &
  ([\kimport ~ {\tt tensorflow} \kas ~ {\tt tf}, & 
  (c) Construct output statements \\
  && {\tt import horovod.tensorflow as hvd}, & \\
  && {\tt hvd\_broadcast\_done = False}, & \\
  && {\tt hvd.init()}], \smodenvsubs{1}) & \\

  \kwith ~ \smodenvsubs{1} 
  & \kteq & \talias{\naliassubs{1}}{\smodenvempty} & \\
  & = & \talias{{\tt tensorflow} \kas ~ {\tt tf}}{\smodenvempty} & \\ 
  & \kteq & \smodenvempty[\tflow $\mapsto$ {\tt tf}] & \\
  & \kteq & [\tflow $\mapsto$ {\tt tf}] & \\
\end{tabular}
  \caption{Transform function evaluated on the first statement AST}
  \label{fig:trans:ex_stmt1}
\end{figure}

\begin{figure}
\begin{tabular}{rcll}
  \tstmt{\nstmtsubs{2}}{\smodenvempty} & \kteq &
  \tstmt{{\tt optimizer = tf.keras.optimizers.Adam(lr)}}{\smodenvsubs{1}} &\\
  &\kteq&
  \tstmt{\nidsubs{r} \oassign 
  \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ 
  \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
  \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenvsubs{1}} &\\
  && {\tt matching} ~ ( 
  \indent\indent\indent\indent\indent (a) Pattern matching the input &\\
  && \indent \nidsubs{r} \kteq ~ {\tt optimizer}, &\\
  && \indent \nexprsubs{1} = {\tt tf.keras.optimizers.Adam}, &\\
  && \indent \nexprsubs{11}= {\tt lr}~) &\\

  &\kteq& \ktif ~ \smodenvsubs{1}(\tflow) ~ \kteq ~ \nidsubs{t} ~ 
  \indent\indent\indent\indent\indent
  (b) Check if it calls {\tt tf.keras.optimizers.Adam} &\\ 
  && \ktand ~ 
  \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{t}.keras.optimizers.Adam} ~ 
  \ktthen& \\

  && ([\nidsubs{r} \oassign \nexprsubs{1} 
  \sparen{\nexprsubs{11} {\tt * hvd.size()} ~ ... ~ \nexprsubs{1n} 
  ~\op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
  \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}], &\\
  && \smodenvsubs{1}[\optmizer $\mapsto$ \nidsubs{r}])&\\
  
  &\kteq& 
  ([{\tt optimizer = tf.keras.optimizers.Adam(lr * hvd.size())}], &\\   
  && \smodenvsubs{1}[\optmizer $\mapsto$ \nidsubs{r}])
  \indent\indent\indent\indent\indent
  (c) Construct output statements &\\ 
\end{tabular}
  \caption{Transform function evaluated on the second statement AST}
  \label{fig:trans:ex_stmt2}
\end{figure}

The figure \ref{fig:trans:ex_stmt2} illustrates the evaluation of
the transform funcion on \nstmtsubs{2}.
The second $\fkstmt$ receives the statement
{\tt optimizer = tf.keras.optimizers.Adam(lr)} as an input.
Part (a) pattern matches the input to the assign statement pattern
with the right-hand side of the function call expression.
Part (b) is a pattern guard that checks if
the function call is {\tt tf.keras.optimizers.Adam}.
Note that the guard refers to the environment parameter
to check the TensorFlow module identifier.
Because after the first $\fkstmt$ call,
the environment parameter stores the TensorFlow module identifier {\tt tf},
so the second $\fkstmt$ call can use the information to
check if the function call is indeed a constructor for TensorFlow
optimizer object.
In part (c), the output statement is returned.
As specified in the transform function definition, 
the first argument expression is multiplied by {\tt hvd.size()}.

\begin{figure}[!ht]
\begin{tabular}{rcll}
  \tmodule{[\nstmtsubs{1}, \nstmtsubs{2}] ~ \ntypignore} 
  &\kteq& (\mul{\nstmtsubs{1}}$'$ \ktconl ~ \mul{\nstmtsubs{2}}$'$) &\\ 
  &\kteq& 
  [\kimport ~ {\tt tensorflow} \kas ~ {\tt tf}, &\\ 
  && {\tt import horovod.tensorflow as hvd}, & \\
  && {\tt hvd\_broadcast\_done = False}, & \\
  && {\tt hvd.init()}, &\\
  && {\tt optimizer = tf.keras.optimizers.Adam(lr * hvd.size())}] &\\   
\end{tabular}
  \caption{Transform function concatenating the resulting statements}
  \label{fig:trans:ex_concat}
\end{figure}

Finally, the output statement lists are concatenated as specified in the
$\fksstmt$, and the transformed module AST is constructed
as illustrated on the figure \ref{fig:trans:ex_concat}.

As described in the example, the transform functions are defined for
each language construct. The functions are described in terms of
pattern matching against the input AST; the matching patterns and 
pattern guards select the target AST to transform, and
the output AST is constructed with the matched variables.

Each training API category defines a corresponding transform function.
Given a training code of the category, the corresponding module AST transform
function is applied to the target code AST with an empty environment parameter.
In the following subsections, we describe the transform function for each
training API category. The paper only explains important parts of the 
definition; the full definition of the transform functions can be found
in the supplementary material.

\subsection{Description of Transform functions}

\subsubsection{Transform function for Session pattern}

The training codes in Session pattern and MonitoredSesion pattern
use legacy version of the TensorFlow 1.x library. 
These legacy codes can be identified by specific import statement as below.

\begin{figure}[h]
\begin{lstlisting}[language=Python]
import tensorflow.compat.v1 as tf
\end{lstlisting}
\caption{Import statements for using TensorFlow 1.x legacy module}
\end{figure}

Session pattern and MonitoredSesison pattern uses the following
transform function to match the legacy import statement
and add the import statement for Horovod library.

\begin{figure}[h]
\noindent
\begin{longtable}{l}
  \tstmt{\kimport ~ \mul{\nalias}}{\smodenv} = \\
  \inden \ktif ~ \smodenvsubs{1} ~ \envsub ~ \smodenv ~ \kteq ~ [\tflowc $\mapsto$ \nid] ~ \ktthen \\
  \inden\hspace{1em} ([\kimport ~ \mul{\nalias}, \\
  \inden\hspace{1em} \kimport ~ {\tt horovod.tensorflow} \kas ~ {\tt hvd}, \\
  \inden\hspace{1em} {\tt hvd.init()}], \smodenvsubs{1})\\
  \inden \ktelse~([\kimport ~ \mul{\nalias}], \smodenvsubs{1})
\end{longtable}
\caption{Session pattern transform function: Importing Horovod}
\end{figure}

The Session pattern codes use the {\tt Session} object to define the computation environment.
The {\tt Session} objects are usually defined by {\tt With} statements,
and later used by calling the method {\tt Session.run}.
(TODO: add explanation)

The {\tt Optimizer} instances must be modified for distributed training.
First, the learning rate is scaled by {\tt hvd.size()},
and the optimizer instance is wrapped with {\tt hvd.DistributedOptimizer}.
The following rule describes the part of the transform function that
performs learning rate scaling and the wrapping.
The function matches an assignment statement that assigns an
instance of {\tt Optimizer} class, and modifies the learning rate argument
given the the instance constructor. Then the rule appends the statement
that wraps the {\tt Optimizer} instance with {\tt hvd.DistributedOptimizer}.

\begin{figure}[h]
\noindent
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm}{\smodenv} = \\
\inden \ktif ~ \nexprsubs{1} \ktsubty ~ {\tt tensorflow.keras.optimizers.Optimizer} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt learning\_rate} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i} {\tt * hvd.size()}\\
  \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm \\
  \inden\inden\inden {\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
  \inden\inden \ktelse \\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11}
  {\tt * hvd.size()} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)}
\nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm, \\
  \inden\inden\inden{\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
\end{longtable}
  \caption{Session pattern transform function: Optimizer learning rate scaling and wrapping}
  \label{fig:sess:opt}
\end{figure}


\subsubsection{Transform function for MonitoredSession pattern}

The training codes matched in the MoniotoredSesion pattern uses the
{\tt MonitoredSession} instance to define the training step and
automatically repeat the step.  
The transform function for MonitoredSession pattern uses same rules
as the figure \ref{fig:sess:opt} to identify the optimizer instance
and modify it.

The codes for MonitoreSession pattern additionally need to modify the
{\tt StopAtStepHook} instance in order to scale down the number of
training steps. The {\tt Hook} instances are given as an list argument of
the {\tt MonitoredSesion} instance constructor.
To modify the number of traning steps, the transform function
identifies by expression that constructs the {\tt StopAtStepHook} instance
and transform its {\tt last\_step} keyword argument.

\begin{figure}[h]
 \noindent
\begin{tabular}{l}
  \texpr{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
  \inden \ktif ~ \smodenv(\tflowc) ~ \kteq ~ \nidsubs{t} ~ \ktand ~ \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{t}.train.StopAtStepHook} ~ \ktthen \\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt last\_step} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i} {\tt // hvd.size()}\\
  \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}\\
  \inden\inden \ktelse \\
  \inden\inden\inden \nexprsubs{1} \sparen{\nexprsubs{11} {\tt // hvd.size()} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \\
  \inden \ktelse ~ \texpr{\nexprsubs{1}}{\smodenv} \sparen{\texpr{\nexprsubs{11}}{\smodenv}  ... \texpr{\nexprsubs{1n}}{\smodenv} \\
  \inden\inden \op{(\nidsubs{1} \oassign)} \texpr{\nexprsubs{21}}{\smodenv} ... \op{(\nidsubs{k} \oassign)} \texpr{\nexprsubs{2k}}{\smodenv}}\\
\end{tabular}\\\vpar
\caption{MonitoredSession pattern transform function: Modifying {\tt StopAtStepHook} instance}
\end{figure}

\subsubsection{Transform function for GradientTape pattern}

In training codes matched in the GradientTape pattern,
the {\tt GradientTape} instance is wrapped with
{\tt hvd.DistributedGradientTape} function.
The {\tt GradientTape} instances are created by {\tt with} statements
in the training codes, which automatically invokes initialization
and destruction functions before and after the {\tt with} statement scope.
The transform function matches the {\tt with} statement that
creates the {\tt GradientTape} instance and inserts the
wrapping statement after it.
Figure \ref{fig:trans:gtape} illustrates the transform function definition
regarding the {\tt GradientTape} wrapping.

\begin{figure}[h]
\noindent
\begin{tabular}{l}
  \tstmt{\optypcomm ~ \kwith ~ \mul{\nwithitem} ~ \kcolon ~ \mul{\nstmt}}{\smodenv} = \\
  \inden \ktlet ~ \mul{\nwithitem}$'$, \smodenvsubs{1} \kteq ~ \twwithitem{\mul{\nwithitem}}{\smodenv} \ktin \\
  \inden \ktlet ~ \mul{\nstmt}$'$, \smodenvsubs{2} \kteq ~ \tsstmt{\mul{\nstmt}}{\smodenvsubs{1}} \ktin \\
  \inden \ktif ~ \smodenvsubs{1} \envsub ~ \smodenv ~ \kteq ~ [\gtape $\mapsto$ \nid] ~ \ktthen\\
  \inden\inden ([\optypcomm ~ \kwith ~ \mul{\nwithitem}$'$ ~ \kcolon ~ \mul{\nstmt}$'$, \\
  \inden\inden \nid ~ \oassign {\tt hvd.DistributedGradientTape(\nid)}], \smodenvsubs{2})\\
  \inden \ktelse ~ ([\optypcomm ~ \kwith ~ \mul{\nwithitem}$'$ ~ \kcolon ~ \mul{\nstmt}$'$], \smodenvsubs{2})
\end{tabular}
  \caption{GradientTape pattern transform function: wrapping {\tt GradientTape} instance}
  \label{fig:trans:gtape}
\end{figure}

In addition, the variable broadcast should be manually done with
{\tt hvd.broadcast\_variables} function.
Because the variable broadcast should be done exactly once at the start of the
training, we utilize a global boolean variable {\tt hvd\_broadcast\_done}
that tracks whether the broadcast is already performed or not.
The variable {\tt hvd\_broadcast\_done} is declared within the Horovod
initialization codes and initialized to {\tt False} as described in
figure \ref{fig:trans:gtape2}.

\begin{figure}[h]
\begin{tabular}{l}
  \tstmt{\kimport ~ \mul{\nalias}}{\smodenv} = \\
  \inden \ktlet ~ \smodenvsubs{1} ~ \kteq ~ \taalias{\mul{\nalias}}{\smodenv} \ktin \\
  \inden \ktif ~ \smodenvsubs{1} ~ \envsub ~ \smodenv ~ \kteq ~ [\tflow $\mapsto$ \nid] ~ \ktthen \\
  \inden\hspace{1em} ([\kimport ~ \mul{\nalias}, \\
  \inden\hspace{1em} \kimport ~ {\tt horovod.tensorflow} \kas ~ {\tt hvd}, \\
  \inden\hspace{1em} {\tt hvd\_broadcast\_done} \oassign {\tt False}, \\
  \inden\hspace{1em} {\tt hvd.init()}, \\
  \inden\hspace{1em} {\tt gpus = \nid.config.experimental.list\_physical\_devices(`GPU')}, \\
  \inden\hspace{1em} {\tt for gpu in gpus: \nid.config.experimental.set\_memory\_growth(gpu, True)},\\
  \inden\hspace{1em} {\tt if gpus: \nid.config.experimental.set\_visible\_devices(gpus[hvd.local\_rank()], `GPU')}], \smodenvsubs{1})\\
  \inden \ktelse~([\kimport ~ \mul{\nalias}], \smodenvsubs{1})
\end{tabular}
  \caption{GradientTape pattern transform function: initializing {\tt hvd\_broadcast\_done}}
  \label{fig:trans:gtape2}
\end{figure}

The statements that perform the variable broadcast are inserted after
the {\tt Optimizer} instance applies the training gradient.
The corresponding transform function is described in figure \ref{fig:trans:gtape3}.
The inserted codes check and modify the boolean value of 
{\tt hvd\_broadcast\_done} to ensure that the variable broadcast
happens only once.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
  % variable broadcasting
  \inden \ktif  ~ \smodenv(\optmizer) ~ \kteq ~ \nidsubs{t} ~ \ktand ~ \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{t}.apply\_gradients} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt grads\_and\_vars} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden \ktlet ~ \nidsubs{z} ~ \kteq ~ \newid ~ \ktin \\
  \inden\inden\inden ([\nidsubs{z} ~ \oassign ~ \nexprsubs{2i},\\
  \inden\inden\inden \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nidsubs{z} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}},\\
  \inden\inden\inden {\tt global hvd\_broadcast\_done}, \\
  \inden\inden\inden {\tt if not hvd\_broadcast\_done:} [ {\tt hvd.broadcast\_variables([x[1] for x in \nidsubs{z}], root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd.broadcast\_variables(\nidsubs{t}.variables(), root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd\_broadcast\_done = True} ]], \smodenv) \\
  \inden\inden \ktelse \\
  \inden\inden\inden \ktlet ~ \nidsubs{z} ~ \kteq ~ \newid ~ \ktin \\
  \inden\inden\inden ([\nidsubs{z} ~ \oassign ~ \nexprsubs{11},\\
  \inden\inden\inden \nexprsubs{1} \sparen{\nidsubs{z} \nexprsubs{12} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}},\\
  \inden\inden\inden {\tt global hvd\_broadcast\_done}, \\
  \inden\inden\inden {\tt if not hvd\_broadcast\_done:} [ {\tt hvd.broadcast\_variables([x[1] for x in \nidsubs{z}], root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd.broadcast\_variables(\nidsubs{t}.variables(), root\_rank=0)}, \\
  \inden\inden\inden\inden {\tt hvd\_broadcast\_done = True} ]], \smodenv) \\
\end{longtable}
  \caption{GradientTape pattern transform fuinction: the variable broadcast}
  \label{fig:trans:gtape3}
\end{figure}
 
\subsubsection{Transform function for Keras pattern}

In the training codes matching the Keras pattern,
the {\tt Model} instances are created and used to invoke the
training process. The transform function first stores the {\tt Model} instance
variable name in the environment as described in figure \ref{fig:trans:ker}.
The AST pattern guard tests that the given function expression is a subclass
of {\tt tensorflow.keras.Model}, which corresponds to the class constructors
of subclass instances of {\tt tensorflow.keras.Model} class.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm}{\smodenv} = \\
  \inden \ktelif ~ \nexprsubs{1} ~ \ktsubty ~ {\tt tensorflow.keras.Model} ~ \ktthen\\
  \inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{2} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}], \smodenv[\tmodel \nidsubs{r}])\\
\end{longtable}
  \caption{Keras pattern transform function: storing the {\tt Model} instance}
  \label{fig:trans:ker}
\end{figure}

The training process is invoked with {\tt fit} method of the {\tt Model} instance.
For the training codes matching the Keras pattern, the variable broadcast should
be done by adding the {\tt hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)}
callback to the {\tt fit} method argument.
The transform function utilizes the environment parameter to identify the {\tt fit}
method call and insert the callback object to the argument.
Figure \ref{fig:trans:ker2} describes the transform function that
adds the callback to the {\tt fit} function call expression.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}}}{\smodenv} = \\
  \inden \ktelif ~ \nidsubs{m} ~ \kteq ~ \smodenv(\tmodel) ~ \ktand ~ 
          \nexprsubs{1} ~ \kteq ~ {\tt \nidsubs{m}.fit} ~ \ktthen \\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt callbacks} ~ \ktwhen ~ 2 $\leq$ i $\leq$ k ~ \ktthen \\
  \inden\inden\inden ([{\tt callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)]},\\
  \inden\inden\inden ~ {\tt if hvd.rank() == 0: callbacks.append(\nexprsubs{2i})}, \\
  \inden\inden\inden ~ {\tt \nexprsubs{1} (optim ... \nexprsubs{1n}}
                              \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                              {\tt callbacks \oassign callbacks} ... 
                              \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}{\tt )}], \smodenv) \\
  \inden\inden \ktelse \\
  \inden\inden\inden ([{\tt callbacks = [hvd.callbacks.BroadcastGlobalVariablesCallback(root\_rank=0)]},\\
  \inden\inden\inden ~ {\tt \nexprsubs{1} (optim ... \nexprsubs{1n}}
                              \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... 
                              \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}...
                              {\tt callbacks \oassign callbacks} {\tt )}], \smodenv) \\ 
\end{longtable}
  \caption{Keras pattern transform function: adding the broadcast callback}
  \label{fig:trans:ker2}
\end{figure}

The Keras pattern transform function also modifies the {\tt Optimizer} instance.
First, the learning rate parameter of the instance should be scaled by
{\tt hvd.size()}. Second, the instance should be wrapped with
{\tt hvd.Distributed Optimizer} function, similar to the GradientTape pattern's case.
Figure \ref{fig:trans:ker3} describes the transform function modifying
the {\tt Optimizer} instance.

\begin{figure}[h]
\begin{longtable}{l}
  \tstmt{\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm}{\smodenv} = \\
  \inden \ktelif ~ \nexprsubs{1} \ktsubty ~ {\tt tensorflow.keras.optimizers.Optimizer} ~ \ktthen\\
  \inden\inden \ktif ~ \nidsubs{i} ~ \kteq ~ {\tt learning\_rate} ~ \ktwhen ~ 1 $\leq$ i $\leq$ k ~ \ktthen\\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)} \nexprsubs{21} ... \nidsubs{i} \oassign \nexprsubs{2i} {\tt * hvd.size()}\\
  \inden\inden\inden\inden ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm \\
  \inden\inden\inden {\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
  \inden\inden \ktelse \\
  \inden\inden\inden ([\nidsubs{r} \oassign \nexprsubs{1} \sparen{\nexprsubs{11}
  {\tt * hvd.size()} ... \nexprsubs{1n} ~ \op{(\nidsubs{1} \oassign)}
\nexprsubs{21} ... \op{(\nidsubs{k} \oassign)} \nexprsubs{2k}} \optypcomm, \\
  \inden\inden\inden{\tt \nidsubs{r} = hvd.DistributedOptimizer(\nidsubs{r})}],
  \smodenv[\optmizer $\mapsto$ \nidsubs{r}])\\
\end{longtable}
  \caption{Keras pattern transform function: modifying the {\tt Optimizer} instance}
  \label{fig:trans:ker3}
\end{figure}
