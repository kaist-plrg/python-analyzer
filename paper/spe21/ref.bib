@article{sergeev2018horovod,
  Author = {Alexander Sergeev and Mike Del Balso},
  Journal = {arXiv preprint arXiv:1802.05799},
  Title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
  Year = {2018}
}

@misc{horovodgithub,
  author={Horovod},
  title={Horovod},
  year={2022},
  publisher={GitHub},
  journal = {GitHub repository},
  howpublished={\url{https://github.com/horovod/horovod}},
}

@misc{tensorboard,
  author={TensorFlow},
  title={TensorBoard},
  year={2022},
  howpublished={\url{https://www.tensorflow.org/tensorboard}},
}

@misc{tfmodelgarden,
  author={TensorFlow},
  title={TensorFlow Model Garden},
  year={2022},
  publisher={GitHub},
  journal = {GitHub repository},
  howpublished={\url{https://github.com/tensorflow/models}},
}

@misc{tfexamplesdamien,
  author={Aymeric Damien},
  title={TensorFlow Examples},
  year={2022},
  publisher={GitHub},
  journal={GibHub repository},
  howpublished={\url{https://github.com/aymericdamien/TensorFlow-Examples}},
}

@misc{tf2tutogithub,
  author={Jackie Loong},
  title={TensorFlow 2.0 Tutorials},
  year={2022},
  publisher={GitHub},
  journal={GibHub repository},
  howpublished={\url{https://github.com/dragen1860/TensorFlow-2.x-Tutorials}},
}

@misc{cifar10github,
  author={\relax{Arconsis IT-Solutions GmbH}},
  title={CIFAR 10 with TensorFlow},
  year={2022},
  publisher={GitHub},
  journal={GibHub repository},
  howpublished={\url{https://github.com/arconsis/cifar-10-with-tensorflow2/blob/master/BetterNetwork.py}},
}

@article{facebook2018,
      title={Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}, 
      author={Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
      year={2018},
      Journal = {arXiv preprint arXiv:1706.02677},
      eprint={1706.02677},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@online{approaches2019Mao,
  author = {Lei Mao},
  title = {Data Parallelism VS Model Parallelism in Distributed Deep Learning Training},
  published = 2019,
  url = {https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/},
  urldate = {2020-01-12},
  originalyear = {2019}
}



@misc{silver2017alphazero,
      title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
      author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
      year={2017},
      eprint={1712.01815},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{zhang2019distrspeech,
  author={Zhang, Wei and Cui, Xiaodong and Finkler, Ulrich and Kingsbury, Brian and Saon, George and Kung, David and Picheny, Michael},
  journal={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Distributed Deep Learning Strategies for Automatic Speech Recognition}, 
  year={2019},
  volume={},
  number={},
  pages={5706-5710},
  doi={10.1109/ICASSP.2019.8682888}
}}

@ARTICLE{tian2020distrwebattack,
  author={Tian, Zhihong and Luo, Chaochao and Qiu, Jing and Du, Xiaojiang and Guizani, Mohsen},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={A Distributed Deep Learning System for Web Attack Detection on Edge Devices}, 
  year={2020},
  volume={16},
  number={3},
  pages={1963-1971},
  doi={10.1109/TII.2019.2938778}
}

@article{bianco2018benchmark,
   title={Benchmark Analysis of Representative Deep Neural Network Architectures},
   volume={6},
   ISSN={2169-3536},
   howpublished={\url{http://dx.doi.org/10.1109/ACCESS.2018.2877890}},
   DOI={10.1109/access.2018.2877890},
   journal={IEEE Access},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Bianco, Simone and Cadene, Remi and Celona, Luigi and Napoletano, Paolo},
   year={2018},
   pages={64270–64277}
}

@book{tapl,
author = {Pierce, Benjamin C.},
title = {Types and Programming Languages},
year = {2002},
isbn = {0262162091},
publisher = {The MIT Press},
edition = {1st},
}

@misc{pythonref,
  title = {The Python Language Reference},
  author = {\relax{Python Software Foundation}},
  year = {2022},
  url = {https://docs.python.org/3/reference/}
}

@misc{horovodtf,
  author = {\relax{The Horovod Authors}},
  year = {2019},
  title = {Horovod with TensorFlow},
  url = {https://horovod.readthedocs.io/en/stable/tensorflow.html} 
}

@misc{horovodmpi,
  author = {\relax{The Horovod Authors}},
  year = {2019},
  title = {Horovod with MPI},
  url = {https://horovod.readthedocs.io/en/stable/mpi_include.html},
}

@misc{openmpiorg,
  author       = {Edgar Gabriel and Graham E. Fagg and George Bosilca
                  and Thara Angskun and Jack J. Dongarra and Jeffrey
                  M. Squyres and Vishal Sahay and Prabhanjan Kambadur
                  and Brian Barrett and Andrew Lumsdaine and Ralph
                  H. Castain and David J. Daniel and Richard L. Graham
                  and Timothy S. Woodall },
  title        = {Open {MPI}: Goals, Concept, and Design of a Next
                  Generation {MPI} Implementation},
  booktitle    = {Proceedings, 11th European PVM/MPI Users' Group
                  Meeting},
  year         = 2004,
  address      = {Budapest, Hungary},
  pages        = {97--104},
  month        = {September}
} 

@misc{tf2announce,
  title = {TensorFlow 2.0 is now available!},
  author = {\relax{The TensorFlow team}},
  year = {2019},
  howpublished = {\url{https://blog.tensorflow.org/2019/09/tensorflow-20-is-now-available.html}},
}

@misc{keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@online{monitoredsession,
  author = {\relax{The TensorFlow Team}},
  title = {tf.compat.v1.train.MonitoredSession},
  howpublished = {\url{https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MonitoredSession}},
  urldate = {2022-02-03},
  originalyear = {2022}
}

@inproceedings{bryan2004peg,
author = {Ford, Bryan},
title = {Parsing Expression Grammars: A Recognition-Based Syntactic Foundation},
year = {2004},
isbn = {158113729X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
howpublished = {\url{https://doi.org/10.1145/964001.964011}},
doi = {10.1145/964001.964011},
booktitle = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {111–122},
numpages = {12},
keywords = {parsing expression grammars, context-free grammars, scannerless parsing, BNF, unified grammars, packrat parsing, TDPL, regular expressions, GTDPL, syntactic predicates, lexical analysis},
location = {Venice, Italy},
series = {POPL '04}
}

@inproceedings{bryan02packrat,
author = {Ford, Bryan},
title = {Packrat Parsing: Simple, Powerful, Lazy, Linear Time, Functional Pearl},
year = {2002},
isbn = {1581134878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
howpublished = {\url{https://doi.org/10.1145/581478.581483}},
doi = {10.1145/581478.581483},
booktitle = {Proceedings of the Seventh ACM SIGPLAN International Conference on Functional Programming},
pages = {36–47},
numpages = {12},
keywords = {lexical analysis, scannerless parsing, Haskell, top-down parsing, memoization, backtracking, parser combinators},
location = {Pittsburgh, PA, USA},
series = {ICFP '02}
}

@misc{scalaparser,
  author = {\relax{The Scala Programming Language}},
  title={Scala Parser Combinators},
  year={2014},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished={\url{https://github.com/scala/scala-parser-combinators}} 
}



@article{doi:10.3102/1076998619872761,
  author = {Bo Pang and Erik Nijkamp and Ying Nian Wu},
  title ={Deep Learning With TensorFlow: A Review},
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {45},
  number = {2},
  pages = {227-248},
  year = {2020},
  doi = {10.3102/1076998619872761},
  URL = {
          https://doi.org/10.3102/1076998619872761
  },
  eprint = {
          https://doi.org/10.3102/1076998619872761
  }
}

@inproceedings {tensorflow,
author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
title = {{TensorFlow}: A System for {Large-Scale} Machine Learning},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {265--283},
howpublished = {\url{https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi}},
publisher = {USENIX Association},
month = nov,
}

@InProceedings{Loulergue2020,
  author    = {Loulergue, Fr{\'e}d{\'e}ric and Philippe, Jolan},
  booktitle = {Algorithms and Architectures for Parallel Processing},
  title     = {Automatic Optimization of Python Skeletal Parallel Programs},
  year      = {2020},
  address   = {Cham},
  editor    = {Wen, Sheng and Zomaya, Albert and Yang, Laurence T.},
  pages     = {183--197},
  publisher = {Springer International Publishing},
  abstract  = {Skeletal parallelism is a model of parallelism where parallel constructs are provided to the programmer as usual patterns of parallel algorithms. High-level skeleton libraries often offer a global view of programs instead of the common Single Program Multiple Data view in parallel programming. A program is written as a sequential program but operates on parallel data structures. Most of the time, skeletons on a parallel data structure have counterparts on a sequential data structure. For example, the map function that applies a given function to all the elements of a sequential collection (e.g., a list) has a map skeleton counterpart that applies a sequential function to all the elements of a distributed collection.},
  groups    = {Code Trans. for ML},
  isbn      = {978-3-030-38991-8},
}


@InProceedings{mlcatchup,
  author    = {Haryono, Stefanus A. and Thung, Ferdian and Lo, David and Lawall, Julia and Jiang, Lingxiao},
  booktitle = {2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  title     = {MLCatchUp: Automated Update of Deprecated Machine-Learning APIs in Python},
  year      = {2021},
  pages     = {584-588},
  doi       = {10.1109/ICSME52107.2021.00061},
  groups    = {Code Trans. for ML},
}


@Misc{torchfx,
  author    = {Reed, James K. and DeVito, Zachary and He, Horace and Ussery, Ansley and Ansel, Jason},
  title     = {Torch.fx: Practical Program Capture and Transformation for Deep Learning in Python},
  year      = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2112.08429},
  groups    = {Code Trans. for ML},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  publisher = {arXiv},
  howpublished       = {\url{https://arxiv.org/abs/2112.08429}},
}

@misc{tfonspark,
  author= {Yahoo},
  title = {TensorFlowOnSpark},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/yahoo/TensorFlowOnSpark}},
}

@incollection{pytorch2019,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
howpublished = {\url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}}
}

@inproceedings{deepspeed,
author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
title = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406703},
doi = {10.1145/3394486.3406703},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3505–3506},
numpages = {2},
keywords = {distributed deep learning, machine learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}


@Article{Visser2001,
  author   = {Eelco Visser},
  journal  = {Electronic Notes in Theoretical Computer Science},
  title    = {A Survey of Rewriting Strategies in Program Transformation Systems},
  year     = {2001},
  issn     = {1571-0661},
  note     = {WRS 2001, 1st International Workshop on Reduction Strategies in Rewriting and Programming},
  pages    = {109-143},
  volume   = {57},
  doi      = {https://doi.org/10.1016/S1571-0661(04)00270-1},
  groups   = {Code Trans. for ML},
  url      = {https://www.sciencedirect.com/science/article/pii/S1571066104002701},
}


@Article{LeCun2015,
author={LeCun, Yann
and Bengio, Yoshua
and Hinton, Geoffrey},
title={Deep learning},
journal={Nature},
year={2015},
month={May},
day={01},
volume={521},
number={7553},
pages={436-444},
abstract={Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
issn={1476-4687},
doi={10.1038/nature14539},
url={https://doi.org/10.1038/nature14539}
}


@misc{vggnet2014,
  doi = {10.48550/ARXIV.1409.1556},
  url = {https://arxiv.org/abs/1409.1556},
  author = {Simonyan, Karen and Zisserman, Andrew},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{resnet2015,
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Residual Learning for Image Recognition},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{bert2018,
  doi = {10.48550/ARXIV.1810.04805},
  url = {https://arxiv.org/abs/1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{gpt32020,
  doi = {10.48550/ARXIV.2005.14165},
  url = {https://arxiv.org/abs/2005.14165},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models are Few-Shot Learners},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{imagenet2014,
  doi = {10.48550/ARXIV.1409.0575},
  url = {https://arxiv.org/abs/1409.0575},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences, I.4.8; I.5.2},
  title = {ImageNet Large Scale Visual Recognition Challenge},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{imagenettraining2017,
  doi = {10.48550/ARXIV.1709.05011},
  url = {https://arxiv.org/abs/1709.05011},
  author = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ImageNet Training in Minutes},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@Article{Silver2017alphagozero,
author={Silver, David
and Schrittwieser, Julian
and Simonyan, Karen
and Antonoglou, Ioannis
and Huang, Aja
and Guez, Arthur
and Hubert, Thomas
and Baker, Lucas
and Lai, Matthew
and Bolton, Adrian
and Chen, Yutian
and Lillicrap, Timothy
and Hui, Fan
and Sifre, Laurent
and van den Driessche, George
and Graepel, Thore
and Hassabis, Demis},
title={Mastering the game of Go without human knowledge},
journal={Nature},
year={2017},
month={Oct},
day={01},
volume={550},
number={7676},
pages={354-359},
abstract={A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
issn={1476-4687},
doi={10.1038/nature24270},
url={https://doi.org/10.1038/nature24270}
}

