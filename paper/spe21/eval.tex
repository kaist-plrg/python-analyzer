\section{Evaluation}\label{sec:eval}

We implemented the formal transformation rule for the distributed training
as a software. We designed the software to be a standalone software, so 
the ML developers can utilize the software regardless of their environment. 
The software is written in Scala; the source code and released software
is available at https://github.com/kaist-plrg/python-analyzer.

To evaluate the automated transformation tool, we set up two research
questions.

\begin{itemize}
\item RQ1. (Correctness) Does the tool correctly transform single-GPU based DL model codes into
the corresponding distributed model codes?

\item RQ2. (Effectiveness) Does the automatic transformation result in speed-up 
in distributed training compared to single-GPU training?
\end{itemize}

To answer these questions,  
we designed two experiments: \textbf{transformation experiment} and
\textbf{distributed training experiment}.
The transformation experiment aims to answer the RQ1; it tests if the
transformation tool correctly transforms the target model codes into
the answer code.
The distributed training experiment aims to answer the RQ2;
it measures the training speed of single-GPU based model and its
corresponding transformed distributed model and compare them.
In this section, we describe the experiment setting,
and discuss the results of the experiments and its implications.

\subsection{Experiment Targets and Settings}

We gathered target DL model codes to be transformed.
The target models are open-source DL model codes written in TensorFlow.
The models come from two sources. The first source is the official
Horovod code examples published in the Horovod GitHub\cite{horovodgithub}. 
The Horovod repository's example directory contains examples of
distributed training codes using several DL frameworks.  
We selected the codes for TensorFlow and Keras,
which are the target libraries of the transformation tool.

(TODO: should check ETRI model sources)

Selected example models are manually rewritten into
single-GPU based model codes. To manually rewrite the distributed Horovod
training codes, cetrain API calls are eliminated and rest are remained.   

(TODO: clarify with example)

The second source is the TensorFlow 2.x tutorials published in
GitHub\cite{tf2tutogithub}. The GitHub repository provides over twenty
TensorFlow DL models of various architectures. 
We selected the TensorFlow 2.x tutorial codes in addition to the official
Horovod examples codes in order to show that the transformation tool
also works well on the codes of various DL architectures.

We excluded some models from experiments to keep only trainable model codes. 
For example, the CycleGAN model in the TensorFlow 2.x tutorials
was unsuable because the training dataset was not accessible from the
Internet. Additionally we modified some parts of the model in order to patch
minor bugs in the code in training.

\subsection{Transformation Experiment}

The transformation experiment aims to evaluate the correctness of the
transformation tool. To measure the correctness, we pair up every
target model codes with \textit{"answer"} codes. In specific,      
each single-GPU based codes are considered as our \textit{target codes},
and each target codes are paired with \textit{answer codes}, which are
correct implementations of the distributed version of the target codes. 
We manually paired each target model code with a corresponding answer codes.
For a target code from the official Horovod example, each single-GPU based
model codes are paired with the original Horovod model codes.     


\subsection{Distributed Training Experiment}

Second, we compared the training speed between the original models and
corresponding transformed model. 
To quantify the training speed, we measured the epoch loss during the training
of each model.
The loss value is a result value of the loss function which is computed
at each epoch or step in the ML training process.
In theory, the loss value derease over training time and converge
to some low value.
The training speed can be understood as the time required in the
loss convergence.

We measure the per-epoch loss during the traininig using
TensorBoard library \cite{tensorboard}. 
TensorBoard is a library that provides visualization and tools
for ML experiments.
We used the TensorBoard APIs to record the loss value every traininig epoch
to gather the data and visualize in time-loss graph.

\newcommand{\orgbf}{\textbf{ORG}}
\newcommand{\hvdbf}{\textbf{HVD}}

The distributed training experiment is done as follows.
First, a pair of the original ML model and the corresponding transformed model
is prepared. For convenience, we refer to the original, single-GPU based model
as \textbf{ORG} model, and the auto-transformed, distributed model as
\textbf{HVD} model. To measure the epoch loss, we manually modify both  
\orgbf and \hvdbf training code to include the TensorBoard API. 
(TODO: detailed explain on TensorBoard code)

Second, we run the training of \textbf{ORG} model and
\textbf{HVD} model. We utilize the {\tt mpirun} command to execute both
\textbf{ORG} and \textbf{HVD} training. The command specifies the number of
GPUs used for the training; 1 GPU is used for \textbf{ORG} model training  
and 4 GPUs are used for \textbf{HVD} model training. 
(TODO: machine spec of the training server)
As mentioned earlier, the epoch loss is recorded and stored in the
TensorBoard log file.
After the training is done, the TensorBoard log file is used to plot the
graph between time and epoch loss. 

(TODO: add the graph figure)

The results of the distributed traininig experiment 
are shown in the figure \ref{??}.
The x-axis of the graphs are the time passed in the training.
The red 

