\section{Evaluation}\label{sec:eval}

We implemented the formal transformation rule for the distributed training
as a software. We designed the software to be a standalone software, so 
the ML developers can utilize the software regardless of their environment. 
The software is written in Scala; the source code and released software
is available at https://github.com/kaist-plrg/python-analyzer.

To evaluate the automated transformation tool, we set up two research
questions.

\begin{itemize}
\item RQ1. (Correctness) Does the tool correctly transform single-GPU based DL model codes into
the corresponding distributed model codes?

\item RQ2. (Effectiveness) Does the automatic transformation result in speed-up 
in distributed training compared to single-GPU training?
\end{itemize}

To answer these questions,  
we designed two experiments: \textbf{transformation experiment} and
\textbf{distributed training experiment}.
The transformation experiment aims to answer the RQ1; it tests if the
transformation tool correctly transforms the target model codes into
the answer code.
The distributed training experiment aims to answer the RQ2;
it measures the training speed of single-GPU based model and its
corresponding transformed distributed model and compare them.
In this section, we describe the experiment setting,
and discuss the results of the experiments and its implications.

\subsection{Experiment Targets and Settings}

We gathered target DL model codes to be transformed.
The target models are open-source DL model codes written in TensorFlow.
The models come from two sources. The first source is the official
Horovod code examples published in the Horovod GitHub\cite{horovodgithub}. 
The Horovod repository's example directory contains examples of
distributed training codes using several DL frameworks.  
We selected the codes for TensorFlow and Keras,
which are the target libraries of the transformation tool.

(TODO: should check ETRI model sources)

Selected example models are manually rewritten into
single-GPU based model codes. To manually rewrite the distributed Horovod
training codes, cetrain API calls are eliminated and rest are remained.   

(TODO: clarify with example)

The second source is the TensorFlow 2.x tutorials published in
GitHub\cite{tf2tutogithub}. The GitHub repository provides over twenty
TensorFlow DL models of various architectures. 
We selected the TensorFlow 2.x tutorial codes in addition to the official
Horovod examples codes in order to show that the transformation tool
also works well on the codes of various DL architectures.

We excluded some models from experiments to keep only trainable model codes. 
For example, the CycleGAN model in the TensorFlow 2.x tutorials
was unsuable because the training dataset was not accessible from the
Internet. Additionally we modified some parts of the model in order to patch
minor bugs in the code in training.
The full list of the experiment target models is described in the
figure \ref{fig:eval:targets}.

\begin{figure}[!ht]
  \begin{center}
  \begin{tabular}{|c|c|c|}
    \hline
    Model Name & API Pattern & Source \\
    \hline
    LSTM-MNIST & GradientTape & Americ Damien \\
    SimpleCNN-GradientTape-1 & GradientTape & Americ Damien \\
    SimpleCNN-GradientTape-2 & GradientTape & Horovod Examples \\
    SimpleCNN-MonitoredSession & MonitoredSession & Horovod Examples \\
    SimpleCNN-Session & Session & TensorFlow Model Garden \\
    VGG-CIFAR10 & Keras & CIFAR-10 Example with TensorFlow 2.0 \\
    Play-with-MNIST & GradientTape & TensorFlow 2.x Tutorials \\
    Linear-Regression & GradientTape & TensorFlow 2.x Tutorials \\
    Fashion-MNIST & Keras & TensorFlow 2.x Tutorials \\
    CIFAR10-VGG16 & GradientTape & TensorFlow 2.x Tutorials\\
    Inception-Network & GradientTape & TensorFlow 2.x Tutorials \\
    RNN-Sentiment-Analysis & Keras & TensorFlow 2.x Tutorials \\
    Stacked-LSTM-ColorBot & GradientTape & TensorFlow 2.x Tutorials \\
    Auto-Encoder & GradientTape & TensorFlow 2.x Tutorials \\
    Variational-Auto-Encoder & GradientTape & TensorFlow 2.x Tutorials \\
    DCGAN & GradientTape & TensorFlow 2.x Tutorials \\
    BERT & Keras & TensorFlow 2.x Tutorials \\
    \hline
  \end{tabular}
  \end{center}
  \caption{List of the target Models}
  \label{fig:eval:targets}
\end{figure}

\subsection{Transformation Experiment}

The transformation experiment aims to evaluate the correctness of the
transformation tool. To measure the correctness, we pair up every
target model codes with \textit{"answer"} codes. In specific,      
each single-GPU based codes are considered as our \textit{target codes},
and each target codes are paired with \textit{answer codes}, which are
correct implementations of the distributed version of the target codes. 
We manually paired each target model code with a corresponding answer codes.
For target codes from the official Horovod example, each single-GPU based
model codes are paired with the original Horovod model codes.     
For other open-source model codes, we manually examined the training code
file and modified it according to the Horovod document.


\begin{figure}[!ht]
  \begin{center}
  \begin{tabular}{|c|c|c|}
    \hline
    Model Name & API Pattern & Transformation Success \\
    \hline
    LSTM-MNIST & GradientTape & o \\
    SimpleCNN-GradientTape-1 & GradientTape & o\\
    SimpleCNN-GradientTape-2 & GradientTape & x \\
    SimpleCNN-MonitoredSession & MonitoredSession &o\\
    SimpleCNN-Session & Session & o\\
    VGG-CIFAR10 & Keras & o \\ 
    Play-with-MNIST & GradientTape & o \\
    Linear-Regression & GradientTape & o \\
    Fashion-MNIST & Keras & o \\
    CIFAR10-VGG16 & GradientTape & o\\
    Inception-Network & GradientTape & o \\
    RNN-Sentiment-Analysis & Keras & o \\
    Stacked-LSTM-ColorBot & GradientTape & o \\
    Auto-Encoder & GradientTape & o \\
    Variational-Auto-Encoder & GradientTape & o \\
    DCGAN & GradientTape & o \\
    BERT & Keras & x \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Transformation experiment Result}
  \label{fig:eval:trans}
\end{figure}

The transformation experiment result is described in the 
figure \ref{fig:eval:trans}. Among 17 targets, only two models are failed to
be correctly transformed.
(TODO: detailed discussion on the failure, and why it is ok)

\subsection{Distributed Training Experiment}

Second, we compared the training speed between the original models and
corresponding transformed model. 
To quantify the training speed, we measured the epoch loss during the training
of each model.
The loss value is a result value of the loss function which is computed
at each epoch or step in the ML training process.
In theory, the loss value derease over training time and converge
to some low value.
The training speed can be understood as the time required in the
loss convergence.

We measure the per-epoch loss during the traininig using
TensorBoard library \cite{tensorboard}. 
TensorBoard is a library that provides visualization and tools
for ML experiments.
We used the TensorBoard APIs to record the loss value every traininig epoch
to gather the data and visualize in time-loss graph.

\newcommand{\orgbf}{\textbf{ORG}}
\newcommand{\hvdbf}{\textbf{HVD}}

The distributed training experiment is done as follows.
First, a pair of the original ML model and the corresponding transformed model
is prepared. For convenience, we refer to the original, single-GPU based model
as \textbf{ORG} model, and the auto-transformed, distributed model as
\textbf{HVD} model. To measure the epoch loss, we manually modify both  
\orgbf and \hvdbf training code to include the TensorBoard API. 
(TODO: detailed explain on TensorBoard code)

Second, we run the training of \textbf{ORG} model and
\textbf{HVD} model. We utilize the {\tt mpirun} command to execute both
\textbf{ORG} and \textbf{HVD} training. The command specifies the number of
GPUs used for the training; 1 GPU is used for \textbf{ORG} model training  
and 4 GPUs are used for \textbf{HVD} model training. 
(TODO: machine spec of the training server)
As mentioned earlier, the epoch loss is recorded and stored in the
TensorBoard log file.
After the training is done, the TensorBoard log file is used to plot the
graph between time and epoch loss. 

\begin{figure}[!ht]
  \includegraphics[width=\hsize]{tape-lstm-graph}
  \caption{Time-loss graph of LSTM-MNIST model}
\end{figure}

\begin{figure}[!ht]
  \includegraphics[width=\hsize]{tape-simplecnn1-graph}
  \caption{Time-loss graph of LSTM-MNIST model}
\end{figure}

(TODO: include other graphs, too, and make it sane size)

The results of the distributed traininig experiment 
are shown in the figure \ref{??}.
The points in the graphs represent the loss value of the given time;
sky blue points correspond to the \orgbf model training and
orange points coresspond to the \hvdbf model training.
The lines represent the smoothed loss values;
blue lines correspond to the \orgbf model training
and the red lines correspond to the \hvdbf model training.
As shown in the figure, all of the training data show that
the loss value decreases over time, meaning that the training
process is valid.

The distributed training experiment result show that the 8 out of 17 transformed
models show speedup in distributed traininig. The result is rather surprising,
considering that the distributed training itself is successfully executed
without errors. To further investigate the factors that influence the
training speed in the distributed training, 
we performed additional distributed training of some models with
different hyperparameters.

(TODO: include the result of exp with changed lr)

The results show that the training speed difference between single-GPU and
distributed training can differ according to the model hyperparameters
such as the learning rate. The fact that various combinations of hyperparameters
must be considered and experiement to reach optimal training speed is 
well-known in ML community. This problem also applies to distributed training,
where optimial hyperparameters should be re-searched after the training
code is transformed. Our transformation tool do not target to search the 
optimal hyperparameters, although we do implement the guidelines from the
official Horovod documents to scale learning rate and epoch numbers
by number of the GPUs.  
