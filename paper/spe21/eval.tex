\section{Evaluation}\label{sec:eval}

We implemented the formal transformation rule for the distributed training
as a software. We designed the software to be a standalone software, so 
the ML developers can utilize the software regardless of their environment. 
The software is written in Scala; the source code and released software
is available at https://github.com/kaist-plrg/python-analyzer.

To evaluate the automated transformation tool, we set up two research
questions.

\begin{itemize}
\item RQ1. (Correctness) Does the tool correctly transform single-GPU based DL model codes into
the corresponding distributed model codes?

\item RQ2. (Effectiveness) Does the automatic transformation result in speed-up 
in distributed training compared to single-GPU training?
\end{itemize}

To answer these questions,  
we designed two experiments: \textbf{transformation experiment} and
\textbf{distributed training experiment}.
The transformation experiment aims to answer the RQ1; it tests if the
transformation tool correctly transforms the target model codes into
the answer code.
The distributed training experiment aims to answer the RQ2;
it measures the training speed of single-GPU based model and its
corresponding transformed distributed model and compare them.
In this section, we describe the experiment setting,
and discuss the results of the experiments and its implications.

\subsection{Experiment Targets and Settings}

We gathered open-source TensorFlow DL model codes as the experiment target.
The full list of the experiment target models is described in the
figure \ref{fig:eval:targets}.
At first, we targetd the example model codes in the Horovod 
Github\cite{horovodgithub}. Although the Horovod GitHub repository provides
examples for various ML frameworks, we could only use two models based on the
TensorFlow frameworks. As a result, we searched for other GitHub repositories
that contain TensorFlow ML model codes.
The TensorFlow model garden\cite{tfmodelgarden} is one of the official
collections of ML models from TensorFlow.
Other sources\cite{tfexamplesdamien}\cite{cifar10github}\cite{tf2tutogithub} are
non-official ML examples that include various ML architectures.
We selected targets with various ML architectures to show that
our transformation tool works well regardless of the architecture types.

After gathering the models, we selected valid experiment targets to be used. 
We excluded some models from experiments to keep only trainable model codes. 
For example, the CycleGAN model in the TensorFlow 2.x tutorials
was unsuable because the training dataset was not accessible from the
Internet. Additionally we modified some parts of the model in order to patch
minor bugs in the code in training.
Then, we manually rewrite each model code into distributed model codes using
the Horovod library. The manually rewritten codes will be used as "answers" 
for the transform experiment.
For the case of Horovod GitHub example codes, which are distirbuted model
codes from the start, we manually inspect the codes and eliminate
Horovod library-related codes to rewrite them into single-GPU based
models, and use them as the target codes.

\begin{figure}[!ht]
  \begin{center}
  \begin{tabular}{c|c|l}
    \hline
    Model Name & API Pattern & Source \\
    \hline
    LSTM-MNIST & GradientTape & \multirow{2}{*}{TensorFlow Examples by Americ Damien\cite{tfexamplesdamien}} \\
    SimpleCNN-GradientTape-1 & GradientTape \\
    \hline
    SimpleCNN-GradientTape-2 & GradientTape & \multirow{2}{*}{Horovod GitHub\cite{horovodgithub}} \\
    SimpleCNN-MonitoredSession & MonitoredSession  \\
    \hline
    SimpleCNN-Session & Session & TensorFlow Model Garden\cite{tfmodelgarden} \\
    \hline
    VGG-CIFAR10 & Keras & CIFAR-10 Example with TensorFlow 2.0\cite{cifar10github} \\
    \hline
    Play-with-MNIST & GradientTape & \multirow{11}{*}{TensorFlow 2.x Tutorials\cite{tf2tutogithub}} \\
    Linear-Regression & GradientTape  \\
    Fashion-MNIST & Keras  \\
    CIFAR10-VGG16 & GradientTape \\
    Inception-Network & GradientTape  \\
    RNN-Sentiment-Analysis & Keras  \\
    Stacked-LSTM-ColorBot & GradientTape  \\
    Auto-Encoder & GradientTape  \\
    Variational-Auto-Encoder & GradientTape  \\
    DCGAN & GradientTape  \\
    BERT & Keras  \\
    \hline
  \end{tabular}
  \end{center}
  \caption{List of the target Models}
  \label{fig:eval:targets}
\end{figure}

\subsection{Transformation Experiment}

The transformation experiment aims to evaluate the correctness of the
transformation tool. To measure the correctness, we pair up every
target model codes with \textit{"answer"} codes. In specific,      
each single-GPU based codes are considered as our \textit{target codes},
and each target codes are paired with \textit{answer codes}, which are
correct implementations of the distributed version of the target codes. 
We manually paired each target model code with a corresponding answer codes.
For target codes from the official Horovod example, each single-GPU based
model codes are paired with the original Horovod model codes.     
For other open-source model codes, we manually examined the training code
file and modified it according to the Horovod document.


\begin{figure}[!ht]
  \begin{center}
  \begin{tabular}{|c|c|c|}
    \hline
    Model Name & API Pattern & Transformation Success \\
    \hline
    LSTM-MNIST & GradientTape & o \\
    SimpleCNN-GradientTape-1 & GradientTape & o\\
    SimpleCNN-GradientTape-2 & GradientTape & x \\
    SimpleCNN-MonitoredSession & MonitoredSession &o\\
    SimpleCNN-Session & Session & o\\
    VGG-CIFAR10 & Keras & o \\ 
    Play-with-MNIST & GradientTape & o \\
    Linear-Regression & GradientTape & o \\
    Fashion-MNIST & Keras & o \\
    CIFAR10-VGG16 & GradientTape & o\\
    Inception-Network & GradientTape & o \\
    RNN-Sentiment-Analysis & Keras & o \\
    Stacked-LSTM-ColorBot & GradientTape & o \\
    Auto-Encoder & GradientTape & o \\
    Variational-Auto-Encoder & GradientTape & o \\
    DCGAN & GradientTape & o \\
    BERT & Keras & x \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Transformation experiment Result}
  \label{fig:eval:trans}
\end{figure}

The transformation experiment result is described in the 
figure \ref{fig:eval:trans}. Among 17 targets, only two models are failed to
be correctly transformed.
(TODO: detailed discussion on the failure, and why it is ok)

\subsection{Distributed Training Experiment}

Second, we compared the training speed between the original models and
corresponding transformed model. 
To quantify the training speed, we measured the epoch loss during the training
of each model.
The loss value is a result value of the loss function which is computed
at each epoch or step in the ML training process.
In theory, the loss value derease over training time and converge
to some low value.
The training speed can be understood as the time required in the
loss convergence.

We measure the per-epoch loss during the traininig using
TensorBoard library \cite{tensorboard}. 
TensorBoard is a library that provides visualization and tools
for ML experiments.
We used the TensorBoard APIs to record the loss value every traininig epoch
to gather the data and visualize in time-loss graph.

\newcommand{\orgbf}{\textbf{ORG}}
\newcommand{\hvdbf}{\textbf{HVD}}

The distributed training experiment is done as follows.
First, a pair of the original ML model and the corresponding transformed model
is prepared. For convenience, we refer to the original, single-GPU based model
as \textbf{ORG} model, and the auto-transformed, distributed model as
\textbf{HVD} model. To measure the epoch loss, we manually modify both  
\orgbf and \hvdbf training code to include the TensorBoard API. 
(TODO: detailed explain on TensorBoard code)

Second, we run the training of \textbf{ORG} model and
\textbf{HVD} model. We utilize the {\tt mpirun} command to execute both
\textbf{ORG} and \textbf{HVD} training. The command specifies the number of
GPUs used for the training; 1 GPU is used for \textbf{ORG} model training  
and 4 GPUs are used for \textbf{HVD} model training. 
(TODO: machine spec of the training server)
As mentioned earlier, the epoch loss is recorded and stored in the
TensorBoard log file.
After the training is done, the TensorBoard log file is used to plot the
graph between time and epoch loss. 

\begin{figure}[!ht]
  \includegraphics[width=\hsize]{tape-lstm-graph}
  \caption{Time-loss graph of LSTM-MNIST model}
\end{figure}

\begin{figure}[!ht]
  \includegraphics[width=\hsize]{tape-simplecnn1-graph}
  \caption{Time-loss graph of LSTM-MNIST model}
\end{figure}

(TODO: include other graphs, too, and make it sane size)

The results of the distributed traininig experiment 
are shown in the figure \ref{??}.
The points in the graphs represent the loss value of the given time;
sky blue points correspond to the \orgbf model training and
orange points coresspond to the \hvdbf model training.
The lines represent the smoothed loss values;
blue lines correspond to the \orgbf model training
and the red lines correspond to the \hvdbf model training.
As shown in the figure, all of the training data show that
the loss value decreases over time, meaning that the training
process is valid.

The distributed training experiment result show that the 8 out of 17 transformed
models show speedup in distributed traininig. The result is rather surprising,
considering that the distributed training itself is successfully executed
without errors. To further investigate the factors that influence the
training speed in the distributed training, 
we performed additional distributed training of some models with
different hyperparameters.

(TODO: include the result of exp with changed lr)

The results show that the training speed difference between single-GPU and
distributed training can differ according to the model hyperparameters
such as the learning rate. The fact that various combinations of hyperparameters
must be considered and experiement to reach optimal training speed is 
well-known in ML community. This problem also applies to distributed training,
where optimial hyperparameters should be re-searched after the training
code is transformed. Our transformation tool do not target to search the 
optimal hyperparameters, although we do implement the guidelines from the
official Horovod documents to scale learning rate and epoch numbers
by number of the GPUs.  
