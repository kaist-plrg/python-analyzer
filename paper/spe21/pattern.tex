\section{Training API Pattern Analysis}\label{sec:pattern}

\subsection{TensorFlow ML Training API Patterns}

\begin{figure}[ht!]
\centering
  \begin{subfigure}[b]{0.4\textwidth}
    \begin{lstlisting}[language=Python]
for x, y in train_data.take(training_steps):
    with tf.GradientTape() as tape:
        pred = model(x, is_training=True)
        loss = loss_compute(y, pred)

    trainable_vars = model.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)
    pairs = zip(gradients, trainable_vars)
    optimizer.apply_gradients(pairs) 
    \end{lstlisting}
    \caption{Using low-level training API}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
model.compile(
    optimizer = optimizer, 
    loss = loss_compute) 
model.fit(train_data.take(training_steps))
    \end{lstlisting} 
    \caption{Using high-level training API}
  \end{subfigure}

  \caption{TensorFlow model code example using two different API patterns}
  \label{fig:pattern:ex01}
\end{figure}

TensorFlow provides various APIs to define the training process.
The figure \ref{fig:pattern:ex01} illustrates two examples of using 
different APIs for the model training.
Line 2 to 10 uses low-level APIs to repeat training steps over the training
data set. Inside the {\tt for} loop, 
the code uses the {\tt GradientTape} instance to record the loss computation,
then uses the {\tt gradient} method and the {\tt apply\_gradient} method to
back-propagate the gradient to the model parameters.
In contrast, lines 13 and 14 use high-level APIs to automatically
invoke the model training process by two methods calls, {\tt compile} and
{\tt fit}. The {\tt fit} method repeats the same process with the {\tt for}
loop in line 2; it takes each training data from the set and
applies the gradient to the model parameter.
(TODO: update explanation to figure 6)

While both code fragments train the model in the same way, 
the code structures significantly differ.
Using high-level APIs simplifies the program,
reducing additional codes that compute the prediction loss and apply gradients.
Low-level APIs are verbose, however, developers can fully control
the training process.
Developers can freely choose from different training code styles to
take advantage of each way.

To correctly transform the training codes, 
the transformation software must transform different training APIs
based on different transformation rules. 
We implemented an \textit{API pattern analysis} that identifies
the TensorFlow API usage with \textit{training API patterns} 
Each training API pattern is a set of code patterns for AST,
and code patterns are ASTs with
special holes that are later matched with concrete value.
When a code pattern is matched against a code AST,
it succeeds with the holes matched with the AST subnodes
or fails to match the given AST.
The software matches a training API pattern to the target training code,
and if the match succeeds, the corresponding transformation rule
is used to transform the training code.

We manually inspected multiple TensorFlow training codes to identify
common patterns of training-related API usage. 
To this end, we defined four training API patterns
that effectively identify kinds of training APIs usage in the target code.
Figure \ref{tab:patterns} briefly explains each training API pattern.
Note that each pattern is assigned with different TensorFlow versions.
The transformation software also supports legacy training codes
that use compatibility modules for TensorFlow version 1.x.

\begin{figure}
  \centering
  \begin{tabular}{|c|c|l|}
    \hline
    TF version & Pattern name & Explanation \\
    \hline
    1.x & Session & Low-level training API using {\tt Session} instance\\
    \hline
    1.x & MonitoredSession & Low-level training API using {\tt MonitoresSession} instance \\
    \hline
    2.x & GradientTape & Low-level training API using {\tt GradientTape} instance\\
    \hline
    2.x & Keras & High-level training API using {\tt fit} method of {\tt keras.models.Model} instance\\
    \hline
  \end{tabular}
  \caption{Training API patterns}
  \label{tab:patterns}
\end{figure}

We now explain each training API pattern with concrete code examples.

\textbf{Session Pattern} 
The {\tt tf.compat.v1.train.Session} instance provides methods to
invoke computations on the computation graph.
Developers use the {\tt run} method to run the optimization on the model,
usually inside the {\tt for} loop to repeat the process with training data.
The Session pattern matches call expressions that the function
call expression is the method {\tt tensorflow.Session.run}.

\begin{figure}[!ht]
\begin{lstlisting}[language=Python]
with tf.Session() as sess:
    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):
      sess.run(optimizer, feed_dict=feed_dict)
\end{lstlisting}
\caption{Session pattern code example}
\end{figure}

\textbf{MonitoredSession Pattern}
The {\tt tf.compat.v1.train.MonitoredSession} is used to handle initialization,
recovery, and hooks in the training process\cite{monitoredsession}.
Similar to the {\tt Session} instance, the {\tt run} method of
{\tt MonitoredSession} instance is used to invoke the training computation.
In addition, {\tt MonitoredSession} instances can automatically
initialize and invoke methods of {\tt Hook} objects.
The MonitoresSession pattern matches the constructor calls for the
{\tt MonitoresSession} instance in the training code.

\begin{figure}[!ht]
  \begin{lstlisting}[language=Python]
with tf.train.MonitoredTrainingSession(hooks=hooks) as mon_sess:
    while not mon_sess.should_stop():
        mon_sess.run(train_op, feed_dict=feed_dict)
  \end{lstlisting}
  \caption{MonitoredSession pattern code example}
\end{figure}


\textbf{GradientTape Pattern}
In TensorFlow 2.x, the computations are eagerly evaluated.
Unlike in TensorFlow 1.x, developers do not need to explicitly invoke the computations
with {\tt Session} or {\tt MonitoredSession} instances.
Instead, TensorFlow 2.x training codes use {\tt tf.GradientTape} instances
to record the forward pass operation and automatically compute the gradient.
The GradientTape pattern matches the call expressions {\tt with} statements
that construct {\tt GradientTape} instances.

\begin{figure}[!ht]
  \begin{lstlisting}[language=Python]
for x, y in train_data.take(training_steps):
  with tf.GradientTape() as g:
    pred = conv_net(x)
    loss = cross_entropy(pred, y)
    
  trainable_variables = list(weights.values()) + list(biases.values())
  gradients = g.gradient(loss, trainable_variables)
  optimizer.apply_gradients(zip(gradients, trainable_variables))
  \end{lstlisting}
  \caption{GradientTape pattern code example}
\end{figure}

\textbf{Keras Pattern}
Another common pattern to define the training process is to define
the model with {\tt tf.keras} API and automatically train the model
instance with {\tt fit} method.
In this method, the models are subclasses of {\tt tf.keras.models.Model} class
and inherits training-related method.
The Keras pattern matches the call expressions that is invoking the {\tt fit}
method of the subclass instances of {\tt tf.keras.models.Model}
In the pattern matching process, the API pattern analyzer
makes use of the CHG to identify subclass relationships. 

\begin{figure}[!ht]
  \begin{lstlisting}[language=Python]
class ResNet(tf.keras.models.Model):
  # model definition

model = ResNet([2, 2, 2], num_classes)
model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,
  \end{lstlisting}
  \caption{Keras pattern code example}
\end{figure}


\subsection{Implementation}

We implemented the training API using pattern matching.
The training API patterns are implemented in the forms of pattern matching
on the AST object. The ASTs are implemented as \textit{case classes}.
Case classes in Scala are equipped with destructors(the method {\tt unapply}),
so they can be used as patterns. 
As a result, training API patterns can be directly converted into
Scala pattern expressions.
