\section{Training API Pattern Analysis}\label{sec:pattern}

\subsection{TensorFlow ML Training API Patterns}

\begin{figure}[ht!]
\centering
  \begin{subfigure}[b]{0.4\textwidth}
    \begin{lstlisting}[language=Python]
for x, y in train_data.take(training_steps):
    with tf.GradientTape() as tape:
        pred = model(x, is_training=True)
        loss = loss_compute(y, pred)

    trainable_vars = model.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)
    pairs = zip(gradients, trainable_vars)
    optimizer.apply_gradients(pairs) 
    \end{lstlisting}
    \caption{Using low-level training API}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
model.compile(
    optimizer = optimizer, 
    loss = loss_compute) 
model.fit(train_data.take(training_steps))
    \end{lstlisting} 
    \caption{Using high-level training API}
  \end{subfigure}

  \caption{TensorFlow model code example using two different API patterns}
  \label{fig:pattern:ex01}
\end{figure}

TensorFlow provides various APIs to define the training process.
The figure \ref{fig:pattern:ex01} illustrates two examples of using 
different APIs for the model training.
Line 2 to 10 in the code \ref{fig:pattern:ex01}(a)  
uses low-level APIs to repeat training steps over the training
data set. Inside the {\tt for} loop, 
the code uses the {\tt GradientTape} instance to record the loss computation,
then uses the {\tt gradient} method and the {\tt apply\_gradient} method to
back-propagate the gradient to the model parameters.
In contrast, lines 1 and 4 in the code \ref{fig:pattern:ex01}(b)
uses high-level APIs to automatically
invoke the model training process by two methods calls, {\tt compile} and
{\tt fit}. The {\tt fit} method repeats the same process with the {\tt for}
loop in the code \ref{fig:pattern:ex01}(a); it takes each training data from the set and
applies the gradient to the model parameter.

While both code train the model in the same way, 
the code structures are significantly different.
Using high-level APIs simplifies the program,
reducing additional codes that compute the prediction loss and apply gradients.
Low-level APIs are verbose, however, developers can fully control
the training process.
Developers can freely choose from different training code styles to
take advantage of each way.

To correctly transform the training codes, 
the tool must apply different transformation rule for different usages
of training APIs.
We implemented an \textit{API pattern analysis} that identifies
the TensorFlow API usage with \textit{training API patterns} 
Each training API pattern is a set of code patterns for AST,
and code patterns are ASTs with
special holes that are later matched with concrete value.
When a code pattern is matched against a code AST,
it succeeds with the holes matched with the AST subnodes
or fails to match the given AST.
The software matches a training API pattern to the target training code,
and if the match succeeds, the corresponding transformation rule
is used to transform the training code.

We manually inspected multiple TensorFlow training codes to identify
common patterns of training-related API usage. 
We defined four training API patterns
that effectively identify kinds of training APIs usage in the target code.
Figure \ref{tab:patterns} briefly explains each training API pattern.
Note that each pattern is assigned with different TensorFlow versions.
The transformation software also supports legacy training codes
that use compatibility modules for TensorFlow version 1.x.

\begin{figure}
  \centering
  \begin{tabular}{|c|c|l|}
    \hline
    TF version & Pattern name & Explanation \\
    \hline
    1.x & Session & Low-level training API using {\tt Session} instance\\
    \hline
    1.x & MonitoredSession & Low-level training API using {\tt MonitoresSession} instance \\
    \hline
    2.x & GradientTape & Low-level training API using {\tt GradientTape} instance\\
    \hline
    2.x & Keras & High-level training API using {\tt fit} method of {\tt keras.models.Model} instance\\
    \hline
  \end{tabular}
  \caption{Training API patterns}
  \label{tab:patterns}
\end{figure}

We now explain each training API pattern with concrete code examples.

\textbf{Session Pattern} 
The Session pattern refers to TensorFlow 1.x training codes that
uses the {\tt Session} class instance to manually repeat the training 
computation in low-level manner.
Figure \ref{fig:sessionpattern} is a training code example of 
Session pattern.
In line 1, an instance of {\tt tf.compat.v1.train.Session} class is 
created with {\tt with} statement.
The {\tt Session} instance is an object providing a link to 
the TensorFlow runtime, equipped with methods related to the computation graph.
As in line 3, the method {\tt run} is called to invoke the
execution of computation on the pre-defined computation graph.
In the training process, {\tt Session.run} method is repeated mutiple times
so that the forward propagation of the training data and
backward propagation of gradient is performed many times.

To identify a Session pattern training code,
the analyzer searches for a {\tt with} statement that creates a
{\tt Session} instance.
If the body statements of {\tt with} statement calls a 
{\tt run} method of the {\tt Session} instance,
the analyzer concludes that the training code belongs to the Session pattern.

\begin{figure}[!ht]
\begin{lstlisting}[language=Python]
with tf.Session() as sess:
    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):
      sess.run(optimizer, feed_dict=feed_dict)
\end{lstlisting}
\caption{Session pattern code example}
\label{fig:sessionpattern}
\end{figure}

\textbf{MonitoredSession Pattern}
The MonitoredSession pattern refors to TensorFlow 1.x training codes that
uses the {\tt MonitoredSession} class instance to manually repeat
the training computation in low-level manner.
The pattern is same to the {\tt Session} pattern, except that the
{\tt MonitoredSession} instance is used instead of the {\tt Session} instance.
Figure \ref{fig:monsesspattern} is a code example of MonitoredSession pattern.
In line 1, the {\tt MonitoredSession} class instance is created with
the {\tt tf.traing.MonitoredTrainingSession} call inside {\tt with} statement
(Note that the constructor has different name with the class).
Similar to the Session pattern's case, line 3 calls the {\tt run} method of the
{\tt MonitoredSession} instance to repeat the training computation.

To identify a MoniotoredSession pattern training code,
the analyzer searches for a {\tt with} statement that creates a
{\tt MonitoredSession} instance.
If the body statements calls a {\tt run} methods of the {\tt MonitoredSession}
instance, the analyzer concludes that the training code belongs to the
MonitoredSession pattern.

The {\tt tf.compat.v1.train.MonitoredSession} is used to handle initialization,
recovery, and hooks in the training process\cite{monitoredsession}.
Similar to the {\tt Session} instance, the {\tt run} method of
{\tt MonitoredSession} instance is used to invoke the training computation.
In addition, {\tt MonitoredSession} instances can automatically
initialize and invoke methods of {\tt Hook} objects.
The MonitoresSession pattern matches the constructor calls for the
{\tt MonitoresSession} instance in the training code.

\begin{figure}[!ht]
  \begin{lstlisting}[language=Python]
with tf.train.MonitoredTrainingSession(hooks=hooks) as mon_sess:
    while not mon_sess.should_stop():
        mon_sess.run(train_op, feed_dict=feed_dict)
  \end{lstlisting}
  \label{fig:monsesspattern}
  \caption{MonitoredSession pattern code example}
\end{figure}


\textbf{GradientTape Pattern}
In TensorFlow 2.x, the computations are eagerly evaluated.
Unlike in TensorFlow 1.x, developers do not need to explicitly invoke the computations
with {\tt Session} or {\tt MonitoredSession} instances.
Instead, TensorFlow 2.x training codes use {\tt tf.GradientTape} instances
to record the forward pass operation and automatically compute the gradient.
The GradientTape pattern matches the call expressions {\tt with} statements
that construct {\tt GradientTape} instances.

\begin{figure}[!ht]
  \begin{lstlisting}[language=Python]
for x, y in train_data.take(training_steps):
  with tf.GradientTape() as g:
    pred = conv_net(x)
    loss = cross_entropy(pred, y)
    
  trainable_variables = list(weights.values()) + list(biases.values())
  gradients = g.gradient(loss, trainable_variables)
  optimizer.apply_gradients(zip(gradients, trainable_variables))
  \end{lstlisting}
  \caption{GradientTape pattern code example}
\end{figure}

\textbf{Keras Pattern}
Another common pattern to define the training process is to define
the model with {\tt tf.keras} API and automatically train the model
instance with {\tt fit} method.
In this method, the models are subclasses of {\tt tf.keras.models.Model} class
and inherits training-related method.
The Keras pattern matches the call expressions that is invoking the {\tt fit}
method of the subclass instances of {\tt tf.keras.models.Model}
In the pattern matching process, the API pattern analyzer
makes use of the CHG to identify subclass relationships. 

\begin{figure}[!ht]
  \begin{lstlisting}[language=Python]
class ResNet(tf.keras.models.Model):
  # model definition

model = ResNet([2, 2, 2], num_classes)
model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,
  \end{lstlisting}
  \caption{Keras pattern code example}
\end{figure}

