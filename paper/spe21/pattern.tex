\section{Training API Pattern Analysis}\label{sec:pattern}

\subsection{TensorFlow ML Training API Patterns}

\begin{figure}[ht!]
\centering
  \begin{subfigure}[b]{0.4\textwidth}
    \begin{lstlisting}[language=Python]
for x, y in train_data.take(training_steps):
    with tf.GradientTape() as tape:
        pred = model(x, is_training=True)
        loss = loss_compute(y, pred)

    trainable_vars = model.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)
    pairs = zip(gradients, trainable_vars)
    optimizer.apply_gradients(pairs) 
    \end{lstlisting}
    \caption{Using low-level training API}
  \end{subfigure}
  \hspace{5mm}
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{lstlisting}[language=Python]
model.compile(
    optimizer = optimizer, 
    loss = loss_compute) 
model.fit(train_data.take(training_steps))
    \end{lstlisting} 
    \caption{Using high-level training API}
  \end{subfigure}

  \caption{TensorFlow model code example using two different API patterns}
  \label{fig:pattern:ex01}
\end{figure}

TensorFlow provides various APIs to define the training process.
The figure \ref{fig:pattern:ex01} illustrates two examples of using 
different training APIs.
Lines 2 to 10 in the code \ref{fig:pattern:ex01}(a)  
uses low-level APIs to repeat training steps over the training
data set. Inside the {\tt for} loop, 
the {\tt GradientTape} instance records the operations,
then uses the {\tt gradient} method and the {\tt apply\_gradient} method to
optimize the model parameters.
In contrast, lines 1 and 4 in the code \ref{fig:pattern:ex01}(b)
uses high-level APIs to automatically
invoke the model training process. 
The {\tt fit} method automatically repeats the training steps,
which eliminates loops and manual operations from the written code.
While both codes train the model in the same way, 
their code structures are significantly different.
Using high-level APIs simplifies the program,
reducing additional codes that compute the prediction loss and apply gradients.
Low-level APIs are verbose, however, developers can fully control
the training process.
Developers can freely choose from different training APIs to
take advantage of each way.

To correctly transform the codes using different TensorFlow APIs, 
the tool must apply different transformation rules for different usages
of the APIs.
We manually inspected open-source TensorFlow DL models to identify
common patterns of training-related API usage.
In this end, we defined four training API patterns and 
an \textit{API pattern analysis} that categorizes TensorFlow DL models
into one of the four training API patterns.
Training API patterns are sets of code patterns that can match
parts of the DL model code using TensorFlow APIs.
Figure \ref{tab:patterns} describe the training API patterns.
When one of the API patterns is matched against an input model code,
we can conclude whether the model belongs to the API pattern category or not. 
Then we defined four different transformation rules for each training API
pattern.
Given an input model code, we match four training API patterns to the code
to identify  which training API pattern the code uses and apply corresponding
transformation rule to correctly transform the input code.

\begin{figure}
  \centering
  \begin{tabular}{|c|c|l|}
    \hline
    TF version & Pattern name & Explanation \\
    \hline
    1.x & Session & Low-level training API using {\tt Session} instance\\
    \hline
    1.x & MonitoredSession & Low-level training API using {\tt MonitoresSession} instance \\
    \hline
    2.x & GradientTape & Low-level training API using {\tt GradientTape} instance\\
    \hline
    2.x & Keras & High-level training API using {\tt fit} method of {\tt keras.models.Model} instance\\
    \hline
  \end{tabular}
  \caption{Training API patterns}
  \label{tab:patterns}
\end{figure}

\subsection{Explanation of the training API patterns}

In this section, we explain each training API pattern with
concrete code examples and method to identify the pattern
within a given input code.

\subsubsection{Session Pattern}

\begin{figure}[!ht]
\begin{lstlisting}[language=Python]
with tf.Session() as sess:
    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):
      sess.run(optimizer, feed_dict=feed_dict)
\end{lstlisting}
\caption{Session pattern code example}
\label{fig:sessionpattern}
\end{figure}
The Session pattern refers to TensorFlow 1.x training codes that
uses the {\tt Session} class instance to manually repeat the training 
computation in low-level manner.
Figure \ref{fig:sessionpattern} is a training code example of 
Session pattern.
In line 1, an instance of {\tt tf.compat.v1.train.Session} class is 
created with {\tt with} statement.
The {\tt Session} instance is an object providing a link to 
the TensorFlow runtime, equipped with methods related to the computation graph.
As in line 3, the method {\tt run} is called to invoke the
execution of computation on the pre-defined computation graph.
In the training process, {\tt Session.run} method is repeated mutiple times
so that the forward propagation of the training data and
backward propagation of gradient is performed many times.

To identify a Session pattern training code,
the analyzer searches for a {\tt with} statement that creates a
{\tt Session} instance.
If the body statements of {\tt with} statement calls a 
{\tt run} method of the {\tt Session} instance,
the analyzer concludes that the training code belongs to the Session pattern.

\subsubsection{MonitoredSession Pattern}

\begin{figure}[!ht]
  \begin{lstlisting}[language=Python]
with tf.train.MonitoredTrainingSession(hooks=hooks) as mon_sess:
    while not mon_sess.should_stop():
        mon_sess.run(train_op, feed_dict=feed_dict)
  \end{lstlisting}
  \caption{MonitoredSession pattern code example}
  \label{fig:monsesspattern}
\end{figure}

The MonitoredSession pattern refers to TensorFlow 1.x training codes that
uses the {\tt MonitoredSession} class instance to manually repeat
the training computation in low-level manner.
The pattern is same to the {\tt Session} pattern, except that the
{\tt MonitoredSession} instance is used instead of the {\tt Session} instance.
Figure \ref{fig:monsesspattern} is a code example of MonitoredSession pattern.
In line 1, the {\tt MonitoredSession} class instance is created with
the {\tt tf.traing.MonitoredTrainingSession} call inside {\tt with} statement
(Note that the constructor has different name with the class).
Similar to the Session pattern's case, line 3 calls the {\tt run} method of the
{\tt MonitoredSession} instance to repeat the training computation.

To identify a MoniotoredSession pattern training code,
the analyzer searches for a {\tt with} statement that creates a
{\tt MonitoredSession} instance.
If the body statements calls a {\tt run} methods of the {\tt MonitoredSession}
instance, the analyzer concludes that the training code belongs to the
MonitoredSession pattern.


\subsubsection{GradientTape Pattern}

\begin{figure}[!ht]
  \begin{lstlisting}[language=Python]
for x, y in train_data.take(training_steps):
  with tf.GradientTape() as g:
    pred = conv_net(x)
    loss = cross_entropy(pred, y)
    
  trainable_variables = list(weights.values()) + list(biases.values())
  gradients = g.gradient(loss, trainable_variables)
  optimizer.apply_gradients(zip(gradients, trainable_variables))
  \end{lstlisting}
  \caption{GradientTape pattern code example}
  \label{fig:tapepattern}
\end{figure}



The GradientTape pattern refers to TensorFlow 2.x training codes that
uses the {\tt GradientTape} class instnace tot manually repeat the 
training computation in low-level manner.
Figure \ref{fig:tapepattern} is a code example of GradientTape pattern.
In line 2, the {\tt tf.GradientTape} instance is created with
{\tt with} statement.
The {\tt GradientTape} instance is an object that records operations
occur inside the {\tt with} statement which itself was created. 
It's main usage is similar to TensorFlow 1.x {\tt Session} and
{\tt MonitoredSession} instance; however TensorFlow 2.x uses eager evaluation
by default, so the {\tt GradientTape} instance can be only used for
subset of training operation that its gradient requires to be computed.
In line 7, the gradients of model parameters is retrieved with
{\tt gradient} method on the {\tt GradientTape} instance.
Finally in line 8, the {\tt Optimizer} instance applies gradient descent with
{\tt apply\_gradients} method call.
The {\tt Optimizer} class represents various optimization algorithms studied in 
the DL domain.

To identify a GradientTape pattern training code,
the analyzer searches for a {\tt with} statement that creates a
{\tt GradientTape} instance.
The analyzer also searches for a statement that calls the {\tt apply\_gradients}
method of {\tt Optimizer} instance.
If both statements are found, the analyzer concludes that the code
belongs to the GradientTape pattern.

\subsubsection{Keras Pattern}

\begin{figure}[ht!]
  \begin{lstlisting}[language=Python]
# model definition
class ResNet(tf.keras.models.Model):
  def __init__(self, params):
    ...

model = ResNet([2, 2, 2], num_classes)
model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,
  \end{lstlisting}
 
  \caption{Keras pattern code example}
  \label{fig:keraspattern}
\end{figure}

The Keras pattern refers to TensorFlow 2.x training codes that uses
the Keras library to define the model and invoke an automatic
training process.
The Keras library\cite{keras} is a layer-based DL library which is
included in the TensorFlow platform.
In Keras training codes, DL models are represeted as instances of
subclass of {\tt tf.keras.models.Model} class.
The {\tt Model} class provides several training-related methods.
The {\tt compile} method allows developers to configure the training,
such as selecting optimization algorithm or setting hyperparameters.
The {\tt fit} method automatically trains the model with
the training dataset given as the method arguments.
These methods allow developers to abstract out the complex implementation
details of the training process and quickly start training.

Figure \ref{fig:keraspattern} is a code example of Keras pattern.
Line 1 defines a {\tt ResNet} class that inherits the 
{\tt tf.keras.models.Model} class. As mentioned earlier, the {\tt ResNet}
class represents a new model definition.
Implementation details of the {\tt ResNet} class is omitted from the 
figure; developers may freely define or extend methods that conforms to the
{\tt Model} class specification.
Line 6 creates an instance of {\tt ResNet} class, and line 7
calls the {\tt fit} method of the instance. 
The {\tt fit} method call here corresponds to the start of the training process
in the code example.

To identify a Keras pattern training code, the analyzer searches for the
{\tt Model.fit} method call. An important caveat here is to 
detect only the {\tt fit} method of the {\tt tf.keras.models.Model} 
subclass instances. The pattern analyzer make use of the class inheritance
information received from the class hierarchy analyzer to search for
the {\tt Model} subclass instances. In figure \ref{fig:keraspattern},
for example, the analyzer uses the fact that the {\tt ResNet} class is
a subclass of the {\tt Model} class to conclude that the {\tt model.fit}
is the {\tt Model.fit} method, and finally conclude that the code
belongs to Keras pattern.



