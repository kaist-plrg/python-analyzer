\section{Conclusion}\label{sec:conclusion}

We presented the automated code transformation for distributed training.  
By manually inspecting the documentations and code example,
we defined \textit{training API patterns} for categorizing TensorFlow
training codes by their API usage, and constructed \textit{transformation rules}
to distribute the trainig codes of each tranining patterns.
We implement the transformation in a form of software,
which includes class hierarchy analysis and pattern analysis to recognize
the correct transformation rule for the given input model
and automatically apply the transformation to produce
the corresponding distributed model as an output.
We evaluated the correctness of the transformation tool against
16 open-source DL models, which all but one transformations are
successful. Evaluating the training performance of the
transformed model showed us that none or only minimal amounts of
hyperparameter tuning is required for distributed training speedup. 
We believe that our transformation tool frees the users from
heavy burden of rewriting the model code, and allows them to
swiftly move from single-GPU-based training to distributed training.
In future works, we aim to search for methods that can fully automate
the deployments of DL models on distributed systems, including
automated hyperparameter tunings suited for the distributed system.
