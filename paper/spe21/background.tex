\section{Background}\label{sec:background}
\subsection{TensorFlow ML Models}
Describe the structure of TensorFlow ML Models.

(briefly introduce TF library concepts)

TensorFlow\cite{tensorflow} is a machine learning platform library
developed in 2016 by Google Brains.

(explain how TF was designed: limitation of previous frameworks)

The develoepers were inspired by
limitations of previous machine learning architectures.
DistBelief, the direct predecessor of TensorFlow,
is based on parameter server architecture.
In parameter server architecture,
'parameter server' stores and updates the mutable stable of
the model parameters, and separate 'worker processes'
process the training workloads upon receiving required parameters
from the parameter server.
DistBelief provides pre-defined layer abstractions
for users to define and training neural network models.
However, in order to define new layers and training algorithms,
the user should implement the corresponding lower-level programs
for the layer and algorithm.
This involves developing both worker process programs
for defining low-level computations and
and parameter sever program for managing and updating model parameters.
This hinders researchers to quickly develop new models
and experiment with them.

(how TF design solves the limitations)

TensorFlow solves this problem by providing more flexible abstractions.
In TensorFlow program, a dataflow graph defines the model.
The dataflow graph is a directed graph representation of computation,
where the nodes represent the operation and
the edges represent flow of values between the operations.
One crucial innovation of TensorFlow is that the mutable state
is also represented as operation within the dataflow graph.
A variable node maintains a mutable buffer that can be read or modified.
This approach allows TensorFlow to define the model parameters
and updating mechanism at the same time.

TensorFlow also allows users to easily distribute model computation
over low-level devices.
Each operation on the dataflow graph defines kernel,
a low-level implementation for devices such as CPU or GPU.
This provides users to flexibly assign kernels of the dataflow graph
according to the devices reside on the system.

(how TF defins DL model and training: by example codes)

(how TF version 2 differs : by example codes)

\subsection{Distributed Training and Horovod Framework}

(introduce general concept of distributed training)

Distributed training is a technique to boast efficiency in ML training
by parallelizing computation workloads in time.
Training an ML model is an expensive process that involves repetitive
arithmetic computation over multiple tensors,
which can be accelerated by multicore devices such as GPU or TPU. 
Distribtue training takes advantage of multiple accelerator devices to
The total training workload is divided into smaller, independent workloads
so they can be assigned into multiple accelerator devices and
processed simultaneously. 
In recent years, several works proposed application of distributed
deep learning in various fields.

(introduce model-parallel and data-parallel approach\cite{approaches2019Mao})

There are two approaches in distributed ML training.
In model-parallel approach, a large ML model is divided into separate unit
of computation pipes and assigned to different accelerators. The whole model
is pipelined over series of accelerators and so the forward-propagation and
back-propagation can be done in the distributed system.
In data-parallel apporach, however, multiple instances of same ML model
is assigned to each accelerators. The total training dataset is instead
divided into a number of smaller datasets which then assigned to different
accelerators. The training proceeds by simultaneously calculating gradients
in each accelerators. Then the gradients are averaged before being 
back-propagated into the model parameters.

(introduce facebook's data-parallel approach and improvements)

Facebook's experiment \cite{facebook2018} provides empirical evidence of
scalability of data-parallel distributed training. In the paper, 
halving/doubling-based allreduce algorithm is used to reduce communication cost 
of averaging gradients between the GPUs. 
Allreduce algorithm eliminates performance bottleneck of traditional
averaging algorithm by aggregating the average value through series of
point-to-point communication between GPUs. 
Experimental results of the paper show that the performance linearly
scales from 8-GPUs to 352 GPUs, while achieving over 90\% of ideal performance.

(introduce horovod)

Inspired by Facebook's results, Uber Engineering developed a distributed DL
training framework namely Horovod \cite{sergeev2018horovod}. It is published in
2017 as a part of DL toolkit for Michelangelo, a ML-as-a-service platform.
The main motivation of Horovod is to scale single-GPU DL model training
into distributed model training. Similar to the Facebook's approach,
Horovod implements allreduce algorithm in NCCL. NCCL is collective communication
library from NVIDIA, which provides various performant algorithms including
allreduce. In addition, Horovod introduces high-level API that enables
convenient use of the distributed training.  

(horovod provides convenient API to change existing model into distributed one)

(example of using horovod to distribute model
