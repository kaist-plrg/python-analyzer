\section{Background}\label{sec:background}
\subsection{TensorFlow Deep Learning Models}

TensorFlow\cite{tensorflow} is a machine learning platform library
developed by Google Brains.
ML developers can use TensorFlow-provided APIs to 
methods given by TensorFlow library.
This section describes three representative forms of TensorFlow
model codes, which are TensorFlow 1.x version form 
and TensorFlow 2.x version form using manual loops.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow1_mnist.py}
  \caption{TensorFlow 1.x model example}
\label{fig:back:tf1}
\end{figure}

Figure~\ref{fig:back:tf1} is a code example of TensorFlow 1.x version model.
In TensorFlow 1.x version models, developers msut explicitly define
the model structure and the training loop with TensorFlow APIs.
In figure \ref{fig:back:tf1}, for instance, lines 3 to 15 defines the model
structure of a neural network with two hidden layers.
The lines 3 and 4 first creates placeholder variables for input and output
tensors of the model. The placeholder variables explicitly define the
input and output vector dimensions of the model.
The lines 6 to 8 defines the first hidden dense layer.
The code uses {\tt Variable} APIs to define weight and bias parameters of
the dense layer. This allows the TensorFlow runtime to later modify the
layer parameters according to the gradient descent optimization.
Then, the line 8 defines the compuation of the first layer,
which involves a multiplication, an addition, and a ReLU activation.
Similarly, the lines 10 to 12 defines the second hidden layer parameters
and computation.
The line 14 defines the final output and the loss of the model by
applying softmax function across the second hidden layer output and
computing the cross entropy. 
The line 15 defines an optimizer for the model.
The optimizer implmenets a gradient descent for the model,
which involves automatically computing the gradient for the model operations
and optimizing the model parameters according to each gradient..
After defining the model structure and optimizer, the lines 17 to 21
manually defines the training loop for the model.
The code first creates a {\tt Session} object via {\tt with} statement.
The model code can access and control the TensorFlow runtime execution
via various methods of {\tt Session} object.
Developers can use {\tt Session.run} method to invoke a computation.
The lines 19 to 21, for instance, uses an {\tt for} loop and
a call to {\tt Session.run} method to repeatedly invoke a
training computation defined in the line 15.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow2_ex.py}
  \caption{TensorFlow 2.x model example}
\label{fig:back:tf2}
\end{figure}

% 위 코드는 tf 2.x 버전의 코드이다.
% 2.x 버전에서는 모델 구조를 6~15줄과 같이 keras API를 사용하여
% 간편하게 정의할 수 있다.
% 예를 들어 위 코드에서는 keras의 Sequential API를 이용하여
% 두 개의 convolutional 레이어와 두 개의 fully connected 레이어를
% 순서대로 연결한 모델을 정의했다.
% 이 모델을 훈련시키기 위해서 tf 2.x 모델에서는 GradientTape API를 사용한다.
% 위 코드의 22번줄과 같이 with문을 이용하여 GradientTape 오브젝트를
% 생성하면, 해당 with 문 내에서 실행되는 계산들에 대한 gradient를
% 자동으로 계산할 수 있다.
% 이렇게 자동으로 계산된 gradient를 26-27번줄에서 사용하여
% 모델 파라미터를 gradient descent를 통해 최적화한다.
Figure~\ref{fig:back:tf2} is a code example of TensorFlow 2.x version model.
Compared to the 1.x version, TensorFlow 2.x version has two differences.
First, the TensorFlow 2.x version includes the Keras library, a layer-based
DL library built on top of TensorFlow APIs.
ML Developers can use Keras APIs to conveniently define the model structure.
For instance, in figure~\ref{fig:back:tf2},
the lines 5 to 16 defines a sequential model with two convolutional layers and
two densely-connected layers.
Second, the 2.x version supports the eager evaluation feature, which allows
developers to define the training process with Python syntax.
The lines 18 to 24 show how developers use the eager evaluation feature
to define the forward and back propagations. 
The code creates a {\tt GradientTape} object, which records the gradients
of the operations occur inside the {\tt with} statement.
After computing the loss value,
the code optimizes the model parameters by calling the 
{\tt apply\_gradients} method in the lines 22 to 23.
\subsection{Horovod Distributed Training Framework}

\begin{figure}
 \lstinputlisting[language=Python]
{horovod_ex.py}
  \caption{Horovod distributed model example}
\label{fig:back:hvd} 
\end{figure}

% Horovod는 기존 ml 모델을 분산 훈련 모델으로 변환하기 위한 라이브러리이다.
% Horovod는 model-parallel 방식으로 ml 모델을 분산 훈련을 위한 모델로 바꾼다.
Horovod is a Python library for distributed training of TensorFlow models.
The library adopts model-parallel approach of distributed training;
in model-parallel approach, multiple instances of models are simultaneously
trained by same number of GPUs.
Each GPU is responsible of forward propagation of a single model instance.
Then, the gradients are averages over GPUs then used by gradient descent
to optimize the model parameters. 

The developers can use Horovod APIs to rewrite existing ML models into
distributed models.
Figure~\ref{fig:back:hvd} is an example of using Horovod library to convert
the model in~\ref{fig:back:tf2} into a distributed model.
After importing the Horovod module, the line 5 first initializes the
Horovod process. 
The lines 7 to 11 recognizes GPUs in the system, and pins each GPU to
a dedicated process.
The codes defines a model structure and training process same as the original
code in~\ref{fig:back:tf2}.
The line 31 wraps the {\tt GradientTape} object with a dedicated 
Horovod API so that the gradients in multiples processes are averaged.
The lines 36 to 38 is responsible of variable broadcast, a procedure which
synchronizes the initial model parameters across multiple processes.
The variable broadcast should occur exactly once at the first iteration of
the training loop.
