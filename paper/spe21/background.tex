\section{Background}\label{sec:background}
\subsection{TensorFlow Deep Learning Models}

TensorFlow\cite{tensorflow} is a machine learning platform library
developed by Google Brains.
ML developers can use TensorFlow-provided APIs to 
methods given by TensorFlow library.
This section describes three representative forms of TensorFlow
model codes, which are TensorFlow 1.x version form 
and TensorFlow 2.x version form using manual loops.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow1_mnist.py}
  \caption{TensorFlow 1.x model example}
\label{fig:back:tf1}
\end{figure}

Figure~\ref{fig:back:tf1} is a code example of TensorFlow 1.x version model.
In TensorFlow 1.x version models, developers msut explicitly define
the model structure, the optimization algorithm, and the training loop 
with low-level TensorFlow APIs.
First, the lines 3 to 14 defines the model
structure of a neural network with two hidden layers.
The lines 3 and 4 first creates placeholder variables for input and output
tensors of the model. The placeholder variables explicitly define the
input and output vector dimensions of the model, and later will
be replaced with actual vector values of input data.
The lines 6 to 8 defines the first hidden dense layer.
The code uses {\tt Variable} APIs to define weight and bias parameters of
the dense layer. This allows the gradient descent algorithm to 
later modify the parameters in training process. 
Then, the line 8 defines the computation of the first layer.
The layer multiplies an input vector {\tt x} and a weight matrix
{\tt W\_1}, add a bias vector {\tt b\_1}, and apply the ReLU activation
function.
Similarly, the lines 10 to 12 defines the second hidden layer parameters
and computation.
The line 14 defines the final output and the loss of the model by
applying softmax function across the second hidden layer output and
computing the cross entropy. 
The line 15 defines an optimization algorithm for the model training with
the optimizer object.
The optimizer object is an abstraction of an optimization algorithm,
which includes automatically computing the gradients of the model output loss 
and modifying the model parameters according to the gradients.
After defining the model structure and optimizer, the lines 17 to 21
manually defines the training loop for the model.
The code first creates a {\tt Session} object via {\tt with} statement.
The model code can access and control the TensorFlow runtime execution
via various methods of {\tt Session} object.
Developers can use {\tt Session.run} method to invoke a computation.
The lines 19 to 21, for instance, uses an {\tt for} loop and
a call to {\tt Session.run} method to repeatedly invoke a
optimization algorithm defined in the line 15.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow2_ex.py}
  \caption{TensorFlow 2.x model example}
\label{fig:back:tf2}
\end{figure}

% tf 1.x 의 각 부분 (모델 ㅁ부분 / running 부분)과 매칭시켜서 설명)
Figure~\ref{fig:back:tf2} is a code example of TensorFlow 2.x version model.
Compared to the 1.x version, TensorFlow 2.x version has two differences.
First, the TensorFlow 2.x version includes the Keras library, a layer-based
DL library built on top of TensorFlow APIs.
ML Developers can use Keras APIs to conveniently define the model structure.
For instance, in figure~\ref{fig:back:tf2},
the lines 5 to 8 defines a sequential model with two convolutional layers and
two densely-connected layers.
Using the Keras APIs effectively reduces burden of manually building the neural
network layers and simplify the model code.
Second, the 2.x version supports the eager evaluation feature, which allows
developers to define the training process with Python syntax.
The lines 12 to 18 show how developers use the eager evaluation feature
to define the forward and back propagations. 
The code creates a {\tt GradientTape} object, which records the operations
happen inside the {\tt with} statement body.
After the {\tt with} statement, the lines 17 and 18 refer to the 
{\tt GradientTape} object to automatically compute the gradients 
for the model parameters and optimizer them accordingly.
The optimizer provides {\tt apply\_gradients} methods that optimizes 
the model parameters by gradient descent algorithm. 

\subsection{Horovod Distributed Training Framework}



% Horovod는 기존 ml 모델을 분산 훈련 모델으로 변환하기 위한 라이브러리이다.
% Horovod는 model-parallel 방식으로 ml 모델을 분산 훈련을 위한 모델로 바꾼다.
Horovod is a Python library for distributed training of TensorFlow models.
The library adopts model-parallel approach of distributed training;
in model-parallel approach, multiple instances of models are simultaneously
trained by same number of GPUs.
Each GPU is responsible of forward propagation of a single model instance.
Then, the gradients are averages over GPUs then used by gradient descent
to optimize the model parameters. 

\begin{figure}[ht!]
 \lstinputlisting[language=Python]
{horovod_tf1_ex.py}
  \caption{Horovod distributed model example}
\label{fig:back:hvd1} 
\end{figure}

Figure~\ref{fig:back:hvd1} is a code example of using Horovod to distribute
the TensorFlow 1.x model in~\ref{fig:back:tf1}.

\begin{figure}[ht!]
 \lstinputlisting[language=Python]
{horovod_ex.py}
  \caption{Horovod distributed model example}
\label{fig:back:hvd2} 
\end{figure}


The developers can use Horovod APIs to rewrite existing ML models into
distributed models.
Figure~\ref{fig:back:hvd} is an example of using Horovod library to convert
the model in~\ref{fig:back:tf2} into a distributed model.
After importing the Horovod module, the line 5 first initializes the
Horovod process. 
The lines 7 to 11 recognizes GPUs in the system, and pins each GPU to
a dedicated process.
The codes defines a model structure and training process same as the original
code in~\ref{fig:back:tf2}.
The line 31 wraps the {\tt GradientTape} object with a dedicated 
Horovod API so that the gradients in multiples processes are averaged.
The lines 36 to 38 is responsible of variable broadcast, a procedure which
synchronizes the initial model parameters across multiple processes.
The variable broadcast should occur exactly once at the first iteration of
the training loop.
