\section{Background}\label{sec:background}
\subsection{TensorFlow Deep Learning Models}

TensorFlow\cite{tensorflow} is a machine learning platform library
developed by Google Brains.
ML developers can use TensorFlow-provided APIs to 
methods given by TensorFlow library.
This section describes three representative forms of TensorFlow
model codes, which are TensorFlow 1.x version form 
and TensorFlow 2.x version form using manual loops.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow1_mnist.py}
  \caption{TensorFlow 1.x model example}
\label{fig:back:tf1}
\end{figure}

% new
Figure~\ref{fig:back:tf1} is a code example of a TensorFlow 1.x model.
The code defines a neural network model with two hidden dense layers,
which classifies an input image of 784 pixels into one of the 10 classes.
To define a neural network model in TensorFlow 1.x, 
developers must explicitly define the model structure,
the operations between the model components, 
and the training loop with low-level TensorFlow APIs.

First, the lines 5 to 17 defines the model structure and manually build up
computational relationship between the model components.
The lines 5 and 6 first create placeholder variables {\tt x} and {\tt y},
which are the placeholders for the input image vectors 
and the answer label vectors of training data.
The placeholder variables only specify the vector size of the input images
and the labels; they will be replaced with actual values during the training
process. 
The lines 8 to 10 defines the first hidden dense layer, which outputs a
vector of length 100.
A dense layer is parametrized by a weight matrix and a bias vector.
The size of the weight matrix is (the size of the input vector) by 
(the size of the first layer output vector), and the size of the bias vector
is equal to the size of the first layer output vector.
The line 8 uses the {\tt random\_uniform} API to create a random weight matrix of
size 784 by 100, and the line 9 uses the {\tt zero} API to create a zero-vector
of size 100.
Then, the lines 8 and 9 wrap the weight matrix and the bias vector with
{\tt Variable} API.
The {\tt Variable} API creates a TensorFlow variable that can be later modified
and optimized during the runtime.
Thus, the lines 8 and 9 create weight and bias parameters for the first
hidden layer which have correct sizes and are modifiable during the 
training process.
The line 10 manually defines the operations of the first hidden layer. 
The operations multiply the input vector {\tt x} and the weight matrix 
{\tt W\_1}, add the bias vector {\tt b\_1}, then applies the ReLU activation
function.
In TensorFlow 1.x, developers have to explicitly specify the model parameters
and operations between them as the lines 8 to 10.
% todo: next line 뭔가 여기 들어가기 어색함
Note that these lines does not actually compute them.
The lines 12 to 14 defines the second hidden dense layer that outputs a
vector of length 10.
Similar to the lines 8 and 9, the lines 12 and 13 define the weight matrix
and the bias vector parameter for the second hidden layer.
Then the line 14 defines the operations of the second hidden layer,
which multiply the first hidden layer output {\tt layer\_1} and
the weight {\tt W\_2}, add the bias vector {\tt b\_2}, and apply the
Softmax activation function.
For the final part of the model definition,
the lines 16 and 17 defines how to compute the loss and optimize the model
parameters.
The line 16 defines the loss between the model output {\tt layer\_2} and
the answer label {\tt y}, with categorial cross entropy function.9.
The line 17 defines the optimization algorithm for the model.
The line first calls the {\tt AdamOptimizer} constructor
function to create an optimizer object that abstracts the
Adam gradient descent algorithm.
Then the {\tt minimize} method defines an operation that updates the
{\tt Variable} values to minimize the first argument.
Thus, the line defines the operation of a single training step,
which optimizes the model parameters via gradient descent with respect to
the loss value.

After the model structure and operations are defined, 
the lines 19 to 22 define the training loop.
The training loop iterates over the training dataset to feed the training
input images and labels to the model and optimize the model parameters via
gradient descent.
The line 19 first creates a {\tt Session} object.
The {\tt Session} object provides the {\tt run} method that can invoke
computation of TensorFlow operations.
The line 20 uses the {\tt run} method to initialize the TensorFlow variables.
The variables that should be initialized
include the model parameters in the lines 8 to 13, and
internal state variables of the optimizer which are implicitly introduced
when the optimizer object is created.
The line 20 refers to the TensorFlow global variables to access and initialize
all of the model parameter variables and the optimizer internal variables.
After the variables are initialized, the line 21 uses the {\tt for} loop
to iterate over the dataset and get training batches of the images and the
labels. Finally, the line 22 calls the {\tt run} method to
invoke computation of the training operation {\tt train\_op},
which repeatedly optimizes the model parameters by the gradient descent
optimization.


\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow2_ex.py}
  \caption{TensorFlow 2.x model example}
\label{fig:back:tf2}
\end{figure}


% new
Figure~\ref{fig:back:tf2} is a code example of TensorFlow 2.x model.
The code defines the same model with~\ref{fig:back:tf1},
a neural network model for image classification with two hidden dense layers.
The developers using TensorFlow 2.x must define the model structure and
the training loop using TensorFlow APIs.
Compared to TensorFlow 1.x, using TensorFlow 2.x APIs hides unnecessary details
and make the code compact.

The lines 5 to 11 first define the model structure.
First, the lines 5 to 8 use the Keras library APIs to define the neural
network model with two hidden dense layers.
Keras is a layer-based deep learning model library included in TensorFlow 2.x.
Using Keras APIs allows devleopers to conveniently define the model
structure without explicitly stating the operations between the layers.
The lines 6 and 7 defines the first and second hidden dense layers
with the {\tt Dense} API.
The first argument specifies the size of output vector.
Given the output vector size, the {\tt Dense} API will automatically create
the weight matrix and the bias vector parameters according to the size of 
the previous layer.
The second argument specifices the activation function of the layer,
where the name of the activation function is given as a string. 
The hidden layers are then composed into a linear sequential model
by the {\tt Sequential} API in the line 5.
Compared to TensorFlow 1.x models, using Keras APIs in TensorFlow 2.x model
reduces programmer's burden of explicitly defining the operations inside
the model components.
The lines 10 and 11 define the loss of the model output and
the optimizer object for the model training.
The line 10 uses the {\tt CategoricalCrossentropy} API to 
define the loss function.
The line 11 uses the {\tt Adam} API to define the optimizer object.
This optimizer object is equal to the {\tt AdamOptimizer} in the TensorFlow
1.x model.
However, the usage of the optimizer object is different in two TensorFlow
versions.
In TensorFlow 1.x, the optimizer object is mainly used to define the
training operation {\tt train\_op}, which will be later used as an argument
for the {\tt sess.run} method.
In TensorFlow 2.x, the optimizer object contains methods such as
{\tt apply\_gradients} that is responsible of acutal modification 

After the model structure, loss function and optimizer is defined,
the lines 13 to 19 define the training loop for the model.
The training loop starts with the {\tt for} loop that iterates over the dataset
and takes a batch of training images and answer labels.
The line 14 creates a new {\tt GradientTape} object by {\tt with} statement.
When a {\tt GradientTape} object is created by {\tt with} statement,
the operations that happen inside the {\tt with} statement body will
have their gradients recorded by the {\tt GradientTape} object.
The lines 15 and 16 define the forward propagation stage in a single
training step.
Note that in TensorFlow 2.x, the model objects created with Keras API can be
used like a Python function.
Thus, the line 15 computes the output vector of the model when the
training image is given as an input.
The line 16 computes the cross entropy loss between the model output and
the answer label, as defined in the line 10.
The lines 18 and 19 then optimizes the model parameters by gradient descent. 
The {\tt model.trainable\_variable} method returns the list of 
TensorFlow variables of the model parameters,
and the {\tt tape.gradient} method returns the list of
gradients for model parameters automatically computed against the
loss value.
The line 19 finally calls the {\tt apply\_gradients} method of the optimizer
object to optimize the model parameters according to gradient descent algorithm.


\subsection{Horovod Distributed Training Framework}

Horovod is a Python library for distributed training of TensorFlow models.
The library adopts model-parallel approach of distributed training;
in model-parallel approach, multiple instances of models are simultaneously
trained by same number of GPUs.
Each GPU is responsible of forward propagation of a single model instance.
Then, the gradients are averages over GPUs then used by gradient descent
to optimize the model parameters. 

\begin{figure}[ht!]
 \lstinputlisting[language=Python]
{horovod_tf1_ex.py}
  \caption{Horovod distributed model example}
\label{fig:back:hvd1} 
\end{figure}

%todo: paragraph 나누는 기준 좀 고민
% p1 : figure intro, explain initialization + gpu pinning
Figure~\ref{fig:back:hvd1} is a code example of using Horovod to distribute
the TensorFlow 1.x model in~\ref{fig:back:tf1}.
To explain the Horovod library,
we will point out the differene between the original TensorFlow 1.x code in
the figure \ref{fig:back:tf1} and the figure \ref{fig:Back:hvd1}
The line 4 first initializes the Horovod configuration.
The lines 6 to 8 create the same number of processes with GPUs in the system,
and pins each GPU per process. 
The GPU pinning is a process where each GPU in the system is assignd to a single 
training process.
This is done by refering to the local rank of the process 
by the {\tt local\_rank} API.
This ensures the model training workload is equally distributed over
every GPUs in the system.

% p2: explain optimizer
The line 24 defines the training operation, which is a distributed version
of the gradient descent algorithm.
Compare to the line 17 in the figure~\ref{fig:back:tf1},
the line 24 makes two changes to the optimizer object.
First, the learning rate argument is multiplied by {\tt hvd.size()}.
According to the Horovod library document,
the learning rate of the new distributed optimizer 
should be scaled by the number of GPUs for efficient distributed training.
To acheive the scaling, the line 24 calls the {\tt hvd.size()} method to
get the total number of processes that Horovod has created,
which is equal to the number of GPUs.
Second, the optimizer object is wrapped with the Horovod library API,
{\tt DistributedOptimizer}.
The {\tt DistributedOptimizer} API converts the single GPU-based optimizer 
object to distributed optimizer, which averages the loss gradients across the
training processes before applying the gradient descent optimization.
Thus, wrapping the optimzier with {\tt DistributedOptimizer} API is necessary
for correct distributed training of the model.

% p3: explain variable broadcast
The line 28 broadcasts the model and optimizer variables across the training
processes.
The variable broadcasting is a process where the TensorFlow 
variable values in different training processes are synchronized into
the same single value.
According to the Horovod library documentation, the variable broadcasting
should occur exactly once during the training process,
right after the TensorFlow variables are initialized.
In TensorFlow 1.x, the variable broadcasting is done in the line 28,
which is right after the line 27 that initializes the TensorFlow variables,
and right before the line 29 that starts the training loop.
Note that the {\tt broadcast\_global\_variables} API broadcasts every
TensorFlow variables including the model variables and the optimizer
variables.

% p4 : dataset length scale
Finally, the line 29 starts the training loop.
% todo : GPUs 갯수 만큼 batch를 한꺼번에 학습하므로, batch 갯수는 그만큼 줄어도 된다는 걸 rephrase
According to the Horovod library documentation, the number of training data
batches can be scaled down by the number of GPUs as the same number of
the batches are distributedly trained in one step.
Thus, the line 29 divides the number of batches taken from the dataset
by the number of the GPUs, {\tt hvd.size()}.


\begin{figure}[ht!]
 \lstinputlisting[language=Python]
{horovod_ex.py}
  \caption{Horovod distributed model example}
\label{fig:back:hvd2} 
\end{figure}

% todo: detailed expl of gpu pinning also for 1.x?
% todo : detailed xpl of "memory growth" for gpu pininng
% p1: init & gpu pinning
Figure~\ref{fig:back:hvd2} is a code example of using Horovod to distribute
the TensorFlow 2.x model in~\ref{fig:back:tf2}.
The lines 4 and 5 initializes the horovod configuration and a boolean variable
{\tt hvd\_broadcast\_done}.
The lines 7 to 11 is the GPU pinning code for TensorFlow 2.x. 
The line 7 first gets the list of GPUs in the system.
The line 11 pin the GPU to the process, using the local rank of the process.

% p2 : optimizer lr
The line 19 defines the optimizer object.
As in the optimizer of figure \ref{fig:back:hvd1}, 
the learning rate argument of the optimizer object 
should be scaled by {\tt hvd.size()} for efficient distributed training. 

% p3 : dataset length
The line 21 starts the training loop with the {\tt for} loop.
As mentioned before, the number of training data batches can be
scaled down by the number of GPUs.
To implement this, the {\tt for} loop in the line 21
divides the number of batches taken from the dataset by {\tt hvd.size()}.

% p4 : tape
Inside the training loop,
the line 26 wraps the original {\tt GradientTape} object with the Horovod API  
{\tt DistributedGradientTape}.
The {\tt DistributedGradientTape} API is similar
to the {\tt DistributedOptimizer} API in distributed version of TensorFlow 1.x
model; the loss gradients will be averaged across the training processes when
using {\tt DistributedGradientTape} API before the gradient descent is applied
to the model parameters.

% p5. broadcast
Finally, the lines 31 to 34 broadcasts the model and optimzier variables
across the training variables.
As mentioned before, the variable broadcasting should occur exactly once,
right after the variables are initialized.
To ensure the variable broadcasting occur only during the first training step,
the line 31 uses a global boolean variable {\tt hvd\_broadcast\_done},
then changes the value in the line 34 after the broadcasting is done.
Also note that the broadcasting code is written right after the line 29,
where the optimizer method {\tt apply\_gradient} is finishied.
In TensorFlow 2.x, the variables are not explicitly initialized,
but rather implicitly initialized during the computation.
To ensure that the variable broadcasting happen after the optimizer
variables are initialized, the variable broadcasting codes are placed
after the first gradient descent algorithm is finished, thus the optimizer
variables are already initialized.
