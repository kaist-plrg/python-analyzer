\section{Background}\label{sec:background}
\subsection{TensorFlow Deep Learning Models}

This section describes three code examples of using TensorFlow library
to construct a DL model and train it.
The first example illustrates a TensorFlow 1.x version model
which explicitly constructs a model and invoke the training process.
The second example illustrates a TensorFlow 2.x version model
which implicitly constructs a model and train it with automatic
gradient computation.
The final example illustrates a model written with Keras library,
a layer-based deep neural network library included in TensorFlow.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow1_mnist.py}
  \caption{TensorFlow 1.x model example}
\label{fig:back:tf1}
\end{figure}

Figure~\ref{fig:back:tf1} illustrates a TensorFlow 1.x model example.
In TensorFlow 1.x version, developers must first explicitly construct the model
structure.
The lines 4 to 15 construct a neural network model with two hidden layers. 
Then lines 18 to 19 defines the loss function and optimizer which is required
for the model training process.
The lines 22 to 26 uses the {\tt tf.Session} API to initialize the
model parameters and repeat the training process with training data batches.
 
\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow2_ex.py}
  \caption{TensorFlow 2.x model example}
\label{fig:back:tf2}
\end{figure}

Figure~\ref{fig:back:tf2} illustrates a TensorFlow 2.x model example.
The main difference of Tensorflow 2.x with TensorFlow 1.x is eager evaluation.
In TensorFlow 2.x codes, developers can 
Explicit calls to TensorFlow APIs are replaced by Python function calls,
and TensorFlow variables are replaced by function parameters.
This way, developers can easily define model components and compose them. 
In addition, TensorFlow 2.0 integrates Keras\cite{keras},
a deep learning library that provides high-level abstractions for layers.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow2_keras_ex.py}
  \caption{Keras model example}
\label{fig:back:keras}
\end{figure}

Figure~\ref{fig:back:keras} illustrates a Keras model example.
Keras is a layer-based deep neural network library included in TensorFlow 2.x.

\subsection{Distributed Training and Horovod Framework}

\textit{Distributed training} is a technique to boost efficiency in ML training
by parallelizing computation workloads over multiple devices.
Training a DL model is an expensive process that involves repetitive
arithmetic computation over multiple tensors,
which can be accelerated by specialized hardwares such as GPUs. 
Distributed training takes advantage of parallelizing the compuation over
multiple hardwares, typically using GPUs.
The total training workload is divided into smaller, independent workloads
so they can be assigned into GPUs and processed simultaneously. 
In recent years, several works proposed the application of distributed
deep learning in various fields.

There are two approaches to distributed DL training.
In the \textit{model-parallel} approach, a large DL model is divided into 
the separate unit of computations and assigned to different GPUs. 
The whole model is pipelined over the GPUs, so independent computations
can overlap in time.
In the \textit{data-parallel} approach, multiple instances of a small DL model
are assigned to each GPU. The total training dataset is instead
divided into several smaller datasets then assigned to each device.
accelerator. The training proceeds by simultaneously calculating gradients
in each accelerators. Then the gradients are averaged before being 
applied to the model parameters.

Facebook's experiment \cite{facebook2018} provides empirical evidence of
the scalability of data-parallel distributed training. In the paper, 
the halving/doubling-based allreduce algorithm is used to reduce the communication 
cost of averaging gradients between the GPUs. 
Allreduce algorithm eliminates the performance bottleneck of the traditional
averaging algorithm by aggregating the average value through a series of
point-to-point communication between GPUs. 
Experimental results of the paper show that the performance linearly
scales from 8-GPUs to 352 GPUs while achieving over 90\% of ideal performance.

Inspired by Facebook's results, Uber Engineering developed a distributed DL
training framework Horovod \cite{sergeev2018horovod}. 
The main motivation of Horovod is to scale single-GPU DL model training
into distributed model training using data-parallel approach. 
Similar to Facebook's approach, Horovod implements an allreduce algorithm. 
In addition, Horovod introduces a high-level API that enables
the convenient use of distributed training.  
