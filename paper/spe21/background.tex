\section{Background}\label{sec:background}
\subsection{TensorFlow Deep Learning Models}

TensorFlow\cite{tensorflow} is a machine learning platform library
developed by Google Brains.
ML developers can use TensorFlow-provided APIs to 
methods given by TensorFlow library.
This section describes three representative forms of TensorFlow
model codes, which are TensorFlow 1.x version form 
and TensorFlow 2.x version form using manual loops.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow1_mnist.py}
  \caption{TensorFlow 1.x model example}
\label{fig:back:tf1}
\end{figure}

% new
Figure~\ref{fig:back:tf1} is a code example of a TensorFlow 1.x model.
The code defines a neural network model with two hidden dense layers,
which classifies an input image of 784 pixels into one of the 10 classes.
To define a neural network model in TensorFlow 1.x, 
developers must explicitly define the model structure,
the operations between the model components, 
and the training loop with low-level TensorFlow APIs.

First, the lines 5 to 17 defines the model structure and manually build up
computational relationship between the model components.
The lines 5 and 6 first create placeholder variables {\tt x} and {\tt y},
which are the placeholders for the input image vectors 
and the answer label vectors of training data.
The placeholder variables only specify the vector size of the input images
and the labels; they will be replaced with actual values during the training
process. 
The lines 8 to 10 defines the first hidden dense layer, which outputs a
vector of length 100.
A dense layer is parametrized by a weight matrix and a bias vector.
The size of the weight matrix is (the size of the input vector) by 
(the size of the first layer output vector), and the size of the bias vector
is equal to the size of the first layer output vector.
The line 8 uses the {\tt random\_uniform} API to create a random weight matrix of
size 784 by 100, and the line 9 uses the {\tt zero} API to create a zero-vector
of size 100.
Then, the lines 8 and 9 wrap the weight matrix and the bias vector with
{\tt Variable} API.
The {\tt Variable} API creates a TensorFlow variable that can be later modified
and optimized during the runtime.
Thus, the lines 8 and 9 create weight and bias parameters for the first
hidden layer which have correct sizes and are modifiable during the 
training process.
The line 10 manually defines the operations of the first hidden layer. 
The operations multiply the input vector {\tt x} and the weight matrix 
{\tt W\_1}, add the bias vector {\tt b\_1}, then applies the ReLU activation
function.
In TensorFlow 1.x, developers have to explicitly specify the model parameters
and operations between them as the lines 8 to 10.
% todo: next line 뭔가 여기 들어가기 어색함
Note that these lines does not actually compute them.
The lines 12 to 14 defines the second hidden dense layer that outputs a
vector of length 10.
Similar to the lines 8 and 9, the lines 12 and 13 define the weight matrix
and the bias vector parameter for the second hidden layer.
Then the line 14 defines the operations of the second hidden layer,
which multiply the first hidden layer output {\tt layer\_1} and
the weight {\tt W\_2}, add the bias vector {\tt b\_2}, and apply the
Softmax activation function.
For the final part of the model definition,
the lines 16 and 17 defines how to compute the loss and optimize the model
parameters.
The line 16 defines the loss between the model output {\tt layer\_2} and
the answer label {\tt y}, with categorial cross entropy function.9.
The line 17 defines the optimization algorithm for the model.
The line first calls the {\tt AdamOptimizer} constructor
function to create an optimizer object that abstracts the
Adam gradient descent algorithm.
Then the {\tt minimize} method defines an operation that updates the
{\tt Variable} values to minimize the first argument.
Thus, the line defines the operation of a single training step,
which optimizes the model parameters via gradient descent with respect to
the loss value.

After the model structure and operations are defined, 
the lines 19 to 22 define the training loop.
The training loop iterates over the training dataset to feed the training
input images and labels to the model and optimize the model parameters via
gradient descent.
The line 19 first creates a {\tt Session} object.
The {\tt Session} object provides the {\tt run} method that can invoke
computation of TensorFlow operations.
The line 20 uses the {\tt run} method to initialize the TensorFlow variables.
The variables that should be initialized
include the model parameters in the lines 8 to 13, and
internal state variables of the optimizer which are implicitly introduced
when the optimizer object is created.
The line 20 refers to the TensorFlow global variables to access and initialize
all of the model parameter variables and the optimizer internal variables.
After the variables are initialized, the line 21 uses the {\tt for} loop
to iterate over the dataset and get training batches of the images and the
labels. Finally, the line 22 calls the {\tt run} method to
invoke computation of the training operation {\tt train\_op},
which repeatedly optimizes the model parameters by the gradient descent
optimization.


\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow2_ex.py}
  \caption{TensorFlow 2.x model example}
\label{fig:back:tf2}
\end{figure}

% tf 1.x 의 각 부분 (모델 ㅁ부분 / running 부분)과 매칭시켜서 설명)
Figure~\ref{fig:back:tf2} is a code example of TensorFlow 2.x version model.
Compared to the 1.x version, TensorFlow 2.x version has two differences.
First, the TensorFlow 2.x version includes the Keras library, a layer-based
DL library built on top of TensorFlow APIs.
ML Developers can use Keras APIs to conveniently define the model structure.
For instance, in figure~\ref{fig:back:tf2},
the lines 5 to 8 defines a sequential model with two convolutional layers and
two densely-connected layers.
Using the Keras APIs effectively reduces burden of manually building the neural
network layers and simplify the model code.
Second, the 2.x version supports the eager evaluation feature, which allows
developers to define the training process with Python syntax.
The lines 12 to 18 show how developers use the eager evaluation feature
to define the forward and back propagations. 
The code creates a {\tt GradientTape} object, which records the operations
happen inside the {\tt with} statement body.
After the {\tt with} statement, the lines 17 and 18 refer to the 
{\tt GradientTape} object to automatically compute the gradients 
for the model parameters and optimizer them accordingly.
The optimizer provides {\tt apply\_gradients} methods that optimizes 
the model parameters by gradient descent algorithm. 

\subsection{Horovod Distributed Training Framework}



% Horovod는 기존 ml 모델을 분산 훈련 모델으로 변환하기 위한 라이브러리이다.
% Horovod는 model-parallel 방식으로 ml 모델을 분산 훈련을 위한 모델로 바꾼다.
Horovod is a Python library for distributed training of TensorFlow models.
The library adopts model-parallel approach of distributed training;
in model-parallel approach, multiple instances of models are simultaneously
trained by same number of GPUs.
Each GPU is responsible of forward propagation of a single model instance.
Then, the gradients are averages over GPUs then used by gradient descent
to optimize the model parameters. 

\begin{figure}[ht!]
 \lstinputlisting[language=Python]
{horovod_tf1_ex.py}
  \caption{Horovod distributed model example}
\label{fig:back:hvd1} 
\end{figure}

Figure~\ref{fig:back:hvd1} is a code example of using Horovod to distribute
the TensorFlow 1.x model in~\ref{fig:back:tf1}.

\begin{figure}[ht!]
 \lstinputlisting[language=Python]
{horovod_ex.py}
  \caption{Horovod distributed model example}
\label{fig:back:hvd2} 
\end{figure}


The developers can use Horovod APIs to rewrite existing ML models into
distributed models.
Figure~\ref{fig:back:hvd} is an example of using Horovod library to convert
the model in~\ref{fig:back:tf2} into a distributed model.
After importing the Horovod module, the line 5 first initializes the
Horovod process. 
The lines 7 to 11 recognizes GPUs in the system, and pins each GPU to
a dedicated process.
The codes defines a model structure and training process same as the original
code in~\ref{fig:back:tf2}.
The line 31 wraps the {\tt GradientTape} object with a dedicated 
Horovod API so that the gradients in multiples processes are averaged.
The lines 36 to 38 is responsible of variable broadcast, a procedure which
synchronizes the initial model parameters across multiple processes.
The variable broadcast should occur exactly once at the first iteration of
the training loop.
