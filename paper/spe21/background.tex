\section{Background}\label{sec:background}
\subsection{TensorFlow ML Models}
Describe the structure of TensorFlow ML Models.

(add more introduction on TF)

TensorFlow is one of the famous deep learning software libraries.
It has APIs that can be used in many languages such as Python.

(how TF work)

To build deep learning model using TensorFlow,
user should construct a graph of the network.
In this graph construction, edges represent the flows of data in the form of tensor,
which is related to its name, TensorFlow,
and nodes represent computations on the tensor.
TensorFlow provides APIs of these basic building blocks
such as activation function and convolutional layer.
Using these basic blocks in TensorFlow libraries, user can define a neural network
that takes input tensor and returns the results.
Also, it provides APIs for calculating the loss from the output of the network.
This sequence of getting loss from the input is called forward pass.
With the loss of the network, parameters in the networks should be updated,
which is called backward pass.
TensorFlow also provides APIs that automatically computes gradient from the loss,
and updates parameters.
In this process, TensorFlow provides many optimization APIs
such as SGD and Adam.

(kinds of APIs in TF)

Using low-level APIs, user should specify the sequence of processes mentioned above.
It requires some understandings of these processes and TensorFlow,
but gives much flexibility.
In contrast, using high-level APIs such as Keras and Estimator,
user just builds a network, and calls `compile` and `fit` methods of the model.
Many processes are implicitly done internally in the compile and fit methods.
Therefore, user can omit the process, so it is easy to build the model.


\subsection{Distributed DL Training and Horovod Framework}
Describe the Horovod framework.

(introduce general concept of distributed training)

Distributed training is a technique to boast efficiency in ML training
by parallelizing computation workloads in time.
Training an ML model is an expensive process that involves repetitive
arithmetic computation over multiple tensors,
which can be accelerated by multicore devices such as GPU or TPU. 
Distribtue training takes advantage of multiple accelerator devices to
The total training workload is divided into smaller, independent workloads
so they can be assigned into multiple accelerator devices and
processed simultaneously. 
In recent years, several works proposed application of distributed
deep learning in various fields.

(introduce model-parallel and data-parallel approach\cite{approaches2019Mao})

There are two approaches in distributed ML training implementation
In model-parallel approach, a large ML model is divided into separate unit
of computation pipes and assigned to different accelerators. The whole model
is pipelined over series of accelerators and so the forward-propagation and
back-propagation can be done in the distributed system.
In data-parallel apporach, however, multiple instances of same ML model
is assigned to each accelerators. The total training dataset is instead
divided into a number of smaller datasets which then assigned to different
accelerators. The training proceeds by simultaneously calculating gradients
in each accelerators. Then the gradients are averaged before being 
back-propagated into the model parameters.

(introduce facebook's data-parallel approach and improvements)

Facebook's experiment \cite{facebook2018} provides empirical evidence of
scalability of data-parallel distributed training. In the paper, 
halving/doubling-based allreduce algorithm is used to reduce communication cost 
of averaging gradients between the GPUs. 
Allreduce algorithm eliminates performance bottleneck of traditional
averaging algorithm by aggregating the average value through series of
point-to-point communication between GPUs. 
Experimental results of the paper show that the performance linearly
scales from 8-GPUs to 352 GPUs, while achieving over 90\% of ideal performance.

(introduce horovod)

Inspired by Facebook's results, Uber Engineering developed a distributed DL
training framework namely Horovod \cite{sergeev2018horovod}. It is published in
2017 as a part of DL toolkit for Michelangelo, a ML-as-a-service platform.
The main motivation of Horovod is to scale single-GPU DL model training
into distributed model training. Similar to the Facebook's approach,
Horovod implements allreduce algorithm in NCCL. NCCL is collective communication
library from NVIDIA, which provides various performant algorithms including
allreduce. In addition, Horovod introduces high-level API that enables
convenient use of the distributed training.  

(introduce what horovod provides)
