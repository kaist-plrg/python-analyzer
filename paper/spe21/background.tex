\section{Background}\label{sec:background}
\subsection{TensorFlow Deep Learning Models}

TensorFlow\cite{tensorflow} is a machine learning platform developed by Google
Brains in 2016.
%The platform provides high-level APIs for the easy construction and training
%of general machine learning models.
The platform utilizes a kind of dataflow graphs, called {\it computation
graphs}, to easily construct deep learning models.
%TensorFlow is based on the concept of a \textit{dataflow graph}, which is an
%abstraction of the computations between multiple values.
In the computation graph, nodes represent operations and edges represent
dataflows of {\it tensors} between the operations.
%In dataflow graphs, nodes represent operations and edges represent the flow of
%tensors between the operations.
Some of the nodes, called {\it variable nodes}, maintain a mutable buffer that
stores model parameters, such as learning rate and weights, being adjusted in
the training phase.
%One important innovation of TensorFlow is that the mutable states are also
%represented as operations within the dataflow graph. 
%A \textit{variable node} maintains a mutable buffer that can be read or
%modified.  
%This approach allows TensorFlow to define the model parameters and updating
%mechanism at the same time.
TensorFlow allows users to chose or define a loss function and an optimizer.
After an iteration in training, the loss function calculates and passes loss to
the optimizer, and the optimizer, then, modifies the parameters to reduce the
loss in the next iterations.
%TensorFlow also allows users to easily distribute model computation over
%various devices such as GPUs or specialized hardwares.
%Each operation on the dataflow graph defines \textit{kernel}, a low-level
%implementation of the computation.
%Because the dataflow graph explicitly specifies how and where the
%sub-computation results should be sent and received, developers can easily
%assign each kernel to different devices and construct the whole computation
%from individual kernels and the dataflow graph.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow1_mnist.py}
  \caption{TensorFlow model example written in Python}
\label{fig:back:tf1}
\end{figure}

%We explain a typical form of TensorFlow DL model code by an example.
Figure~\ref{fig:back:tf1} illustrates a TensorFlow model example written in
Python. 
The {\tt tensorflow} Python module provides various APIs for deep learning
model construction and training.
%The figure \ref{fig:back:tf1} illustrates the TensorFlow model code for
%a simple neural network with 2 hidden layers.
%Typically, a TensorFlow ML code is divided into two part.
%In the first part, the model structure and training algorithm are defined.
%The model is defined by using API functions to construct the dataflow graph.
%Line 4 to 18 define the neural network structure,
%with the placeholders for input tensor {\tt x} and output tensor {\tt y}.
Line 4 to 15 construct a computation graph with two placeholders for an input
tensor {\tt x} and an output tensor {\tt y}.
The graph contains two hidden layers, and each layer consists of two variable
nodes for a weight and a bias.
The {\tt matmul} and {\tt relu} are operation nodes that produce an output
tensor by applying their operations to given input tensors.
For example, the {\tt matmul} on line 10 takes two tensors from the placeholder
{\tt x} and the variable node {\tt W\_1}, performs matrix multiplication on
them, and generates a tensor as the operation result.
%The hidden layers are constructed with {\tt variables},
%which corresponds to mutable model parameters, weights, and biases.  
Line 18 and 19 define a loss function and an optimizer respectively, which are
used to adjust the weights and biases of the variable nodes in the training
phase.
%Line 20 to 23 defines the training algorithm, by defining loss function and
%optimization scheme.
%Optimization is also defined by using API functions to construct
%an Optimizer object, then assigning the target function.
On line 22 to 26, a loop, called {\tt training loop}, actually trains the model
on a training set. 
The loop repeatedly passes a batch of the training set to the model, executes
the model to generate outputs, calculates loss, and optimizes the parameters of
the variables nodes. 
In the first version of TensorFlow, models can run only in a session that
encapsulates an execution environment for the models.
As shown on line 23 and 26, a session initializes all the variable nodes, and
the model runs in the session to be trained.

%Before executing a model, it must be constructed completely and 
%before its execution, and it, then, runs on the {\tt Session} that encapsulates
%its execution environment. 
%Models can execute only on a {\tt Session} object that encapsulates the
%execution environment.

%batch of the training set 
%the model is trained with the loss function and the
%optimizer, via 
%The second part invokes the actual training process
%In line 25 to 30, the TensorFlow runtime initializes the model parameters
%and repeats the optimization by {\tt Session} API. 

In September 2019, TensorFlow 2.0 was released\cite{tf2announce} with big
changes in both model construction and execution.
Unlike the first version, the new version no longer uses sessions and runs
models at the same time as constructing them.
The {\tt tensorflow} module provides different APIs for users to 
%The main change in the new version is enhancement in both model construction
%and deployment.
%The eager execution feature allows developers to use Python native variables
%and functions to compute the result in real-time and define the
%dataflow graph at the same time.
Explicit calls to TensorFlow APIs are replaced by Python function calls,
and TensorFlow variables are replaced by function parameters.
This way, developers can easily define model components and compose them. 
In addition, TensorFlow 2.0 integrates Keras\cite{keras},
a deep learning library that provides high-level abstractions for layers.


\subsection{Distributed Training and Horovod Framework}

\textit{Distributed training} is a technique to boost efficiency in ML training
by parallelizing computation workloads over multiple devices.
Training a DL model is an expensive process that involves repetitive
arithmetic computation over multiple tensors,
which can be accelerated by specialized hardwares such as GPUs. 
Distributed training takes advantage of parallelizing the compuation over
multiple hardwares, typically using GPUs.
The total training workload is divided into smaller, independent workloads
so they can be assigned into GPUs and processed simultaneously. 
In recent years, several works proposed the application of distributed
deep learning in various fields.

There are two approaches to distributed DL training.
In the \textit{model-parallel} approach, a large DL model is divided into 
the separate unit of computations and assigned to different GPUs. 
The whole model is pipelined over the GPUs, so independent computations
can overlap in time.
In the \textit{data-parallel} approach, multiple instances of a small DL model
are assigned to each GPU. The total training dataset is instead
divided into several smaller datasets then assigned to each device.
accelerator. The training proceeds by simultaneously calculating gradients
in each accelerators. Then the gradients are averaged before being 
applied to the model parameters.

Facebook's experiment \cite{facebook2018} provides empirical evidence of
the scalability of data-parallel distributed training. In the paper, 
the halving/doubling-based allreduce algorithm is used to reduce the communication 
cost of averaging gradients between the GPUs. 
Allreduce algorithm eliminates the performance bottleneck of the traditional
averaging algorithm by aggregating the average value through a series of
point-to-point communication between GPUs. 
Experimental results of the paper show that the performance linearly
scales from 8-GPUs to 352 GPUs while achieving over 90\% of ideal performance.

Inspired by Facebook's results, Uber Engineering developed a distributed DL
training framework Horovod \cite{sergeev2018horovod}. 
The main motivation of Horovod is to scale single-GPU DL model training
into distributed model training using data-parallel approach. 
Similar to Facebook's approach, Horovod implements an allreduce algorithm. 
In addition, Horovod introduces a high-level API that enables
the convenient use of distributed training.  
