\section{Background}\label{sec:background}
\subsection{TensorFlow Deep Learning Models}

TensorFlow\cite{tensorflow} is a machine learning platform library
developed by Google Brains.
ML developers can use TensorFlow-provided APIs to 
methods given by TensorFlow library.
This section describes three representative forms of TensorFlow
model codes, which are TensorFlow 1.x version form, 
TensorFlow 2.x version form using manual loops and
TensorFlow 2.x version form using automatic training methods.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow1_mnist.py}
  \caption{TensorFlow 1.x model example}
\label{fig:back:tf1}
\end{figure}

% 위 코드는 tf 1.x 버전 모델의 코드이다.
% 1.x 버전의 코드에서는 모델의 구조를 Tensorflow API를 통해 정의한다.
% 예를 들어 위 코드는 히든 레이어가 두개인 모델을 4~19번 줄에서 정의하고 있다.
% 8-9번줄은 첫 번째 히든 레이어를,
% 13-14번줄은 두 번째 히든 레이어를 정의하는데,
% 이 때 각 히든 레이어의 weight와 bias를 정의하기 위해
% tf.Variable이라는 API를 사용하고 있음을 알 수 있다.
% 18-19번줄은 모델의 loss 계산 결과값과 optimization algorithm을 정의하고 있다.
% 1.x 버전에서 모델을 훈련시키기 위해서는 Session API를 사용해서
% 훈련 루프를 직접 작성한다.
Figure~\ref{fig:back:tf1} is a code example of TensorFlow 1.x version model.
In TensorFlow 1.x version models, developers msut explicitly define
the model structure and the training loop with TensorFlow APIs.
In figure \ref{fig:back:tf1}, for instance, lines 3 to 15 defines the model
structure of a neural network with two hidden layers.
The code uses {\tt Variable} APIs to define weight and bias parameters of
the hidden layers. This allows the TensorFlow 1.x runtime to
optimize the model parameters later in the training process.
After defining the model structure, the lines 17 to 21
manually defines the training loop for the model.
The code first creates a {\tt Session} object,
then repeats the training step by a loop and a {\tt run} method call to the
trainer object. 

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow2_ex.py}
  \caption{TensorFlow 2.x model example}
\label{fig:back:tf2}
\end{figure}

% 위 코드는 tf 2.x 버전의 코드이다.
% 2.x 버전에서는 모델 구조를 6~15줄과 같이 keras API를 사용하여
% 간편하게 정의할 수 있다.
% 예를 들어 위 코드에서는 keras의 Sequential API를 이용하여
% 두 개의 convolutional 레이어와 두 개의 fully connected 레이어를
% 순서대로 연결한 모델을 정의했다.
% 이 모델을 훈련시키기 위해서 tf 2.x 모델에서는 GradientTape API를 사용한다.
% 위 코드의 22번줄과 같이 with문을 이용하여 GradientTape 오브젝트를
% 생성하면, 해당 with 문 내에서 실행되는 계산들에 대한 gradient를
% 자동으로 계산할 수 있다.
% 이렇게 자동으로 계산된 gradient를 26-27번줄에서 사용하여
% 모델 파라미터를 gradient descent를 통해 최적화한다.
Figure~\ref{fig:back:tf2} is a code example of TensorFlow 2.x version model.
Compared to the 1.x version, TensorFlow 2.x version has two differences.
First, the TensorFlow 2.x version includes the Keras library, a layer-based
DL library built on top of TensorFlow APIs.
ML Developers can use Keras APIs to conveniently define the model structure.
For instance, in figure~\ref{fig:back:tf2},
the lines 5 to 16 defines a sequential model with two convolutional layers and
two densely-connected layers.
Second, the 2.x version supports the eager evaluation feature, which allows
developers to define the training process with Python syntax.
The lines 18 to 24 show how developers use the eager evaluation feature
to define the forward and back propagations. 
The code creates a {\tt GradientTape} object, which records the gradients
of the operations occur inside the {\tt with} statement.
After computing the loss value,
the code optimizes the model parameters by calling the 
{\tt apply\_gradients} method in the lines 22 to 23.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow2_keras_ex.py}
  \caption{TensorFlow 2.x model training with Keras API example}
\label{fig:back:tf2_keras}
\end{figure}

% todo: is this explanation necessary?
Instead of manually writing training loops like~\ref{fig:back:tf2}, 
developers can also use the pre-defined training methods in TensorFlow library. 
Figure~\ref{fig:back:tf2_keras} is a code example of using
TensorFlow-provided methods to automatically train the model.
The code defines the model structure same as the figure~\ref{fig:back:tf2}.
However, the code does not use {\tt GradientTape} object or manual loops to
train the model.
Instead, in the lines 16 to 17, the code calls {\tt compile} and {\tt fit}
method to automatically train the model with 50 epochs and given training
dataset.
The methods are provided within the {\tt tf.keras.models.Model} class,
which is a superclass of {\tt tf.keras.Sequential}.

\subsection{Horovod Distributed Training Framework}

\begin{figure}
 \lstinputlisting[language=Python]
{horovod_ex.py}
  \caption{Horovod distributed model example}
\label{fig:back:hvd} 
\end{figure}

% Horovod는 기존 ml 모델을 분산 훈련 모델으로 변환하기 위한 라이브러리이다.
% Horovod는 model-parallel 방식으로 ml 모델을 분산 훈련을 위한 모델로 바꾼다.
Horovod is a Python library for distributed training of TensorFlow models.
The library adopts model-parallel approach of distributed training;
in model-parallel approach, multiple instances of models are simultaneously
trained by same number of GPUs.
Each GPU is responsible of forward propagation of a single model instance.
Then, the gradients are averages over GPUs then used by gradient descent
to optimize the model parameters. 

The developers can use Horovod APIs to rewrite existing ML models into
distributed models.
Figure~\ref{fig:back:hvd} is an example of using Horovod library to convert
the model in~\ref{fig:back:tf2} into a distributed model.
After importing the Horovod module, the line 5 first initializes the
Horovod process. 
The lines 7 to 11 recognizes GPUs in the system, and pins each GPU to
a dedicated process.
The codes defines a model structure and training process same as the original
code in~\ref{fig:back:tf2}.
The line 31 wraps the {\tt GradientTape} object with a dedicated 
Horovod API so that the gradients in multiples processes are averaged.
The lines 36 to 38 is responsible of variable broadcast, a procedure which
synchronizes the initial model parameters across multiple processes.
The variable broadcast should occur exactly once at the first iteration of
the training loop.
