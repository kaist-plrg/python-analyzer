\section{Background}\label{sec:background}
\subsection{TensorFlow Deep Learning Models}


% Plan
% explain typical form of tf model by code examples
% will explain two categories
% 1. tf 1.x version models
% 2. tf 2.x version models

TensorFlow\cite{tensorflow} is a machine learning platform library
developed by Google Brains.
ML developers can define model structures by composing
TensorFlow model APIs, and train the model with pre-defined training
methods given by TensorFlow library.
This section describes two representative forms of TensorFlow
model codes, which are TensorFlow 1.x version format and 
TensorFlow 2.x version format.

\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow1_mnist.py}
  \caption{TensorFlow 1.x model example}
\label{fig:back:tf1}
\end{figure}

Figure~\ref{fig:back:tf1} illustrates an example of TensorFlow 1.x model.
In TensorFlow 1.x version, developers use TensorFlow APIs to define model
structures.
In figure~\ref{fig:back:tf1}, for instance, the lines 4 to 15
defines a neural network model with two hidden layers.


The lines 4 to 15 construct a neural network model with two hidden layers. 
Then lines 18 to 19 defines the loss function and optimizer which is required
for the model training process.
The lines 22 to 26 uses the {\tt tf.Session} API to initialize the
model parameters and repeat the training process with training data batches.
 
\begin{figure}[ht!]
\lstinputlisting[language=Python]
{tensorflow2_ex.py}
  \caption{TensorFlow 2.x model example}
\label{fig:back:tf2}
\end{figure}

Figure~\ref{fig:back:tf2} illustrates a TensorFlow 2.x model example.
The main difference of Tensorflow 2.x with TensorFlow 1.x is eager evaluation.
In TensorFlow 2.x codes, developers can 
Explicit calls to TensorFlow APIs are replaced by Python function calls,
and TensorFlow variables are replaced by function parameters.
This way, developers can easily define model components and compose them. 
In addition, TensorFlow 2.0 integrates Keras\cite{keras},
a deep learning library that provides high-level abstractions for layers.

\subsection{Distributed Training and Horovod Framework}

\textit{Distributed training} is a technique to boost efficiency in ML training
by parallelizing computation workloads over multiple devices.
Training a DL model is an expensive process that involves repetitive
arithmetic computation over multiple tensors,
which can be accelerated by specialized hardwares such as GPUs. 
Distributed training takes advantage of parallelizing the compuation over
multiple hardwares, typically using GPUs.
The total training workload is divided into smaller, independent workloads
so they can be assigned into GPUs and processed simultaneously. 
In recent years, several works proposed the application of distributed
deep learning in various fields.

There are two approaches to distributed DL training.
In the \textit{model-parallel} approach, a large DL model is divided into 
the separate unit of computations and assigned to different GPUs. 
The whole model is pipelined over the GPUs, so independent computations
can overlap in time.
In the \textit{data-parallel} approach, multiple instances of a small DL model
are assigned to each GPU. The total training dataset is instead
divided into several smaller datasets then assigned to each device.
accelerator. The training proceeds by simultaneously calculating gradients
in each accelerators. Then the gradients are averaged before being 
applied to the model parameters.

Facebook's experiment \cite{facebook2018} provides empirical evidence of
the scalability of data-parallel distributed training. In the paper, 
the halving/doubling-based allreduce algorithm is used to reduce the communication 
cost of averaging gradients between the GPUs. 
Allreduce algorithm eliminates the performance bottleneck of the traditional
averaging algorithm by aggregating the average value through a series of
point-to-point communication between GPUs. 
Experimental results of the paper show that the performance linearly
scales from 8-GPUs to 352 GPUs while achieving over 90\% of ideal performance.

Inspired by Facebook's results, Uber Engineering developed a distributed DL
training framework Horovod \cite{sergeev2018horovod}. 
The main motivation of Horovod is to scale single-GPU DL model training
into distributed model training using data-parallel approach. 
Similar to Facebook's approach, Horovod implements an allreduce algorithm. 
In addition, Horovod introduces a high-level API that enables
the convenient use of distributed training.  
