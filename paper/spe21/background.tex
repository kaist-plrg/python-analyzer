\section{Background}\label{sec:background}
\subsection{TensorFlow ML Models}
Describe the structure of TensorFlow ML Models.

(briefly introduce TF library concepts)

TensorFlow\cite{tensorflow} is a machine learning platform library
developed in 2016 by Google Brains.
The develoepers were inspired by
limitations of previous machine learning architectures.
DistBelief, the direct predecessor of TensorFlow,
is based on parameter server architecture.
In parameter server architecture,
'parameter server' stores and updates the mutable stable of
the model parameters, and separate 'worker processes'
process the training workloads upon receiving required parameters
from the parameter server.
DistBelief provides pre-defined layer abstractions
for users to define and traing neural network models.
However, in order to define new layers and training algorithms,
the user should implement the corresponding lower-level programs
for the layer and algorithm.
This hinders researchers to quickly develop new models
and experiment with them.

TensorFlow solves this problem by providing more flexible abstractions.
TensorFlow is designed based on the computation graph,
a directed graph that nodes represent operations and
edges represent flow of values between the operations.



(explain how ML model is implemented in TF)

(explain how traininig process is executed with TF models)

(explain different APIs, classes in TF)

(explain TF API for distributed training)




---------------------------------------------



(add more introduction on TF)

Nowadays, there are lots of deep learning software libraries
such as Pytorch and Caffe.
Among them, TensorFlow is one of the famous deep learning software libraries.
It not only has many APIs that can be used in many languages,
but also operates on heterogenuous environment\cite{tensorflow}.
Not only that, it has powerful visualization tool TensorBoard
that makes TensorFlow program easy to understand and debug.

(how TF work\cite{doi:10.3102/1076998619872761})

To build deep learning model using TensorFlow,
user should construct a graph of the network.
In this graph construction, edges represent the flows of data in the form of tensor,
which is related to its name, TensorFlow,
and nodes represent computations on the tensor.
TensorFlow provides APIs to construct these basic building blocks
such as activation function and convolutional layer.
Using these basic blocks in TensorFlow libraries, user can define a neural network
that takes input tensor and returns some results.
Also, it provides APIs for calculating the loss from the output of the network.
The sequence of getting loss from the input is called forward pass.
With the loss of the network, parameters in the networks should be updated,
which is called backward pass.
TensorFlow also provides APIs that automatically computes gradient from the loss
and updates parameters.
In this process, TensorFlow provides many optimization APIs
such as SGD and Adam.

(kinds of APIs in TF\cite{doi:10.3102/1076998619872761})

Using low-level APIs, user should specify the sequence of processes mentioned above.
It requires some understandings of these processes and internal of TensorFlow,
but gives much flexibility.
In contrast, using high-level APIs such as Keras and Estimator,
what user has to do is just building a network,
and calling `compile` and `fit` methods of the model.
Many processes are implicitly done internally in the high-level APIs.
Therefore, user can omit some processes, so it makes building the model easy.


\subsection{Distributed DL Training and Horovod Framework}
Describe the Horovod framework.

(introduce general concept of distributed training)

Distributed training is a technique to boast efficiency in ML training
by parallelizing computation workloads in time.
Training an ML model is an expensive process that involves repetitive
arithmetic computation over multiple tensors,
which can be accelerated by multicore devices such as GPU or TPU. 
Distribtue training takes advantage of multiple accelerator devices to
The total training workload is divided into smaller, independent workloads
so they can be assigned into multiple accelerator devices and
processed simultaneously. 
In recent years, several works proposed application of distributed
deep learning in various fields.

(introduce model-parallel and data-parallel approach\cite{approaches2019Mao})

There are two approaches in distributed ML training implementation
In model-parallel approach, a large ML model is divided into separate unit
of computation pipes and assigned to different accelerators. The whole model
is pipelined over series of accelerators and so the forward-propagation and
back-propagation can be done in the distributed system.
In data-parallel apporach, however, multiple instances of same ML model
is assigned to each accelerators. The total training dataset is instead
divided into a number of smaller datasets which then assigned to different
accelerators. The training proceeds by simultaneously calculating gradients
in each accelerators. Then the gradients are averaged before being 
back-propagated into the model parameters.

(introduce facebook's data-parallel approach and improvements)

Facebook's experiment \cite{facebook2018} provides empirical evidence of
scalability of data-parallel distributed training. In the paper, 
halving/doubling-based allreduce algorithm is used to reduce communication cost 
of averaging gradients between the GPUs. 
Allreduce algorithm eliminates performance bottleneck of traditional
averaging algorithm by aggregating the average value through series of
point-to-point communication between GPUs. 
Experimental results of the paper show that the performance linearly
scales from 8-GPUs to 352 GPUs, while achieving over 90\% of ideal performance.

(introduce horovod)

Inspired by Facebook's results, Uber Engineering developed a distributed DL
training framework namely Horovod \cite{sergeev2018horovod}. It is published in
2017 as a part of DL toolkit for Michelangelo, a ML-as-a-service platform.
The main motivation of Horovod is to scale single-GPU DL model training
into distributed model training. Similar to the Facebook's approach,
Horovod implements allreduce algorithm in NCCL. NCCL is collective communication
library from NVIDIA, which provides various performant algorithms including
allreduce. In addition, Horovod introduces high-level API that enables
convenient use of the distributed training.  

(introduce what horovod provides)
