\section{Introduction}\label{sec:intro}

Recent advancements in deep learning(DL) have opened the wide possibility of
applying artificial intelligence in various fields.
One major factor to consider in DL development is training time.
While training is an essential part of DL development,
the process requires heavy computational workloads.
According to the analysis by S. Bianco et al. \cite{bianco2018benchmark},
the majority of the image recognition models including ResNet, VGG, SENet, etc.
require over 5 GFLOPs for a single forward propagation.
The training process is a repetition of training steps until convergence,
which are composed of a forward and backward propagation.
Considering that thousands of training data are fed into the model
for convergence, the training process takes the longest time
in the whole development process. This leads to the conclusion that
reducing training time is the most efficient optimization according to
Amdahl's law.

In one of the efforts to reduce training time, 
researchers utilize \textit{distributed training}.
Distributed training is a technique to parallelize the training computation
workload over multiple GPUs.
By taking advantage of parallelism, distributed training enables researchers
to spend less time in training while preserving accuracy.
Using multiple GPUs or TPUs to train the model is already adopted
in various works \cite{brown2020gpt-3} \cite{silver2017alphazero}
\cite{zhang2019distrspeech} \cite{tian2020distrwebattack}.
As DL models are becoming more complex and training datasets are growing,
the need for distributing the training process in DL research is inevitable.

Transforming an DL model for distributed training involves code rewriting.
Until now, developers have manually rewritten the model codes.
This is a time-consuming and labor-intensive task for developers.
In addition, developers need to understand and locate 
specific components involved in the model training.
This requires the developer to fully understand the library APIs,
which is a difficult challenge.

In this paper, we propose an 
\textit{automated code transformation for distributed training}.
The transformation
converts the single-GPU-based TensorFlow models
into the multi-GPU-based models so that the model can be trained
over a distributed system.
We first formally define the code transformation from
single-GPU-based DL model code to distributed training model code.
Then the transformation is implemented in actual software together with
adequate pre-analysis that provides information used in the transformation.
The transformation software is evaluated by comparing against 
examples of distributed training model codes, 
each including an original single-GPU-based model code 
and a manually transformed distributed model code.

The contributions of this paper are as follows:

\begin{itemize}
  \item We formalize the code transformation for distributed DL training
        of TensorFlow models. We formally define the code transformation
        as functions from AST to AST. After manually inspecting
        the Horovod documentation and code examples,
        We provide transform function definitions for 
        automatically transforming single-GPU model codes into
        distributed DL model codes.

  \item We present the distributed code transformation tool, which implements
        the code transformation functions. We evaluate the tool's performance
        by applying the transformation to TensorFlow example model codes.
\end{itemize}
