\section{Introduction}\label{sec:intro}

Recent advancements in deep learning have opened the wide possibility of
applying artificial intelligence in various fields.
Deep learning(DL) is a machine learning technique which uses
several hidden layers of perceptrons.
The word \textit{deep} comes from the fact that the DL models use
lots of hidden layers.
(TODO: examples of SOTA DL models)

Training is an essential part of DL development.
In the training process, DL model parameters are updated to
minimize the loss with respect to the training dataset.
Gradient descent methods are usually used to update the
parameters given the training example batch.
Each step of the gradient descent is repeated until
the loss converges to some low value.
One major factor to consider in the trainig process is that
the training requires heavy computational workloads.
Considering that thousands of training data are fed into the model
for convergence, the training process takes the longest time
in the whole development process. This leads to the conclusion that
reducing training time is the most efficient optimization according to
Amdahl's law.

In one of the efforts to reduce training time, 
researchers utilize \textit{distributed training}.
Distributed training is a technique to parallelize the training computation
workload over multiple GPUs.
By taking advantage of parallelism, distributed training enables researchers
to spend less time in training while preserving accuracy.
Using multiple GPUs or TPUs to train the model is already adopted
in various works \cite{brown2020gpt-3} \cite{silver2017alphazero}
\cite{zhang2019distrspeech} \cite{tian2020distrwebattack}.
(TODO: explain each citation)
As DL models are becoming more complex and training datasets are growing,
the need for distributing the training process in DL research is inevitable.

However, existing models cannot automatically acheive speedup
running on distributed systems.
The users of the DL model must rewrite the original model training
code into distributed training code. 
The distributed training codes identify available GPUs in the system,
create separate processes for each GPU, and divide the training
workload assign to each process.
This way, the distributed training code is responsible of
actually parallelizing the training process over multiple GPUs.
In practice, developers use distributed training libraries or frameworks
that can be used together with popular DL frameworks.  
For example, Horovod\cite{sergeev2018horovod} is a popular
distributed training library that supports multiple DL frameworks,
including popular ones like TensorFlow\cite{tensorflow} 
or PyTorch\cite{pytorch2019}.

Until now, developers manually rewrote single-GPU-based models
into distributed models.
This is a time-consuming and labor-intensive task for developers.
In addition, developers need to understand and locate 
specific components involved in the model training.
This requires the developer to fully understand the library APIs,
which is a difficult challenge.

In this paper, we propose an 
\textit{automated code transformation for distributed training}.
The transformation
converts the single-GPU-based TensorFlow models
into the multi-GPU-based models so that the model can be trained
over a distributed system.
We first formally define the code transformation from
single-GPU-based DL model code to distributed training model code.
Then the transformation is implemented in actual software together with
adequate pre-analysis that provides information used in the transformation.
The transformation software is evaluated by comparing against 
examples of distributed training model codes, 
each including an original single-GPU-based model code 
and a manually transformed distributed model code.
(TODO: add explanation to tensorflow and horovod, and python)

The contributions of this paper are as follows:

\begin{itemize}
  \item TODO: one more?

  \item We formalize the code transformation for distributed DL training
        of TensorFlow models. We formally define the code transformation
        as functions from AST to AST. After manually inspecting
        the Horovod documentation and code examples,
        We provide transform function definitions for 
        automatically transforming single-GPU model codes into
        distributed DL model codes.

  \item We present the distributed code transformation tool, which implements
        the code transformation functions. We evaluate the tool's performance
        by applying the transformation to TensorFlow example model codes.
\end{itemize}
