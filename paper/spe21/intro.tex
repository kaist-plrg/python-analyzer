\section{Introduction}\label{sec:intro}

Recent advancements in machine learning(ML) have open wide possibility of
applying artificial intelligence in various fields.
(list of area where ML is used. ex. image recognition, autonomous driving)

One major factor to consider in ML development is training time.
While training is an essential part of the ML development,
the process requires heavy computational workloads.
According to the analysis by S. Bianco et al. \cite{bianco2018benchmark},
majority of the image recognition models including ResNet, VGG, SENet, etc.
require over 5 GFLOPs for a single forward propagation.
The training process is repeatition of training steps until convergence,
which each of them is composed of a forward and a backward propagation.
Considering that thousands of training data is feed into the model
for the convergence, the training process takes the longest time
in the whole development process. This leads to the conclusion that
reducing training time is the most efficient optimization according to
the Amdahl's law.

In one of the efforts to reduce training time, 
researchers utilize distributed ML training.
Distributed training is a technique to parallelize the training computation
workload over multiple GPUs in a system or multiple distributed systems
equipped with one or more GPUs.
By taking advantage of parallelism, distributed training enables researchers
to spend less time in training while preserving accuracy.
Using multiple GPUs or TPUs to train the model is already adopted
in various works \cite{brown2020gpt-3} \cite{silver2017alphazero}
\cite{zhang2019distrspeech} \cite{tian2020distrwebattack}.
As ML models are becoming more complex and training dataset are growing,
the need for distributing the training process in ML research is inevitable.

Transforming DL model for distributed training involves manual code rewriting.

In this paper, we propose an automated Python code transformation that enables
TensorFlow ML models to perform distributed training.
We first formally define the code transformation of rewriting from
single-GPU based DL model code to distributed training model code.
Then the transformation is implemented in actual algorithm together with
adequate pre-analysis that provides information used in the transformation.
The transformer software is evaluated by comparing against 
examples of distributed training model codes, 
each including an original single-GPU based model code 
and a manually transformed distributed model code.

The contributions of this paper are as follows:

\begin{itemize}
  \item TODO 0
  \item TODO 1
\end{itemize}
