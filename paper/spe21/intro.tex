\section{Introduction}\label{sec:intro}

Recent advancements in machine learning(ML) have open the wide possibility of
applying artificial intelligence in various fields.
(list of area where ML is used. ex. image recognition, autonomous driving)

One major factor to consider in ML development is training time.
While training is an essential part of the ML development,
the process requires heavy computational workloads.
According to the analysis by S. Bianco et al. \cite{bianco2018benchmark},
majority of the image recognition models including ResNet, VGG, SENet, etc.
require over 5 GFLOPs for a single forward propagation.
The training process is repeatition of training steps until convergence,
which each of them is composed of a forward and a backward propagation.
Considering that thousands of training data is feed into the model
for the convergence, the training process takes the longest time
in the whole development process. This leads to the conclusion that
reducing training time is the most efficient optimization according to
the Amdahl's law.

In one of the efforts to reduce training time, 
researchers utilize \textit{distributed training}.
Distributed training is a technique to parallelize the training computation
workload over multiple GPUs.
By taking advantage of parallelism, distributed training enables researchers
to spend less time in training while preserving accuracy.
Using multiple GPUs or TPUs to train the model is already adopted
in various works \cite{brown2020gpt-3} \cite{silver2017alphazero}
\cite{zhang2019distrspeech} \cite{tian2020distrwebattack}.
As ML models are becoming more complex and training dataset are growing,
the need for distributing the training process in ML research is inevitable.

Transforming an ML model for distributed training involves code rewriting.
Until now, developers have manully rewrite the model codes.
This is time-consuming and labor-intensive tasks for developers.
In addiiton, developers need to understand and locate 
specific components involved in the model training.
This requires the developer to fully understand the library APIs,
which is a difficult challenge.

In this paper, we propose an 
\textit{automated code transformation for distributed training}.
The transformation
converts the single-GPU based TensorFlow models
into the multi-GPU based models, so that the model can be trained
over a distributed systm.
We first formally define the code transformation from
single-GPU based ML model code to distributed training model code.
Then the transformation is implemented in actual software together with
adequate pre-analysis that provides information used in the transformation.
The transformation software is evaluated by comparing against 
examples of distributed training model codes, 
each including an original single-GPU based model code 
and a manually transformed distributed model code.

The contributions of this paper are as follows:

\begin{itemize}
  \item We formalize the code transformation for distributed ML training
        of TensorFlow models. We first formally define the code transformation
        as functions from AST to AST. Then we provide transform function
        definitions for ouistributed ML model code.
  \item We present the distributed code transformation tool, which implements
        the code transformation functions. We evaluate the tool's performance
        by applying transformation to TensorFlow example model codes.
\end{itemize}
