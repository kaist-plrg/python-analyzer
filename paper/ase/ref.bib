@article{sergeev2018horovod,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}

@misc{horovodgithub,
  author={Horovod},
  title={Horovod},
  howpublished={\url{https://github.com/horovod/horovod}},
}

@misc{tensorboard,
  author={TensorFlow},
  title={TensorBoard},
  howpublished={\url{https://www.tensorflow.org/tensorboard}},
}

@misc{tfdistributed,
  author={TensorFlow},
  title={Module: tf.distribute},
  howpublished={\url{https://www.tensorflow.org/api_docs/python/tf/distribute}},
}


@misc{tfmodelgarden,
  author={TensorFlow},
  title={TensorFlow Model Garden},
  howpublished={\url{https://github.com/tensorflow/models}},
}

@misc{tfexamplesdamien,
  author={Aymeric Damien},
  title={TensorFlow Examples},
  howpublished={\url{https://github.com/aymericdamien/TensorFlow-Examples}},
}

@misc{tf2tutogithub,
  author={Jackie Loong},
  title={TensorFlow 2.0 Tutorials},
  howpublished={\url{https://github.com/dragen1860/TensorFlow-2.x-Tutorials}},
}

@misc{cifar10github,
  author={\relax{Arconsis IT-Solutions GmbH}},
  title={CIFAR 10 with TensorFlow},
  howpublished={\url{https://github.com/arconsis/cifar-10-with-tensorflow2/blob/master/BetterNetwork.py}},
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}
%@article{facebook2018,
%      title={Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}, 
%      author={Priya Goyal and Piotr Doll√°r and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
%      year={2018},
%      Journal = {arXiv preprint arXiv:1706.02677},
%      eprint={1706.02677},
%      archivePrefix={arXiv},
%      primaryClass={cs.CV}
%}



@inproceedings{zhang2019distributed,
  title={Distributed deep learning strategies for automatic speech recognition},
  author={Zhang, Wei and Cui, Xiaodong and Finkler, Ulrich and Kingsbury, Brian and Saon, George and Kung, David and Picheny, Michael},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5706--5710},
  year={2019},
  organization={IEEE}
}

%@article{zhang2019distrspeech,
%  author={Zhang, Wei and Cui, Xiaodong and Finkler, Ulrich and Kingsbury, Brian and Saon, George and Kung, David and Picheny, Michael},
%  journal={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
%  title={Distributed Deep Learning Strategies for Automatic Speech Recognition}, 
%  year={2019},
%  volume={},
%  number={},
%  pages={5706-5710},
%  doi={10.1109/ICASSP.2019.8682888}
%}}

@article{tian2019distributed,
  title={A distributed deep learning system for web attack detection on edge devices},
  author={Tian, Zhihong and Luo, Chaochao and Qiu, Jing and Du, Xiaojiang and Guizani, Mohsen},
  journal={IEEE Transactions on Industrial Informatics},
  volume={16},
  number={3},
  pages={1963--1971},
  year={2019},
  publisher={IEEE}
}

%@article{tian2020distrwebattack,
%  author={Tian, Zhihong and Luo, Chaochao and Qiu, Jing and Du, Xiaojiang and Guizani, Mohsen},
%  journal={IEEE Transactions on Industrial Informatics}, 
%  title={A Distributed Deep Learning System for Web Attack Detection on Edge Devices}, 
%  year={2020},
%  volume={16},
%  number={3},
%  pages={1963-1971},
%  doi={10.1109/TII.2019.2938778}
%}



























@inproceedings{abadi2016tensorflow,
  title={TensorFlow: a system for Large-Scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016}
}

%@inproceedings {tensorflow,
%author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
%title = {{TensorFlow}: A System for {Large-Scale} Machine Learning},
%booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
%year = {2016},
%isbn = {978-1-931971-33-1},
%address = {Savannah, GA},
%pages = {265--283},
%howpublished = {\url{https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi}},
%publisher = {USENIX Association},
%month = nov,
%}

@inproceedings{loulergue2019automatic,
  title={Automatic optimization of python skeletal parallel programs},
  author={Loulergue, Fr{\'e}d{\'e}ric and Philippe, Jolan},
  booktitle={International Conference on Algorithms and Architectures for Parallel Processing},
  pages={183--197},
  year={2019},
  organization={Springer}
}

%@InProceedings{Loulergue2020,
%  author    = {Loulergue, Fr{\'e}d{\'e}ric and Philippe, Jolan},
%  booktitle = {Algorithms and Architectures for Parallel Processing},
%  title     = {Automatic Optimization of Python Skeletal Parallel Programs},
%  year      = {2020},
%  address   = {Cham},
%  editor    = {Wen, Sheng and Zomaya, Albert and Yang, Laurence T.},
%  pages     = {183--197},
%  publisher = {Springer International Publishing},
%  abstract  = {Skeletal parallelism is a model of parallelism where parallel constructs are provided to the programmer as usual patterns of parallel algorithms. High-level skeleton libraries often offer a global view of programs instead of the common Single Program Multiple Data view in parallel programming. A program is written as a sequential program but operates on parallel data structures. Most of the time, skeletons on a parallel data structure have counterparts on a sequential data structure. For example, the map function that applies a given function to all the elements of a sequential collection (e.g., a list) has a map skeleton counterpart that applies a sequential function to all the elements of a distributed collection.},
%  groups    = {Code Trans. for ML},
%  isbn      = {978-3-030-38991-8},
%}

@inproceedings{haryono2021mlcatchup,
  title={MLCatchUp: Automated Update of Deprecated Machine-Learning APIs in Python},
  author={Haryono, Stefanus A and Thung, Ferdian and Lo, David and Lawall, Julia and Jiang, Lingxiao},
  booktitle={2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  pages={584--588},
  year={2021},
  organization={IEEE}
}

%@InProceedings{mlcatchup,
%  author    = {Haryono, Stefanus A. and Thung, Ferdian and Lo, David and Lawall, Julia and Jiang, Lingxiao},
%  booktitle = {2021 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
%  title     = {MLCatchUp: Automated Update of Deprecated Machine-Learning APIs in Python},
%  year      = {2021},
%  pages     = {584-588},
%  doi       = {10.1109/ICSME52107.2021.00061},
%  groups    = {Code Trans. for ML},
%}

@article{reed2022torch,
  title={Torch. fx: Practical program capture and transformation for deep learning in python},
  author={Reed, James and DeVito, Zachary and He, Horace and Ussery, Ansley and Ansel, Jason},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={638--651},
  year={2022}
}

%@Misc{torchfx,
%  author    = {Reed, James K. and DeVito, Zachary and He, Horace and Ussery, Ansley and Ansel, Jason},
%  title     = {Torch.fx: Practical Program Capture and Transformation for Deep Learning in Python},
%  year      = {2021},
%  copyright = {arXiv.org perpetual, non-exclusive license},
%  doi       = {10.48550/ARXIV.2112.08429},
%  groups    = {Code Trans. for ML},
%  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences},
%  publisher = {arXiv},
%  howpublished       = {\url{https://arxiv.org/abs/2112.08429}},
%}

@misc{tfonspark,
  author= {Yahoo},
  title = {TensorFlowOnSpark},
  howpublished = {\url{https://github.com/yahoo/TensorFlowOnSpark}},
}

@article{pytorch2019,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

%@article{pytorch2019,
%title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
%author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
%booktitle = {Advances in Neural Information Processing Systems 32},
%editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
%pages = {8024--8035},
%year = {2019},
%publisher = {Curran Associates, Inc.},
%howpublished = {\url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}}
%}

@inproceedings{deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

%@inproceedings{deepspeed,
%author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
%title = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
%year = {2020},
%isbn = {9781450379984},
%publisher = {Association for Computing Machinery},
%address = {New York, NY, USA},
%url = {https://doi.org/10.1145/3394486.3406703},
%doi = {10.1145/3394486.3406703},
%booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on
%  Knowledge Discovery \&  Data Mining},
%pages = {3505‚Äì3506},
%numpages = {2},
%keywords = {distributed deep learning, machine learning},
%location = {Virtual Event, CA, USA},
%series = {KDD '20}
%}

@article{visser2001survey,
  title={A survey of rewriting strategies in program transformation systems},
  author={Visser, Eelco},
  journal={Electronic Notes in Theoretical Computer Science},
  volume={57},
  pages={109--143},
  year={2001},
  publisher={Elsevier}
}

%@Article{Visser2001,
%  author   = {Eelco Visser},
%  journal  = {Electronic Notes in Theoretical Computer Science},
%  title    = {A Survey of Rewriting Strategies in Program Transformation Systems},
%  year     = {2001},
%  issn     = {1571-0661},
%  note     = {WRS 2001, 1st International Workshop on Reduction Strategies in Rewriting and Programming},
%  pages    = {109-143},
%  volume   = {57},
%  doi      = {https://doi.org/10.1016/S1571-0661(04)00270-1},
%  groups   = {Code Trans. for ML},
%  url      = {https://www.sciencedirect.com/science/article/pii/S1571066104002701},
%}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

%@Article{LeCun2015,
%author={LeCun, Yann
%and Bengio, Yoshua
%and Hinton, Geoffrey},
%title={Deep learning},
%journal={Nature},
%year={2015},
%month={May},
%day={01},
%volume={521},
%number={7553},
%pages={436-444},
%abstract={Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
%issn={1476-4687},
%doi={10.1038/nature14539},
%url={https://doi.org/10.1038/nature14539}
%}


@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
%@misc{vggnet2014,
%  doi = {10.48550/ARXIV.1409.1556},
%  url = {https://arxiv.org/abs/1409.1556},
%  author = {Simonyan, Karen and Zisserman, Andrew},
%  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
%  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
%  publisher = {arXiv},
%  year = {2014},
%  copyright = {arXiv.org perpetual, non-exclusive license}
%}


@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

%@misc{resnet2015,
%  doi = {10.48550/ARXIV.1512.03385},
%  url = {https://arxiv.org/abs/1512.03385},
%  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
%  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
%  title = {Deep Residual Learning for Image Recognition},
%  publisher = {arXiv},
%  year = {2015},
%  copyright = {arXiv.org perpetual, non-exclusive license}
%}

@article{bert2018,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

%@misc{bert2018,
%  doi = {10.48550/ARXIV.1810.04805},
%  url = {https://arxiv.org/abs/1810.04805},
%  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
%  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
%  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
%  publisher = {arXiv},
%  year = {2018},
%  copyright = {arXiv.org perpetual, non-exclusive license}
%}

@article{gpt32020,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

%@misc{gpt32020,
%  doi = {10.48550/ARXIV.2005.14165},
%  url = {https://arxiv.org/abs/2005.14165},
%  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
%  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
%  title = {Language Models are Few-Shot Learners},
%  publisher = {arXiv},
%  year = {2020},
%  copyright = {arXiv.org perpetual, non-exclusive license}
%}

@article{imagenet2014,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

%@misc{imagenet2014,
%  doi = {10.48550/ARXIV.1409.0575},
%  url = {https://arxiv.org/abs/1409.0575},
%  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
%  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences, I.4.8; I.5.2},
%  title = {ImageNet Large Scale Visual Recognition Challenge},
%  publisher = {arXiv},
%  year = {2014},
%  copyright = {arXiv.org perpetual, non-exclusive license}
%}

@inproceedings{imagenettraining2017,
  title={Imagenet training in minutes},
  author={You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  booktitle={Proceedings of the 47th International Conference on Parallel Processing},
  pages={1--10},
  year={2018}
}

%@misc{imagenettraining2017,
%  doi = {10.48550/ARXIV.1709.05011},
%  url = {https://arxiv.org/abs/1709.05011},
%  author = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
%  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
%  title = {ImageNet Training in Minutes},
%  publisher = {arXiv},
%  year = {2017},
%  copyright = {arXiv.org perpetual, non-exclusive license}
%}

@article{Silver2017alphagozero,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

%@Article{Silver2017alphagozero,
%author={Silver, David
%and Schrittwieser, Julian
%and Simonyan, Karen
%and Antonoglou, Ioannis
%and Huang, Aja
%and Guez, Arthur
%and Hubert, Thomas
%and Baker, Lucas
%and Lai, Matthew
%and Bolton, Adrian
%and Chen, Yutian
%and Lillicrap, Timothy
%and Hui, Fan
%and Sifre, Laurent
%and van den Driessche, George
%and Graepel, Thore
%and Hassabis, Demis},
%title={Mastering the game of Go without human knowledge},
%journal={Nature},
%year={2017},
%month={Oct},
%day={01},
%volume={550},
%number={7676},
%pages={354-359},
%abstract={A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
%issn={1476-4687},
%doi={10.1038/nature24270},
%url={https://doi.org/10.1038/nature24270}
%}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
