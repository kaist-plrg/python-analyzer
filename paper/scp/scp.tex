\documentclass[preprint, 12pt]{elsarticle}% Math and Physical Sciences Reference Style

% \articletype{Research article}%

% \received{26 April 2016}
% \revised{6 June 2016}
% \accepted{6 June 2016}

\raggedbottom

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{tabularx}

\input{macro}
\usepackage{url}
\usepackage[T1]{fontenc}
\lstdefinestyle{mpython}{
  language=Python,
  upquote=true,
  morekeywords={with,as},
  emphstyle=\color{blue},
  emph={True,False},
  deletekeywords=[2]{compile},
}

\journal{Science of Computer Programming}

\begin{document}

\begin{frontmatter}

\title{Automated Code Transformation for Distributed Training of TensorFlow Deep Learning Models}

\author[inst1]{Yusung Sim}

\author[inst1]{Wonho Shin}

\author[inst2]{Sungho Lee\corref{cor1}}

\cortext[cor1]{Corresponding author}

%\authormark{AUTHOR ONE \textsc{et al}}

%\affil[1]{\orgdiv{School of Computing}, \orgname{KAIST}, \orgaddress{\state{Daejeon}, \country{Republic of Korea}}}

\affiliation[inst1]{organization={KAIST},%Department and Organization
            addressline={291 Daehak-ro, Yuseong-gu}, 
            city={Daejeon},
            postcode={34141}, 
            country={Republic of Korea}}

%\address[2]{\orgdiv{School of Computing}, \orgname{KAIST}, \orgaddress{\state{Daejeon}, \country{South Korea}}}

% \affil[2]{\orgdiv{Department of Computer Science and Engineering}, \orgname{Chungnam National University}, \orgaddress{\state{Daejeon}, \country{Republic of Korea}}}
\affiliation[inst2]{organization={Chungnam National University},%Department and Organization
            addressline={99 Daehak-ro, Yuseong-gu}, 
            city={Daejeon},
            postcode={34141}, 
            country={Republic of Korea}}

% \corres{Sungho Lee, Department of Computer Science and Engineering, Chungnam National University, 99 Daehak-ro, Yuseong-gu, Daejeon 34134, Republic of Korea. \email{eshaj@cnu.ac.kr}}

%\fundingAgency{National Research Foundation of Korea}
%\fundingNumber{2021R1F1A1051310}
%\presentaddress{This is sample for present address text this is sample for present address text}


\begin{abstract}
Distributed training of deep learning models reduces training time by
parallelizing training workloads across multiple GPUs.
Distributed training frameworks, such as Horovod and DeepSpeed, provide APIs,
and model engineers rewrite deep learning models using the APIs to parallelize
their training.
However, the rewriting is time-consuming and labor-intensive because it
requires engineers to read and understand documents and examples of the
frameworks as well as manual efforts to rewrite code.

In this paper, we propose an automated code transformation approach that
transforms TensorFlow deep learning models designed for non-distributed
training to models training on multiple GPUs with the Horovod framework.
We closely inspect the Horovod document and code examples and identify four
common training patterns of TensorFlow deep learning models.
Then, we formalize code transformation rules for each training pattern. 
Using the rules, we implement an automated code transformation tool that takes
a TensorFlow deep learning model written in Python and rewrites it with the
Horovod APIs for distributed training. 
Our evaluation shows that the tool correctly transforms 15 out of 16
open-source TensorFlow deep learning models.
We believe that our approach significantly reduces manual efforts to
parallelize training of existing TensorFlow deep learning models.
\end{abstract}

%%Research highlights
%\begin{highlights}
%\item Research highlight 1
%\item Research highlight 2
%\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
machine learning \sep distributed training \sep code transformation \sep Python
%% PACS codes here, in the form: \PACS code \sep code
%\PACS 0000 \sep 1111
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
%\MSC 0000 \sep 1111
\end{keyword}

% \keywords{machine learning, distributed training, code transformation, Python}

% \maketitle

\end{frontmatter}
%\footnotetext{\textbf{Abbreviations:} ANA, anti-nuclear antibodies; APC, antigen-presenting cells; IRF, interferon regulatory factor}

\input{intro}
\input{background}
\input{overview}
%\input{parse}
\input{cha}
\input{pattern}
\input{trans}
\input{eval}
\input{related}
\input{conclusion}

\section*{Acknowledgements}

This work was supported by Institute for Information \& communications
Technology Planning \& Evaluation(IITP) grant funded by the Korea government
(MSIT)(No.RS-2022-II221200, Convergence security core talent training
business(Chungnam National University)).


\clearpage

\bibliographystyle{elsarticle-num}
\bibliography{ref}

%\bibliography{sn-bibliography}% common bib file

% TODO(all): Add your biography here.
%\begin{biography}{\includegraphics[width=66pt,height=86pt,draft]{empty}}{\textbf{Author Name.} This is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text this is sample author biography text.}
%\end{biography}

\end{document}
