\section{Introduction}\label{sec:intro}

With recent advancements in artificial intelligence, deep learning (DL) has
been widely utilized in various fields.
%Recent advancements in deep learning(DL) have opened the wide possibility of
%applying artificial intelligence in various fields.
LeCun et al.~\cite{lecun2015deep} define deep learning as a machine learning
technique that composes multiple abstraction layers to build up a high-level
representation of raw data. 
%LeCun et al.\cite{LeCun2015} define deep learning as a
%type of machine learning technique that composes multiple
%abstraction layers to build up a high-level representation of raw data. 
Model engineers construct DL models as neural networks consisting of independent
layers of several perceptrons, and each layer gets input signals from the
previous layer and sends output signals to the next layer.
%The neural networks are composed of independent layers of perceptrons,
%which get input signals from the previous layer then sends the
%output signal to the next layer.
%The first layer is the input layer, which gets an raw data as an input signal,
%and the last layer is the output layer, which returns the
%whole computation result of the network.
%Any other layers between the input layer and the output layer are 
%called hidden layers; the word \textit{deep} comes from the fact that
%the neural networks use multiple hidden layers.
%In practice, developers use DL frameworks that support easier building of
%deep neural networks in high-level code, for example, using
%TensorFlow\cite{tensorflow} with Python programming language\cite{pythonref}.
Several DL frameworks, such as TensorFlow~\cite{abadi2016tensorflow} and
PyTorch~\cite{pytorch2019}, provide APIs to easily implement deep neural
networks in general purpose high-level programming languages like Python.
VGG~\cite{simonyan2014very} and ResNet~\cite{he2016deep} for image recognition and
BERT~\cite{bert2018} and GPT-3~\cite{gpt32020} for natural language processing
are popular deep learning applications.


The deep learning development process consists of training and inference
phases.
%Deep learning development consists of two stages: 
%the training stage and the inference stage.
The training phase improves the accuracy of DL models via multiple training
steps that adjust models' parameters via forward and backward propagation. 
The forward propagation calculates answers of DL models for a training dataset
and computes an error rate of the answers, called {\it loss}.
Then, the backward propagation computes a model parameter gradient and adjusts
parameters to reduce the loss using gradient descent algorithms.
The training phase repeats the propagations multiple times over the entire
training dataset for better accuracy.
%In the training stage, the hidden layer parameters are updated
%with respect to the training dataset so that the output from the output layer
%matches the answer in the dataset. 
%In other words, the model learns
%to return correct answer to the given training data. 
%The training stage repeats multiple training steps.
%A training step consists of a \textit{forward propagation} and 
%\textit{backward propagation}.
%The forward propagation feeds an input training batch to the
%network and computes the loss between the network output and the answer.
%The backward propagation computes the gradients of the model parameters
%and optimize them by applying gradient descent algorithm. 
%The training steps are repeated until the accuracy of the model converges.
%In many of the cases, the training repeats multiple times over the entire 
%training dataset in order to gain better accuracy.  
In the inference phase, trained DL models produce actual prediction results on
input data via forward propagation.
%In the inference stage, new data unseen to the model
%is given as an input to the model, a forward propagation returns an output
%and it is used as prediction results for the data.
%Because the model \textit{learned} general patterns from the training
%dataset, the model probably returns correct results for the
%unseen data.

While the training phase is essential to the DL model development, it is the
most time-consuming.
Model engineers train models on a huge training dataset for better accuracy,
but a larger dataset also requires more training time.
According to the report by You et al.~\cite{imagenettraining2017}, it takes 14
days on a single GPU to train the ResNet-50 model on the ImageNet benchmark
dataset containing 1.28 million images~\cite{imagenet2014}.
Because models are frequently modified and retrained during development, the
model development cost increases as the training time increases.

Taking advantages of parallelism, distributed training has emerged to reduce
the training time. 
Since distributed training parallelizes training workload across multiple GPUs,
model engineers can train models in significantly less time than
non-distributed training.
Research in various fields utilizes distributed training without losing
the accuracy of trained models.
Goyal et al.~\cite{goyal2017accurate} trained the ResNet-50 model on the ImageNet
benchmark in one hour with 256 GPUs, which is over 300 times faster than the
non-distributed training result.
Silver et al.~\cite{Silver2017alphagozero} trained AlphaGo with 176
GPUs and 48 TPUs, Zhang et al.~\cite{zhang2019distributed} used 16 GPUs to train
a speech recognition model, and Tian et al.~\cite{tian2019distributed} used
two GPUs to train a web attack detection model on edge devices.

%The training stage is the most essential part of DL model development.
%To generalize well on the unseen data,
%the training process repeats training steps on the huge amount of
%training dataset.
%Repeating training step over large dataset requires enormous
%amount of computation time.
%According to the reports from You et al.\cite{imagenettraining2017},
%training ResNet-50 model with ImageNet benchmark dataset on a 
%single NVIDIA M40 GPU takes 14 days (The ImageNet benchmark\cite{imagenet2014} 
%contains 1.28 million training images). 

%In one of the efforts to reduce training time, 
%researchers utilize \textit{distributed training}.
%Distributed training is a technique to parallelize the training computation
%workload over multiple GPUs.
%By taking advantage of parallelism, distributed training enables DL developers 
%to spend less time in training while preserving accuracy.
%Goyal et al.\cite{facebook2018} trained the ResNet-50 model on ImageNet
%in one hour with 256 GPUs, which is over 300 times faster than the
%single-GPU training result of You et al.\cite{imagenettraining2017}.
%Using multiple GPUs and hardwares to quickly train the DL model is 
%already adopted in previous works.
%Silver et al.\cite{Silver2017alphagozero} trained the famous AlphaGo 
%with 176 GPUs and 48 TPUs;
%Zhang et al.\cite{zhang2019distrspeech} used 16 GPUs to train
%a speech recognition model;
%Tian et al.\cite{tian2020distrwebattack} used 
%two GPUs to train a web attack detection model on edge devices.
%All these works used multiple GPUs and/or specialized hardwares 
%to train complex DL modelover large training dataset, 
%which efficiently reduces training time.
%As DL models are becoming more complex and training datasets are growing,
%the need for distributing the training process in DL research is inevitable.

Meanwhile, DL models designed for non-distributed training are not directly
trainable on multiple GPUs.
Model engineers need to rewrite the models for distributed training
with additional configurations in their training code to identify GPUs in the
system, spawn processes for each GPU, and assign the training dataset to each
process.
There are a few distributed training frameworks, such as
Horovod~\cite{sergeev2018horovod} and DeepSpeed~\cite{deepspeed}, which provide
APIs for distributed training without such complex configurations.

However, manually rewriting the models is time-consuming and labor-intensive.
Even though model engineers utilize the frameworks, additional works should be done,
such as reading documents and code examples of the frameworks, and modifying
the training code using their APIs.
To the best of our knowledge, there is no automated method to train models on
multiple GPUs without manual modifications.


%The distributed training code contains additional implementation for
%recognizing GPUs in the system, spawning processes for each GPU, and assigning
%the training dataset into each process.
%Developers utilize distributed training libraries which can be added to
%existing DL training codes that are written in popular DL frameworks. 
%For example, Horovod\cite{sergeev2018horovod} is a popular distributed training
%library for Python that supports multiple DL frameworks including TensorFlow.
%Until now, developers have manually rewritten the model codes.
%This is a time-consuming and labor-intensive task for developers.
%In addition, developers need to understand and locate specific components
%involved in the model training.
%This requires the developer to fully understand the library APIs, which is a
%difficult challenge.


In this paper, we propose an automated approach that transforms TensorFlow DL
models to ones training on multiple GPUs with the Horovod framework.
%In this paper, we propose an \textit{automated code transformation for
%distributed training}.
%The transformation converts the single-GPU-based TensorFlow models into the
%multi-GPU-based models so that developers can apply distributed training.
%Our goal is to distribute TensorFlow DL models written in Python by modifying
%the training codes with the Horovod library.
We closely inspected the Horovod library documentation and the code examples
that describe the code transformation required to train DL models on multiple
GPUs.
From the description, we identified four common training patterns used in
TensorFlow DL models and the code transformation required for each training
pattern.
Then, we formally defined transformation rules that rewrite models with Horovod
APIs for distributed training.
Based on the formal rules, we implemented an automated model transformation
tool for distributed training.
Our tool first analyzes an input DL model to identify its training
pattern and code locations on which modifications for distributed training
are required.
It then rewrites the model by applying the transformation rules of the
identified training pattern.
Our evaluation shows that our tool successfully transforms 15 out of 16
open-source TensorFlow DL models, and the transformed models with newly tuned
hyperparameters train about 2.28 times faster than the original models.
We also discuss the effects of distributed training of the models in the
evaluation.


%We manually inspected the Horovod library documentation and the code examples
%to define the code transformations required for the distributed training. 
%In this end, we identified four common patterns of TensorFlow DL training API
%usage and code transformation rules for each training API pattern. 
%We then formally define the code transformation from single-GPU-based training
%code to distributed training code.
%We implement the distributed training code transformation as a tool and perform
%transformation experiments with 16 open-source TensorFlow DL models. 

The contributions of this paper are as follows:

\begin{itemize}
  \item {\bf We formalize the code transformation rules for distributed
    training of TensorFlow DL models.} The formal rules allow model engineers
    to understand the transformation in an explicit way rather than implicit
    code examples, as well as provide a basis for automation. 
    %We formally define the code transformation as functions from AST to AST. 
    %After manually inspecting the Horovod documentation and code examples,
    %We provide transform function definitions for automatically
    %transforming single-GPU model codes into distributed DL model codes.

  \item {\bf We design and implement an automated code transformation tool for
    distributed training.} Our tool can reduce manual efforts in rewriting
    models for distributed training via automation.
    %We evaluate the tool's performance
    %by applying the transformation to TensorFlow example model codes.

  \item {\bf We reveal that distributed training often requires additional
    hyperparameter tuning.} 
    Our empirical evaluation shows that distributed training without newly
    tuned hyperparameters may perform worse in training time and inference
    precision than non-distributed training.
    %By evaluation, we show that distributed training of TensorFlow model
    %using Horovod library does not always speed up. 
    %We provide further empricial evidence that the training hyperparameters can
    %be adjusted to increase the training speed.
    %While this implies that developers may have to further tune the
    %hyperparameters, our transformation tool allows quicker testing and tuning
    %for distributed training of existing DL model. 
\end{itemize}
